nohup: ignoring input
Running sweep with config: FT_hp_sweep_class_dist
wandb: WARNING Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.
wandb: WARNING To avoid this, please fix the sweep config schema violations below:
wandb: WARNING   Violation 1. Additional properties are not allowed ('finetuning' was unexpected)
2025-04-10 14:54:39,290 - wandb.wandb_agent - INFO - Running runs: []
Create sweep with ID: rl522wkb
Sweep URL: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
[2025-04-10 14:54:39,290][wandb.wandb_agent][INFO] - Running runs: []
2025-04-10 14:54:39,586 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 14:54:39,586][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 14:54:39,587 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.14399029248418915
	config.input_dropout: 0.41028487159937616
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.456226016033919
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 62
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.014636013034215117
	optimization_config.init_lr: 0.208268864236788
	optimization_config.lr_decay_power: 0.8188169391413362
	optimization_config.lr_frac_warmup_steps: 0.2184591383312132
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0027211074928661697
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 14:54:39,587][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.14399029248418915
	config.input_dropout: 0.41028487159937616
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.456226016033919
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 62
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.014636013034215117
	optimization_config.init_lr: 0.208268864236788
	optimization_config.lr_decay_power: 0.8188169391413362
	optimization_config.lr_frac_warmup_steps: 0.2184591383312132
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0027211074928661697
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 14:54:39,598 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.14399029248418915 config.input_dropout=0.41028487159937616 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.456226016033919 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=62 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.014636013034215117 optimization_config.init_lr=0.208268864236788 optimization_config.lr_decay_power=0.8188169391413362 optimization_config.lr_frac_warmup_steps=0.2184591383312132 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0027211074928661697 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 14:54:39,598][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.14399029248418915 config.input_dropout=0.41028487159937616 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.456226016033919 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=62 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.014636013034215117 optimization_config.init_lr=0.208268864236788 optimization_config.lr_decay_power=0.8188169391413362 optimization_config.lr_frac_warmup_steps=0.2184591383312132 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0027211074928661697 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 14:54:44,610 - wandb.wandb_agent - INFO - Running runs: ['eo9yku0i']
[2025-04-10 14:54:44,610][wandb.wandb_agent][INFO] - Running runs: ['eo9yku0i']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_145445-eo9yku0i
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/eo9yku0i
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.14399029248418915
Overwriting input_dropout in config from 0.4494236115512016 to 0.41028487159937616
Overwriting resid_dropout in config from 0.4939188761966135 to 0.456226016033919
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.14399029248418915
Overwriting input_dropout in config from 0.4494236115512016 to 0.41028487159937616
Overwriting resid_dropout in config from 0.4939188761966135 to 0.456226016033919
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.35it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=ku0i]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.69it/s, v_num=ku0i]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.69it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.57it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.38it/s, v_num=ku0i]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.38it/s, v_num=ku0i]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.63it/s, v_num=ku0i]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.63it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.62it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  0.49it/s, v_num=ku0i]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  0.49it/s, v_num=ku0i]Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=ku0i]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=ku0i]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.57it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=ku0i]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=ku0i]Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.40it/s, v_num=ku0i]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.40it/s, v_num=ku0i]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=ku0i]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.68it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.67it/s, v_num=ku0i]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.67it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.46it/s][A
                                                                      [AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 6:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 6:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 6:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s][A
                                                                      [AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 6:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 7:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 7:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=ku0i]Epoch 7:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=ku0i]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s][A
                                                                      [AEpoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 7:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 8:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 8:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.40it/s, v_num=ku0i]Epoch 8:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.40it/s, v_num=ku0i]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=ku0i]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.66it/s][A
                                                                      [AEpoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 8:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 9:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 9:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 9:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.67it/s, v_num=ku0i]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.67it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s][A
                                                                      [AEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 9:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=ku0i]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=ku0i]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.68it/s][A
                                                                      [AEpoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=ku0i]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=ku0i]Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 11:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 11:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 11:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=ku0i]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s][A
                                                                      [AEpoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 11:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 12:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 12:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=ku0i]Epoch 12:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=ku0i]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.68it/s][A
                                                                      [AEpoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=ku0i]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 12:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 13:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 13:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=ku0i]Epoch 13:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=ku0i]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.76it/s][A
                                                                      [AEpoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 13:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 14:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 14:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.39it/s, v_num=ku0i]Epoch 14:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.39it/s, v_num=ku0i]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.63it/s, v_num=ku0i]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.63it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.67it/s][A
                                                                      [AEpoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  0.50it/s, v_num=ku0i]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  0.50it/s, v_num=ku0i]Epoch 14:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s][A
                                                                      [AEpoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 16:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 16:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 16:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.67it/s, v_num=ku0i]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.67it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.71it/s][A
                                                                      [AEpoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 16:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 17:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 17:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=ku0i]Epoch 17:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=ku0i]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.67it/s, v_num=ku0i]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.67it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.70it/s][A
                                                                      [AEpoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=ku0i]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=ku0i]Epoch 17:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 18:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 18:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.40it/s, v_num=ku0i]Epoch 18:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.40it/s, v_num=ku0i]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s][A
                                                                      [AEpoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=ku0i]Epoch 18:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 19:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 19:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 19:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.41it/s, v_num=ku0i]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.67it/s, v_num=ku0i]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.67it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.76it/s][A
                                                                      [AEpoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 19:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]        Epoch 20:   0%|          | 0/2 [00:00<?, ?it/s, v_num=ku0i]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=ku0i]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=ku0i]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.68it/s, v_num=ku0i]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.65it/s][A
                                                                      [AEpoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=ku0i]Restoring states from the checkpoint path at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/eo9yku0i/checkpoints/epoch=12-val_loss=0.00-best_model.ckpt
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/eo9yku0i/checkpoints/epoch=12-val_loss=0.00-best_model.ckpt
/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.validate()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.

/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/finetune_weights
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/held_out_0.parquet
/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/finetune_weights
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/held_out_0.parquet
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/1 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.72it/s]Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.03it/s]Restoring states from the checkpoint path at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/eo9yku0i/checkpoints/epoch=12-val_loss=0.00-best_model.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/eo9yku0i/checkpoints/epoch=12-val_loss=0.00-best_model.ckpt
/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          Validate metric                       DataLoader 0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             task_AUROC                             0.0
             task_loss                       224.5921173095703
           tuning_TTE_MSE                        13971177.0
          tuning_TTE_MSLE                    11.604084014892578
         tuning_TTE_reg_NLL                  5.432831764221191
     tuning_event_label_cls_NLL              2.0653860569000244
 tuning_event_label_macro_accuracy           0.7213603854179382
 tuning_event_label_micro_accuracy           0.7213603258132935
 tuning_event_label_weighted_AUROC          0.41897788643836975
tuning_event_label_weighted_accuracy         0.5195654034614563
     tuning_event_type_cls_NLL              0.007218413986265659
  tuning_event_type_macro_accuracy                  0.0
  tuning_event_type_micro_accuracy                  0.0
  tuning_event_type_weighted_AUROC                  0.0
tuning_event_type_weighted_accuracy                 0.0
        tuning_feature_0_MSE                 4.1933817863464355
      tuning_feature_0_reg_NLL               1.6101888418197632
       tuning_feature_10_MSE                 70.58033752441406
     tuning_feature_10_reg_NLL               3.0477359294891357
       tuning_feature_11_MSE                 10.302010536193848
     tuning_feature_11_reg_NLL               2.1009669303894043
       tuning_feature_12_MSE                 5.154676914215088
     tuning_feature_12_reg_NLL               1.669507622718811
       tuning_feature_13_MSE                 57.32088088989258
     tuning_feature_13_reg_NLL               3.065699577331543
       tuning_feature_14_MSE                 17.542804718017578
     tuning_feature_14_reg_NLL               2.186286211013794
       tuning_feature_15_MSE                 3.349388360977173
     tuning_feature_15_reg_NLL               1.5137665271759033
       tuning_feature_16_MSE                 6.496450901031494
     tuning_feature_16_reg_NLL               1.8545652627944946
       tuning_feature_17_MSE                 3.8824353218078613
     tuning_feature_17_reg_NLL               1.5955082178115845
       tuning_feature_18_MSE                 1.9025061130523682
     tuning_feature_18_reg_NLL               1.4776276350021362
       tuning_feature_19_MSE                 2.293503999710083
     tuning_feature_19_reg_NLL               1.246198296546936
        tuning_feature_1_MSE                 115.52916717529297
      tuning_feature_1_reg_NLL               3.2852158546447754
       tuning_feature_20_MSE                 14.123266220092773
     tuning_feature_20_reg_NLL               2.208223819732666
       tuning_feature_21_MSE                 158.90419006347656
     tuning_feature_21_reg_NLL               3.5263326168060303
       tuning_feature_22_MSE                 136.21774291992188
     tuning_feature_22_reg_NLL               3.386176824569702
       tuning_feature_23_MSE                 25.767135620117188
     tuning_feature_23_reg_NLL               2.4432554244995117
       tuning_feature_24_MSE                 44.60988998413086
     tuning_feature_24_reg_NLL               2.6824522018432617
        tuning_feature_2_MSE                 9.657840728759766
      tuning_feature_2_reg_NLL               2.045938491821289
        tuning_feature_3_MSE                 10.355274200439453
      tuning_feature_3_reg_NLL               2.110743284225464
        tuning_feature_4_MSE                 63.106510162353516
      tuning_feature_4_reg_NLL               2.944915294647217
        tuning_feature_5_MSE                 100.43040466308594
      tuning_feature_5_reg_NLL               3.3101205825805664
        tuning_feature_6_MSE                 102.81390380859375
      tuning_feature_6_reg_NLL               3.250209093093872
        tuning_feature_7_MSE                 38.003238677978516
      tuning_feature_7_reg_NLL               2.736743450164795
        tuning_feature_8_MSE                 14.358614921569824
      tuning_feature_8_reg_NLL               2.2697865962982178
        tuning_feature_9_MSE                 10.406036376953125
      tuning_feature_9_reg_NLL               2.1594173908233643
            tuning_loss                      291.8251037597656
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.70it/s]Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.21it/s]wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                  epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                       held_out_TTE_MSE ‚ñÅ
wandb:                      held_out_TTE_MSLE ‚ñÅ
wandb:                   held_out_TTE_reg_NLL ‚ñÅ
wandb:           held_out_event_label_cls_NLL ‚ñÅ
wandb:    held_out_event_label_macro_accuracy ‚ñÅ
wandb:    held_out_event_label_micro_accuracy ‚ñÅ
wandb:    held_out_event_label_weighted_AUROC ‚ñÅ
wandb: held_out_event_label_weighted_accuracy ‚ñÅ
wandb:            held_out_event_type_cls_NLL ‚ñÅ
wandb:     held_out_event_type_macro_accuracy ‚ñÅ
wandb:     held_out_event_type_micro_accuracy ‚ñÅ
wandb:     held_out_event_type_weighted_AUROC ‚ñÅ
wandb:  held_out_event_type_weighted_accuracy ‚ñÅ
wandb:                 held_out_feature_0_MSE ‚ñÅ
wandb:             held_out_feature_0_reg_NLL ‚ñÅ
wandb:                held_out_feature_10_MSE ‚ñÅ
wandb:            held_out_feature_10_reg_NLL ‚ñÅ
wandb:                held_out_feature_11_MSE ‚ñÅ
wandb:            held_out_feature_11_reg_NLL ‚ñÅ
wandb:                held_out_feature_12_MSE ‚ñÅ
wandb:            held_out_feature_12_reg_NLL ‚ñÅ
wandb:                held_out_feature_13_MSE ‚ñÅ
wandb:            held_out_feature_13_reg_NLL ‚ñÅ
wandb:                held_out_feature_14_MSE ‚ñÅ
wandb:            held_out_feature_14_reg_NLL ‚ñÅ
wandb:                held_out_feature_15_MSE ‚ñÅ
wandb:            held_out_feature_15_reg_NLL ‚ñÅ
wandb:                held_out_feature_16_MSE ‚ñÅ
wandb:            held_out_feature_16_reg_NLL ‚ñÅ
wandb:                held_out_feature_17_MSE ‚ñÅ
wandb:            held_out_feature_17_reg_NLL ‚ñÅ
wandb:                held_out_feature_18_MSE ‚ñÅ
wandb:            held_out_feature_18_reg_NLL ‚ñÅ
wandb:                held_out_feature_19_MSE ‚ñÅ
wandb:            held_out_feature_19_reg_NLL ‚ñÅ
wandb:                 held_out_feature_1_MSE ‚ñÅ
wandb:             held_out_feature_1_reg_NLL ‚ñÅ
wandb:                held_out_feature_20_MSE ‚ñÅ
wandb:            held_out_feature_20_reg_NLL ‚ñÅ
wandb:                held_out_feature_21_MSE ‚ñÅ
wandb:            held_out_feature_21_reg_NLL ‚ñÅ
wandb:                held_out_feature_22_MSE ‚ñÅ
wandb:            held_out_feature_22_reg_NLL ‚ñÅ
wandb:                held_out_feature_23_MSE ‚ñÅ
wandb:            held_out_feature_23_reg_NLL ‚ñÅ
wandb:                held_out_feature_24_MSE ‚ñÅ
wandb:            held_out_feature_24_reg_NLL ‚ñÅ
wandb:                 held_out_feature_2_MSE ‚ñÅ
wandb:             held_out_feature_2_reg_NLL ‚ñÅ
wandb:                 held_out_feature_3_MSE ‚ñÅ
wandb:             held_out_feature_3_reg_NLL ‚ñÅ
wandb:                 held_out_feature_4_MSE ‚ñÅ
wandb:             held_out_feature_4_reg_NLL ‚ñÅ
wandb:                 held_out_feature_5_MSE ‚ñÅ
wandb:             held_out_feature_5_reg_NLL ‚ñÅ
wandb:                 held_out_feature_6_MSE ‚ñÅ
wandb:             held_out_feature_6_reg_NLL ‚ñÅ
wandb:                 held_out_feature_7_MSE ‚ñÅ
wandb:             held_out_feature_7_reg_NLL ‚ñÅ
wandb:                 held_out_feature_8_MSE ‚ñÅ
wandb:             held_out_feature_8_reg_NLL ‚ñÅ
wandb:                 held_out_feature_9_MSE ‚ñÅ
wandb:             held_out_feature_9_reg_NLL ‚ñÅ
wandb:                          held_out_loss ‚ñÅ
wandb:                             task_AUROC ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                              task_loss ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ
wandb:                    trainer/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                         tuning_TTE_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                        tuning_TTE_MSLE ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÇ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ
wandb:                     tuning_TTE_reg_NLL ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñá‚ñÅ‚ñÅ
wandb:             tuning_event_label_cls_NLL ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñÖ
wandb:      tuning_event_label_macro_accuracy ‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb:      tuning_event_label_micro_accuracy ‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb:      tuning_event_label_weighted_AUROC ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ
wandb:   tuning_event_label_weighted_accuracy ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:              tuning_event_type_cls_NLL ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       tuning_event_type_macro_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       tuning_event_type_micro_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       tuning_event_type_weighted_AUROC ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    tuning_event_type_weighted_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                   tuning_feature_0_MSE ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ
wandb:               tuning_feature_0_reg_NLL ‚ñÇ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÑ
wandb:                  tuning_feature_10_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÜ
wandb:              tuning_feature_10_reg_NLL ‚ñÅ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñà
wandb:                  tuning_feature_11_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÅ
wandb:              tuning_feature_11_reg_NLL ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÑ
wandb:                  tuning_feature_12_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÅ
wandb:              tuning_feature_12_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñÉ
wandb:                  tuning_feature_13_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÇ
wandb:              tuning_feature_13_reg_NLL ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÅ
wandb:                  tuning_feature_14_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÇ
wandb:              tuning_feature_14_reg_NLL ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÑ
wandb:                  tuning_feature_15_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ
wandb:              tuning_feature_15_reg_NLL ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÇ
wandb:                  tuning_feature_16_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÅ
wandb:              tuning_feature_16_reg_NLL ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÉ
wandb:                  tuning_feature_17_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñà‚ñÅ
wandb:              tuning_feature_17_reg_NLL ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÇ
wandb:                  tuning_feature_18_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñà‚ñÅ
wandb:              tuning_feature_18_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñÉ
wandb:                  tuning_feature_19_MSE ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñÅ
wandb:              tuning_feature_19_reg_NLL ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÇ
wandb:                   tuning_feature_1_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñá
wandb:               tuning_feature_1_reg_NLL ‚ñá‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà
wandb:                  tuning_feature_20_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÇ
wandb:              tuning_feature_20_reg_NLL ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÉ
wandb:                  tuning_feature_21_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÑ
wandb:              tuning_feature_21_reg_NLL ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb:                  tuning_feature_22_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñÉ
wandb:              tuning_feature_22_reg_NLL ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ
wandb:                  tuning_feature_23_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÑ
wandb:              tuning_feature_23_reg_NLL ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ
wandb:                  tuning_feature_24_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñÉ
wandb:              tuning_feature_24_reg_NLL ‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ
wandb:                   tuning_feature_2_MSE ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÇ
wandb:               tuning_feature_2_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÉ
wandb:                   tuning_feature_3_MSE ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÖ‚ñÑ
wandb:               tuning_feature_3_reg_NLL ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñá
wandb:                   tuning_feature_4_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñà
wandb:               tuning_feature_4_reg_NLL ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñà
wandb:                   tuning_feature_5_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÉ
wandb:               tuning_feature_5_reg_NLL ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ
wandb:                   tuning_feature_6_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÑ
wandb:               tuning_feature_6_reg_NLL ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ
wandb:                   tuning_feature_7_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñÇ
wandb:               tuning_feature_7_reg_NLL ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÑ
wandb:                   tuning_feature_8_MSE ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÜ
wandb:               tuning_feature_8_reg_NLL ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÖ
wandb:                   tuning_feature_9_MSE ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÅ
wandb:               tuning_feature_9_reg_NLL ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÑ
wandb:                            tuning_loss ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                  epoch 21
wandb:                       held_out_TTE_MSE 16173239.0
wandb:                      held_out_TTE_MSLE 16.26456
wandb:                   held_out_TTE_reg_NLL 6.02144
wandb:           held_out_event_label_cls_NLL 1.95727
wandb:    held_out_event_label_macro_accuracy 0.71836
wandb:    held_out_event_label_micro_accuracy 0.71836
wandb:    held_out_event_label_weighted_AUROC 0.38152
wandb: held_out_event_label_weighted_accuracy 0.47394
wandb:            held_out_event_type_cls_NLL 0.00774
wandb:     held_out_event_type_macro_accuracy 0.0
wandb:     held_out_event_type_micro_accuracy 0.0
wandb:     held_out_event_type_weighted_AUROC 0.0
wandb:  held_out_event_type_weighted_accuracy 0.0
wandb:                 held_out_feature_0_MSE 6.07058
wandb:             held_out_feature_0_reg_NLL 2.03633
wandb:                held_out_feature_10_MSE 78.93822
wandb:            held_out_feature_10_reg_NLL 3.19539
wandb:                held_out_feature_11_MSE 11.45582
wandb:            held_out_feature_11_reg_NLL 2.27261
wandb:                held_out_feature_12_MSE 7.24254
wandb:            held_out_feature_12_reg_NLL 2.19448
wandb:                held_out_feature_13_MSE 58.97672
wandb:            held_out_feature_13_reg_NLL 3.06961
wandb:                held_out_feature_14_MSE 21.06101
wandb:            held_out_feature_14_reg_NLL 2.39359
wandb:                held_out_feature_15_MSE 3.93482
wandb:            held_out_feature_15_reg_NLL 1.6437
wandb:                held_out_feature_16_MSE 7.28929
wandb:            held_out_feature_16_reg_NLL 1.936
wandb:                held_out_feature_17_MSE 4.5109
wandb:            held_out_feature_17_reg_NLL 1.71324
wandb:                held_out_feature_18_MSE 2.42696
wandb:            held_out_feature_18_reg_NLL 2.24773
wandb:                held_out_feature_19_MSE 2.96656
wandb:            held_out_feature_19_reg_NLL 1.49671
wandb:                 held_out_feature_1_MSE 120.01143
wandb:             held_out_feature_1_reg_NLL 3.31551
wandb:                held_out_feature_20_MSE 15.82377
wandb:            held_out_feature_20_reg_NLL 2.31368
wandb:                held_out_feature_21_MSE 159.77654
wandb:            held_out_feature_21_reg_NLL 3.52862
wandb:                held_out_feature_22_MSE 138.09506
wandb:            held_out_feature_22_reg_NLL 3.32659
wandb:                held_out_feature_23_MSE 24.71924
wandb:            held_out_feature_23_reg_NLL 2.48007
wandb:                held_out_feature_24_MSE 34.64237
wandb:            held_out_feature_24_reg_NLL 2.65299
wandb:                 held_out_feature_2_MSE 12.25576
wandb:             held_out_feature_2_reg_NLL 2.25684
wandb:                 held_out_feature_3_MSE 10.60253
wandb:             held_out_feature_3_reg_NLL 2.25175
wandb:                 held_out_feature_4_MSE 60.96799
wandb:             held_out_feature_4_reg_NLL 2.981
wandb:                 held_out_feature_5_MSE 108.19849
wandb:             held_out_feature_5_reg_NLL 3.35211
wandb:                 held_out_feature_6_MSE 109.96964
wandb:             held_out_feature_6_reg_NLL 3.27738
wandb:                 held_out_feature_7_MSE 34.71988
wandb:             held_out_feature_7_reg_NLL 2.71686
wandb:                 held_out_feature_8_MSE 16.20456
wandb:             held_out_feature_8_reg_NLL 2.35734
wandb:                 held_out_feature_9_MSE 9.53802
wandb:             held_out_feature_9_reg_NLL 2.10683
wandb:                          held_out_loss 712.16302
wandb:                             task_AUROC 0.0
wandb:                              task_loss 641.05969
wandb:                    trainer/global_step 42
wandb:                         tuning_TTE_MSE 13971177.0
wandb:                        tuning_TTE_MSLE 11.60408
wandb:                     tuning_TTE_reg_NLL 5.43283
wandb:             tuning_event_label_cls_NLL 2.06539
wandb:      tuning_event_label_macro_accuracy 0.72136
wandb:      tuning_event_label_micro_accuracy 0.72136
wandb:      tuning_event_label_weighted_AUROC 0.41898
wandb:   tuning_event_label_weighted_accuracy 0.51957
wandb:              tuning_event_type_cls_NLL 0.00722
wandb:       tuning_event_type_macro_accuracy 0.0
wandb:       tuning_event_type_micro_accuracy 0.0
wandb:       tuning_event_type_weighted_AUROC 0.0
wandb:    tuning_event_type_weighted_accuracy 0.0
wandb:                   tuning_feature_0_MSE 4.19338
wandb:               tuning_feature_0_reg_NLL 1.61019
wandb:                  tuning_feature_10_MSE 70.58034
wandb:              tuning_feature_10_reg_NLL 3.04774
wandb:                  tuning_feature_11_MSE 10.30201
wandb:              tuning_feature_11_reg_NLL 2.10097
wandb:                  tuning_feature_12_MSE 5.15468
wandb:              tuning_feature_12_reg_NLL 1.66951
wandb:                  tuning_feature_13_MSE 57.32088
wandb:              tuning_feature_13_reg_NLL 3.0657
wandb:                  tuning_feature_14_MSE 17.5428
wandb:              tuning_feature_14_reg_NLL 2.18629
wandb:                  tuning_feature_15_MSE 3.34939
wandb:              tuning_feature_15_reg_NLL 1.51377
wandb:                  tuning_feature_16_MSE 6.49645
wandb:              tuning_feature_16_reg_NLL 1.85457
wandb:                  tuning_feature_17_MSE 3.88244
wandb:              tuning_feature_17_reg_NLL 1.59551
wandb:                  tuning_feature_18_MSE 1.90251
wandb:              tuning_feature_18_reg_NLL 1.47763
wandb:                  tuning_feature_19_MSE 2.2935
wandb:              tuning_feature_19_reg_NLL 1.2462
wandb:                   tuning_feature_1_MSE 115.52917
wandb:               tuning_feature_1_reg_NLL 3.28522
wandb:                  tuning_feature_20_MSE 14.12327
wandb:              tuning_feature_20_reg_NLL 2.20822
wandb:                  tuning_feature_21_MSE 158.90419
wandb:              tuning_feature_21_reg_NLL 3.52633
wandb:                  tuning_feature_22_MSE 136.21774
wandb:              tuning_feature_22_reg_NLL 3.38618
wandb:                  tuning_feature_23_MSE 25.76714
wandb:              tuning_feature_23_reg_NLL 2.44326
wandb:                  tuning_feature_24_MSE 44.60989
wandb:              tuning_feature_24_reg_NLL 2.68245
wandb:                   tuning_feature_2_MSE 9.65784
wandb:               tuning_feature_2_reg_NLL 2.04594
wandb:                   tuning_feature_3_MSE 10.35527
wandb:               tuning_feature_3_reg_NLL 2.11074
wandb:                   tuning_feature_4_MSE 63.10651
wandb:               tuning_feature_4_reg_NLL 2.94492
wandb:                   tuning_feature_5_MSE 100.4304
wandb:               tuning_feature_5_reg_NLL 3.31012
wandb:                   tuning_feature_6_MSE 102.8139
wandb:               tuning_feature_6_reg_NLL 3.25021
wandb:                   tuning_feature_7_MSE 38.00324
wandb:               tuning_feature_7_reg_NLL 2.73674
wandb:                   tuning_feature_8_MSE 14.35861
wandb:               tuning_feature_8_reg_NLL 2.26979
wandb:                   tuning_feature_9_MSE 10.40604
wandb:               tuning_feature_9_reg_NLL 2.15942
wandb:                            tuning_loss 291.8251
wandb: 
wandb: üöÄ View run generative_event_stream_transformer at: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/eo9yku0i
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_145445-eo9yku0i/logs

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             Test metric                           DataLoader 0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
           held_out_TTE_MSE                         16173239.0
          held_out_TTE_MSLE                     16.264556884765625
         held_out_TTE_reg_NLL                   6.021440029144287
     held_out_event_label_cls_NLL               1.9572727680206299
 held_out_event_label_macro_accuracy            0.7183555960655212
 held_out_event_label_micro_accuracy            0.7183555960655212
 held_out_event_label_weighted_AUROC            0.3815213441848755
held_out_event_label_weighted_accuracy         0.47393879294395447
     held_out_event_type_cls_NLL               0.007743584457784891
  held_out_event_type_macro_accuracy                   0.0
  held_out_event_type_micro_accuracy                   0.0
  held_out_event_type_weighted_AUROC                   0.0
held_out_event_type_weighted_accuracy                  0.0
        held_out_feature_0_MSE                  6.070583343505859
      held_out_feature_0_reg_NLL                2.0363259315490723
       held_out_feature_10_MSE                  78.93822479248047
     held_out_feature_10_reg_NLL                3.195390462875366
       held_out_feature_11_MSE                  11.455819129943848
     held_out_feature_11_reg_NLL                2.272610664367676
       held_out_feature_12_MSE                  7.242537021636963
     held_out_feature_12_reg_NLL                 2.19447922706604
       held_out_feature_13_MSE                  58.97671890258789
     held_out_feature_13_reg_NLL                3.069612979888916
       held_out_feature_14_MSE                  21.06101417541504
     held_out_feature_14_reg_NLL                2.3935863971710205
       held_out_feature_15_MSE                  3.9348154067993164
     held_out_feature_15_reg_NLL                1.6436984539031982
       held_out_feature_16_MSE                  7.289294242858887
     held_out_feature_16_reg_NLL                1.9359986782073975
       held_out_feature_17_MSE                  4.510895252227783
     held_out_feature_17_reg_NLL                1.7132381200790405
       held_out_feature_18_MSE                  2.426959753036499
     held_out_feature_18_reg_NLL                2.247734785079956
       held_out_feature_19_MSE                  2.966562509536743
     held_out_feature_19_reg_NLL                1.4967076778411865
        held_out_feature_1_MSE                  120.01142883300781
      held_out_feature_1_reg_NLL                3.3155109882354736
       held_out_feature_20_MSE                  15.823772430419922
     held_out_feature_20_reg_NLL                2.3136825561523438
       held_out_feature_21_MSE                  159.7765350341797
     held_out_feature_21_reg_NLL                3.5286169052124023
       held_out_feature_22_MSE                  138.09506225585938
     held_out_feature_22_reg_NLL                3.3265862464904785
       held_out_feature_23_MSE                  24.719236373901367
     held_out_feature_23_reg_NLL                2.4800703525543213
       held_out_feature_24_MSE                  34.64236831665039
     held_out_feature_24_reg_NLL                2.6529862880706787
        held_out_feature_2_MSE                  12.255764961242676
      held_out_feature_2_reg_NLL                2.256843328475952
        held_out_feature_3_MSE                  10.60252571105957
      held_out_feature_3_reg_NLL                2.2517480850219727
        held_out_feature_4_MSE                  60.967994689941406
      held_out_feature_4_reg_NLL                2.9810030460357666
        held_out_feature_5_MSE                   108.198486328125
      held_out_feature_5_reg_NLL                3.3521108627319336
        held_out_feature_6_MSE                  109.96964263916016
      held_out_feature_6_reg_NLL                3.2773756980895996
        held_out_feature_7_MSE                  34.719879150390625
      held_out_feature_7_reg_NLL                2.7168569564819336
        held_out_feature_8_MSE                  16.204561233520508
      held_out_feature_8_reg_NLL                2.3573427200317383
        held_out_feature_9_MSE                   9.53802490234375
      held_out_feature_9_reg_NLL                2.1068291664123535
            held_out_loss                       712.1630249023438
              task_AUROC                               0.0
              task_loss                         641.0596923828125
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Saving final metrics...
2025-04-10 14:56:33,226 - wandb.wandb_agent - INFO - Cleaning up finished run: eo9yku0i
[2025-04-10 14:56:33,226][wandb.wandb_agent][INFO] - Cleaning up finished run: eo9yku0i
2025-04-10 14:56:34,183 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 14:56:34,183][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 14:56:34,184 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.346492769606475
	config.input_dropout: 0.3989420608443606
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.25421318997129616
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 40
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0542690379224201
	optimization_config.init_lr: 5.574218251270788e-05
	optimization_config.lr_decay_power: 2.1018353444568287
	optimization_config.lr_frac_warmup_steps: 3.702039496317427e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0042219876706900555
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 14:56:34,184][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.346492769606475
	config.input_dropout: 0.3989420608443606
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.25421318997129616
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 40
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0542690379224201
	optimization_config.init_lr: 5.574218251270788e-05
	optimization_config.lr_decay_power: 2.1018353444568287
	optimization_config.lr_frac_warmup_steps: 3.702039496317427e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0042219876706900555
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 14:56:34,195 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.346492769606475 config.input_dropout=0.3989420608443606 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.25421318997129616 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=40 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0542690379224201 optimization_config.init_lr=5.574218251270788e-05 optimization_config.lr_decay_power=2.1018353444568287 optimization_config.lr_frac_warmup_steps=3.702039496317427e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0042219876706900555 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 14:56:34,195][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.346492769606475 config.input_dropout=0.3989420608443606 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.25421318997129616 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=40 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0542690379224201 optimization_config.init_lr=5.574218251270788e-05 optimization_config.lr_decay_power=2.1018353444568287 optimization_config.lr_frac_warmup_steps=3.702039496317427e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0042219876706900555 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 14:56:39,207 - wandb.wandb_agent - INFO - Running runs: ['yui8k0vp']
[2025-04-10 14:56:39,207][wandb.wandb_agent][INFO] - Running runs: ['yui8k0vp']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_145641-yui8k0vp
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/yui8k0vp
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.346492769606475
Overwriting input_dropout in config from 0.4494236115512016 to 0.3989420608443606
Overwriting resid_dropout in config from 0.4939188761966135 to 0.25421318997129616
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.346492769606475
Overwriting input_dropout in config from 0.4494236115512016 to 0.3989420608443606
Overwriting resid_dropout in config from 0.4939188761966135 to 0.25421318997129616
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.33it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/3 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.62it/s]Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.62it/s, v_num=k0vp]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.77it/s, v_num=k0vp]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.77it/s, v_num=k0vp]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.05it/s, v_num=k0vp]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.05it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.46it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.81it/s, v_num=k0vp]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.81it/s, v_num=k0vp]Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 1:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.00it/s, v_num=k0vp]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.00it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.40it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 1:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 2:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.61it/s, v_num=k0vp]Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.61it/s, v_num=k0vp]Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.76it/s, v_num=k0vp]Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.76it/s, v_num=k0vp]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.04it/s, v_num=k0vp]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.04it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.65it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.80it/s, v_num=k0vp]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.80it/s, v_num=k0vp]Epoch 2:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 3:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.59it/s, v_num=k0vp]Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.59it/s, v_num=k0vp]Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.01it/s, v_num=k0vp]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.01it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.39it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.79it/s, v_num=k0vp]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 3:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 4:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 4:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.56it/s, v_num=k0vp]Epoch 4:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.56it/s, v_num=k0vp]Epoch 4:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 4:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.45it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 4:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 5:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 5:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.54it/s, v_num=k0vp]Epoch 5:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.54it/s, v_num=k0vp]Epoch 5:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.70it/s, v_num=k0vp]Epoch 5:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.70it/s, v_num=k0vp]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.96it/s, v_num=k0vp]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.96it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.58it/s][A
                                                                      [AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.75it/s, v_num=k0vp]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.75it/s, v_num=k0vp]Epoch 5:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 6:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 6:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.55it/s, v_num=k0vp]Epoch 6:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.55it/s, v_num=k0vp]Epoch 6:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 6:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s][A
                                                                      [AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 6:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 7:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 7:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.59it/s, v_num=k0vp]Epoch 7:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.59it/s, v_num=k0vp]Epoch 7:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 7:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.00it/s, v_num=k0vp]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.00it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.58it/s][A
                                                                      [AEpoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 7:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 8:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 8:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 8:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 8:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 8:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.01it/s, v_num=k0vp]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.01it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.38it/s][A
                                                                      [AEpoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.79it/s, v_num=k0vp]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.79it/s, v_num=k0vp]Epoch 8:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 9:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 9:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 9:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 9:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 9:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.00it/s, v_num=k0vp]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.00it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s][A
                                                                      [AEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 9:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 10:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 10:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.54it/s, v_num=k0vp]Epoch 10:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.54it/s, v_num=k0vp]Epoch 10:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.70it/s, v_num=k0vp]Epoch 10:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.70it/s, v_num=k0vp]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.97it/s, v_num=k0vp]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.97it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.41it/s][A
                                                                      [AEpoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.75it/s, v_num=k0vp]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.75it/s, v_num=k0vp]Epoch 10:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 11:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 11:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 11:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 11:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 11:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.44it/s][A
                                                                      [AEpoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 11:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 12:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 12:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 12:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 12:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 12:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.44it/s][A
                                                                      [AEpoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 12:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 13:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 13:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 13:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 13:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 13:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s][A
                                                                      [AEpoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 13:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 14:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 14:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.56it/s, v_num=k0vp]Epoch 14:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.56it/s, v_num=k0vp]Epoch 14:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 14:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.62it/s][A
                                                                      [AEpoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 14:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 15:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 15:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 15:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 15:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 15:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.01it/s, v_num=k0vp]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.01it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.66it/s][A
                                                                      [AEpoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 15:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 16:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 16:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 16:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 16:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.75it/s, v_num=k0vp]Epoch 16:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.75it/s, v_num=k0vp]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.02it/s, v_num=k0vp]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.02it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s][A
                                                                      [AEpoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.79it/s, v_num=k0vp]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.79it/s, v_num=k0vp]Epoch 16:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 17:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 17:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.59it/s, v_num=k0vp]Epoch 17:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.59it/s, v_num=k0vp]Epoch 17:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 17:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.02it/s, v_num=k0vp]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.02it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.41it/s][A
                                                                      [AEpoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.79it/s, v_num=k0vp]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.79it/s, v_num=k0vp]Epoch 17:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 18:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 18:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.55it/s, v_num=k0vp]Epoch 18:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.55it/s, v_num=k0vp]Epoch 18:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.70it/s, v_num=k0vp]Epoch 18:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.70it/s, v_num=k0vp]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.97it/s, v_num=k0vp]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.97it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.64it/s][A
                                                                      [AEpoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 18:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 19:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 19:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 19:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 19:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 19:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.02it/s, v_num=k0vp]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.02it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s][A
                                                                      [AEpoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 19:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 20:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 20:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.59it/s, v_num=k0vp]Epoch 20:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.59it/s, v_num=k0vp]Epoch 20:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 20:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.74it/s, v_num=k0vp]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.01it/s, v_num=k0vp]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.01it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.58it/s][A
                                                                      [AEpoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 20:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 21:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 21:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.55it/s, v_num=k0vp]Epoch 21:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.54it/s, v_num=k0vp]Epoch 21:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 21:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.67it/s][A
                                                                      [AEpoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 21:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 22:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 22:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 22:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 22:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 22:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.00it/s, v_num=k0vp]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.00it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.63it/s][A
                                                                      [AEpoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 22:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 23:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 23:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.54it/s, v_num=k0vp]Epoch 23:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.54it/s, v_num=k0vp]Epoch 23:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 23:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s][A
                                                                      [AEpoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 23:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 24:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 24:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.56it/s, v_num=k0vp]Epoch 24:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.56it/s, v_num=k0vp]Epoch 24:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 24:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.00it/s, v_num=k0vp]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.00it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.43it/s][A
                                                                      [AEpoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 24:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 25:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 25:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.55it/s, v_num=k0vp]Epoch 25:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.55it/s, v_num=k0vp]Epoch 25:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 25:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s][A
                                                                      [AEpoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 25:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 26:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 26:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.60it/s, v_num=k0vp]Epoch 26:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.60it/s, v_num=k0vp]Epoch 26:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.75it/s, v_num=k0vp]Epoch 26:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.75it/s, v_num=k0vp]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.03it/s, v_num=k0vp]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.03it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.65it/s][A
                                                                      [AEpoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.80it/s, v_num=k0vp]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.80it/s, v_num=k0vp]Epoch 26:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 27:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 27:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 27:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.58it/s, v_num=k0vp]Epoch 27:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 27:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.00it/s, v_num=k0vp]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.99it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s][A
                                                                      [AEpoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.77it/s, v_num=k0vp]Epoch 27:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 28:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 28:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 28:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.57it/s, v_num=k0vp]Epoch 28:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 28:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.73it/s, v_num=k0vp]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.00it/s, v_num=k0vp]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.00it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.52it/s][A
                                                                      [AEpoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.78it/s, v_num=k0vp]Epoch 28:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 29:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 29:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.53it/s, v_num=k0vp]Epoch 29:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.53it/s, v_num=k0vp]Epoch 29:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.69it/s, v_num=k0vp]Epoch 29:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.69it/s, v_num=k0vp]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.96it/s, v_num=k0vp]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.96it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.51it/s][A
                                                                      [AEpoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  0.75it/s, v_num=k0vp]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  0.75it/s, v_num=k0vp]Epoch 29:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 30:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 30:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.54it/s, v_num=k0vp]Epoch 30:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.54it/s, v_num=k0vp]Epoch 30:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.69it/s, v_num=k0vp]Epoch 30:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.69it/s, v_num=k0vp]Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.95it/s, v_num=k0vp]Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.95it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.51it/s][A
                                                                      [AEpoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 30:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 31:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 31:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.56it/s, v_num=k0vp]Epoch 31:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.56it/s, v_num=k0vp]Epoch 31:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 31:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.71it/s, v_num=k0vp]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.98it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.45it/s][A
                                                                      [AEpoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  0.76it/s, v_num=k0vp]Epoch 31:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]        Epoch 32:   0%|          | 0/3 [00:00<?, ?it/s, v_num=k0vp]Epoch 32:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.55it/s, v_num=k0vp]Epoch 32:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  0.55it/s, v_num=k0vp]Epoch 32:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 32:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  0.72it/s, v_num=k0vp]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.00it/s, v_num=k0vp]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.00it/s, v_num=k0vp]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s][A[rank: 1] Child process with PID 1205694 terminated with code -9. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 14:59:03,908 - wandb.wandb_agent - INFO - Cleaning up finished run: yui8k0vp
[2025-04-10 14:59:03,908][wandb.wandb_agent][INFO] - Cleaning up finished run: yui8k0vp
2025-04-10 14:59:04,428 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 14:59:04,428][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 14:59:04,429 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.23390477906564716
	config.input_dropout: 0.03237628242281204
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.16008045669538296
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 57
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0032381335236616213
	optimization_config.init_lr: 0.00024155558965937652
	optimization_config.lr_decay_power: 1.784716629769616
	optimization_config.lr_frac_warmup_steps: 3.492304418260483e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.003437167552879635
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 14:59:04,429][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.23390477906564716
	config.input_dropout: 0.03237628242281204
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.16008045669538296
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 57
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0032381335236616213
	optimization_config.init_lr: 0.00024155558965937652
	optimization_config.lr_decay_power: 1.784716629769616
	optimization_config.lr_frac_warmup_steps: 3.492304418260483e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.003437167552879635
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 14:59:04,440 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.23390477906564716 config.input_dropout=0.03237628242281204 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.16008045669538296 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=57 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0032381335236616213 optimization_config.init_lr=0.00024155558965937652 optimization_config.lr_decay_power=1.784716629769616 optimization_config.lr_frac_warmup_steps=3.492304418260483e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.003437167552879635 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 14:59:04,440][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.23390477906564716 config.input_dropout=0.03237628242281204 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.16008045669538296 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=57 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0032381335236616213 optimization_config.init_lr=0.00024155558965937652 optimization_config.lr_decay_power=1.784716629769616 optimization_config.lr_frac_warmup_steps=3.492304418260483e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.003437167552879635 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 14:59:09,454 - wandb.wandb_agent - INFO - Running runs: ['m26axrn4']
[2025-04-10 14:59:09,454][wandb.wandb_agent][INFO] - Running runs: ['m26axrn4']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_145911-m26axrn4
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/m26axrn4
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.23390477906564716
Overwriting input_dropout in config from 0.4494236115512016 to 0.03237628242281204
Overwriting resid_dropout in config from 0.4939188761966135 to 0.16008045669538296
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.23390477906564716
Overwriting input_dropout in config from 0.4494236115512016 to 0.03237628242281204
Overwriting resid_dropout in config from 0.4939188761966135 to 0.16008045669538296
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.32it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.46it/s]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.46it/s, v_num=xrn4]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.70it/s, v_num=xrn4]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.70it/s, v_num=xrn4]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.70it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=xrn4]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=xrn4]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]        Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=xrn4]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=xrn4]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=xrn4]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=xrn4]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.71it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=xrn4]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=xrn4]Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]        Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.44it/s, v_num=xrn4]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.44it/s, v_num=xrn4]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.69it/s, v_num=xrn4]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  0.69it/s, v_num=xrn4]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=xrn4]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.53it/s, v_num=xrn4]Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]        Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=xrn4]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=xrn4]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.64it/s, v_num=xrn4]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.64it/s, v_num=xrn4]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.50it/s, v_num=xrn4]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.50it/s, v_num=xrn4]Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]        Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.40it/s, v_num=xrn4]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.40it/s, v_num=xrn4]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.62it/s, v_num=xrn4]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.62it/s, v_num=xrn4]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.38it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  0.50it/s, v_num=xrn4]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  0.49it/s, v_num=xrn4]Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]        Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=xrn4]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.42it/s, v_num=xrn4]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=xrn4]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.65it/s, v_num=xrn4]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.66it/s][A
                                                                      [AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.51it/s, v_num=xrn4]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.50it/s, v_num=xrn4]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]        Epoch 6:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]Epoch 6:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=xrn4]Epoch 6:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  0.43it/s, v_num=xrn4][rank: 1] Received SIGTERM: 15
Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=xrn4]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.66it/s, v_num=xrn4]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.47it/s][A
                                                                      [AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=xrn4]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  0.52it/s, v_num=xrn4]Epoch 6:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]        Epoch 7:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xrn4]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
2025-04-10 15:28:22,387 - wandb.wandb_agent - INFO - Cleaning up finished run: m26axrn4
[2025-04-10 15:28:22,387][wandb.wandb_agent][INFO] - Cleaning up finished run: m26axrn4
2025-04-10 15:28:22,820 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:28:22,820][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:28:22,820 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.2620636675178967
	config.input_dropout: 0.15634879088865117
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.0051741375363598685
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 47
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00034610302924893303
	optimization_config.init_lr: 0.0004849575917543773
	optimization_config.lr_decay_power: 0.7734558140044592
	optimization_config.lr_frac_warmup_steps: 0.0002442972424891727
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.008337451616865572
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:28:22,820][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.2620636675178967
	config.input_dropout: 0.15634879088865117
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.0051741375363598685
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 47
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00034610302924893303
	optimization_config.init_lr: 0.0004849575917543773
	optimization_config.lr_decay_power: 0.7734558140044592
	optimization_config.lr_frac_warmup_steps: 0.0002442972424891727
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.008337451616865572
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:28:22,832 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2620636675178967 config.input_dropout=0.15634879088865117 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.0051741375363598685 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=47 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00034610302924893303 optimization_config.init_lr=0.0004849575917543773 optimization_config.lr_decay_power=0.7734558140044592 optimization_config.lr_frac_warmup_steps=0.0002442972424891727 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.008337451616865572 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:28:22,832][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2620636675178967 config.input_dropout=0.15634879088865117 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.0051741375363598685 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=47 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00034610302924893303 optimization_config.init_lr=0.0004849575917543773 optimization_config.lr_decay_power=0.7734558140044592 optimization_config.lr_frac_warmup_steps=0.0002442972424891727 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.008337451616865572 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-10 15:28:27,844 - wandb.wandb_agent - INFO - Running runs: ['vbeuxraf']
[2025-04-10 15:28:27,844][wandb.wandb_agent][INFO] - Running runs: ['vbeuxraf']
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_152830-vbeuxraf
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/vbeuxraf
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.2620636675178967
Overwriting input_dropout in config from 0.4494236115512016 to 0.15634879088865117
Overwriting resid_dropout in config from 0.4939188761966135 to 0.0051741375363598685
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.2620636675178967
Overwriting input_dropout in config from 0.4494236115512016 to 0.15634879088865117
Overwriting resid_dropout in config from 0.4939188761966135 to 0.0051741375363598685
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.82it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.22it/s]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.22it/s, v_num=xraf]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  0.30it/s, v_num=xraf]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  0.30it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.65it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.26it/s, v_num=xraf]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.26it/s, v_num=xraf]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.23it/s, v_num=xraf]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.23it/s, v_num=xraf]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  0.29it/s, v_num=xraf]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  0.29it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.05it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.23it/s, v_num=xraf]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.23it/s, v_num=xraf]Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.21it/s, v_num=xraf]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.21it/s, v_num=xraf]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s, v_num=xraf]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.20it/s, v_num=xraf]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.20it/s, v_num=xraf]Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  0.11it/s, v_num=xraf]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  0.11it/s, v_num=xraf]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:14<00:00,  0.14it/s, v_num=xraf]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:14<00:00,  0.14it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.39it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:17<00:00,  0.12it/s, v_num=xraf]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:17<00:00,  0.12it/s, v_num=xraf]Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  0.17it/s, v_num=xraf]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  0.17it/s, v_num=xraf]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:16<00:00,  0.12it/s, v_num=xraf]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:16<00:00,  0.12it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.75it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:18<00:00,  0.11it/s, v_num=xraf]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:18<00:00,  0.11it/s, v_num=xraf]Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  0.18it/s, v_num=xraf]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  0.18it/s, v_num=xraf]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  0.17it/s, v_num=xraf]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  0.17it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.80it/s][A
                                                                      [AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  0.13it/s, v_num=xraf]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  0.13it/s, v_num=xraf]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 6:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 6:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  0.16it/s, v_num=xraf]Epoch 6:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  0.16it/s, v_num=xraf]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.20it/s, v_num=xraf]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.20it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.70it/s][A
                                                                      [AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.15it/s, v_num=xraf]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.15it/s, v_num=xraf]Epoch 6:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 7:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 7:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  0.13it/s, v_num=xraf]Epoch 7:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  0.13it/s, v_num=xraf]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  0.17it/s, v_num=xraf]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  0.17it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.49it/s][A
                                                                      [AEpoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:14<00:00,  0.14it/s, v_num=xraf]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:14<00:00,  0.14it/s, v_num=xraf]Epoch 7:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 8:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 8:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  0.16it/s, v_num=xraf]Epoch 8:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  0.16it/s, v_num=xraf]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  0.20it/s, v_num=xraf]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  0.20it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.78it/s][A
                                                                      [AEpoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.15it/s, v_num=xraf]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.15it/s, v_num=xraf]Epoch 8:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 9:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 9:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  0.17it/s, v_num=xraf]Epoch 9:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  0.17it/s, v_num=xraf]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.20it/s, v_num=xraf]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.20it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.82it/s][A
                                                                      [AEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.15it/s, v_num=xraf]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.15it/s, v_num=xraf]Epoch 9:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.20it/s, v_num=xraf]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.20it/s, v_num=xraf]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.22it/s, v_num=xraf]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.22it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.82it/s][A
                                                                      [AEpoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  0.16it/s, v_num=xraf]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  0.16it/s, v_num=xraf]Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 11:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 11:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  0.15it/s, v_num=xraf]Epoch 11:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  0.15it/s, v_num=xraf]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  0.19it/s, v_num=xraf]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  0.19it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.68it/s][A
                                                                      [AEpoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  0.16it/s, v_num=xraf]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  0.16it/s, v_num=xraf]Epoch 11:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 12:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 12:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  0.13it/s, v_num=xraf]Epoch 12:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  0.13it/s, v_num=xraf]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  0.18it/s, v_num=xraf]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  0.18it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.66it/s][A
                                                                      [AEpoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  0.15it/s, v_num=xraf]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.15it/s, v_num=xraf]Epoch 12:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 13:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 13:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.22it/s, v_num=xraf]Epoch 13:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.22it/s, v_num=xraf]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.28it/s, v_num=xraf]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.28it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.99it/s][A
                                                                      [AEpoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.25it/s, v_num=xraf]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.25it/s, v_num=xraf]Epoch 13:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 14:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 14:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.22it/s, v_num=xraf]Epoch 14:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.22it/s, v_num=xraf]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.28it/s, v_num=xraf]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.28it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.12it/s][A
                                                                      [AEpoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.23it/s, v_num=xraf]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.23it/s, v_num=xraf]Epoch 14:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  0.27it/s, v_num=xraf]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  0.27it/s, v_num=xraf]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  0.31it/s, v_num=xraf]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  0.31it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.13it/s][A
                                                                      [AEpoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.23it/s, v_num=xraf]Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.23it/s, v_num=xraf]Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 16:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 16:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  0.16it/s, v_num=xraf]Epoch 16:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  0.16it/s, v_num=xraf]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  0.17it/s, v_num=xraf]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  0.17it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.70it/s][A
                                                                      [AEpoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.14it/s, v_num=xraf]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.14it/s, v_num=xraf]Epoch 16:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 17:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 17:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.22it/s, v_num=xraf]Epoch 17:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.22it/s, v_num=xraf]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s, v_num=xraf]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.11it/s][A
                                                                      [AEpoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.22it/s, v_num=xraf]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  0.22it/s, v_num=xraf]Epoch 17:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 18:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 18:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.23it/s, v_num=xraf]Epoch 18:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.23it/s, v_num=xraf]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.28it/s, v_num=xraf]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.16it/s][A
                                                                      [AEpoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.24it/s, v_num=xraf]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.24it/s, v_num=xraf]Epoch 18:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 19:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 19:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  0.31it/s, v_num=xraf]Epoch 19:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  0.31it/s, v_num=xraf]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  0.42it/s, v_num=xraf]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  0.42it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.04it/s][A
                                                                      [AEpoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  0.35it/s, v_num=xraf]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  0.35it/s, v_num=xraf]Epoch 19:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 20:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  0.30it/s, v_num=xraf]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  0.30it/s, v_num=xraf]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  0.33it/s, v_num=xraf]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  0.33it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.06it/s][A
                                                                      [AEpoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s, v_num=xraf]Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s, v_num=xraf]Epoch 20:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 21:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 21:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  0.28it/s, v_num=xraf]Epoch 21:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  0.28it/s, v_num=xraf]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.23it/s, v_num=xraf]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  0.23it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.14it/s][A
                                                                      [AEpoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  0.19it/s, v_num=xraf]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  0.19it/s, v_num=xraf]Epoch 21:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 22:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 22:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.23it/s, v_num=xraf]Epoch 22:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.23it/s, v_num=xraf]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  0.32it/s, v_num=xraf]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  0.32it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.99it/s][A
                                                                      [AEpoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s, v_num=xraf]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s, v_num=xraf]Epoch 22:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]        Epoch 23:   0%|          | 0/2 [00:00<?, ?it/s, v_num=xraf]Epoch 23:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:08<00:08,  0.11it/s, v_num=xraf]Epoch 23:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:08<00:08,  0.11it/s, v_num=xraf]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.15it/s, v_num=xraf]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  0.15it/s, v_num=xraf]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.81it/s][A[rank: 1] Child process with PID 1215312 terminated with code -9. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:33:17,351 - wandb.wandb_agent - INFO - Cleaning up finished run: vbeuxraf
[2025-04-10 15:33:17,351][wandb.wandb_agent][INFO] - Cleaning up finished run: vbeuxraf
2025-04-10 15:33:17,798 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:33:17,798][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:33:17,798 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.0799387039973467
	config.input_dropout: 0.07327837208711757
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.3998836401754587
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 11
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.600930880070185
	optimization_config.init_lr: 0.003198603112644656
	optimization_config.lr_decay_power: 4.278195224567634
	optimization_config.lr_frac_warmup_steps: 0.01049177000562282
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.003916439644077453
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:33:17,798][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.0799387039973467
	config.input_dropout: 0.07327837208711757
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.3998836401754587
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 11
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.600930880070185
	optimization_config.init_lr: 0.003198603112644656
	optimization_config.lr_decay_power: 4.278195224567634
	optimization_config.lr_frac_warmup_steps: 0.01049177000562282
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.003916439644077453
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:33:17,804 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.0799387039973467 config.input_dropout=0.07327837208711757 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.3998836401754587 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=11 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.600930880070185 optimization_config.init_lr=0.003198603112644656 optimization_config.lr_decay_power=4.278195224567634 optimization_config.lr_frac_warmup_steps=0.01049177000562282 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.003916439644077453 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:33:17,804][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.0799387039973467 config.input_dropout=0.07327837208711757 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.3998836401754587 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=11 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.600930880070185 optimization_config.init_lr=0.003198603112644656 optimization_config.lr_decay_power=4.278195224567634 optimization_config.lr_frac_warmup_steps=0.01049177000562282 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.003916439644077453 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-10 15:33:22,816 - wandb.wandb_agent - INFO - Running runs: ['6zblkv4c']
[2025-04-10 15:33:22,816][wandb.wandb_agent][INFO] - Running runs: ['6zblkv4c']
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_153325-6zblkv4c
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/6zblkv4c
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.0799387039973467
Overwriting input_dropout in config from 0.4494236115512016 to 0.07327837208711757
Overwriting resid_dropout in config from 0.4939188761966135 to 0.3998836401754587
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.0799387039973467
Overwriting input_dropout in config from 0.4494236115512016 to 0.07327837208711757
Overwriting resid_dropout in config from 0.4939188761966135 to 0.3998836401754587
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.26it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/8 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/8 [00:00<?, ?it/s] Epoch 0:  12%|‚ñà‚ñé        | 1/8 [00:00<00:05,  1.28it/s]Epoch 0:  12%|‚ñà‚ñé        | 1/8 [00:00<00:05,  1.28it/s, v_num=kv4c]Epoch 0:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:01<00:03,  1.63it/s, v_num=kv4c]Epoch 0:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:01<00:03,  1.63it/s, v_num=kv4c]Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:01<00:02,  1.80it/s, v_num=kv4c]Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:01<00:02,  1.80it/s, v_num=kv4c]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:02<00:02,  1.88it/s, v_num=kv4c]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:02<00:02,  1.88it/s, v_num=kv4c]Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:02<00:01,  1.93it/s, v_num=kv4c]Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:02<00:01,  1.93it/s, v_num=kv4c]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:03<00:01,  1.99it/s, v_num=kv4c]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:03<00:01,  1.99it/s, v_num=kv4c]Epoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:03<00:00,  2.00it/s, v_num=kv4c]Epoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:03<00:00,  2.00it/s, v_num=kv4c]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03<00:00,  2.09it/s, v_num=kv4c]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03<00:00,  2.09it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.36it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04<00:00,  1.68it/s, v_num=kv4c]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04<00:00,  1.67it/s, v_num=kv4c]Epoch 0:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 1:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 1:  12%|‚ñà‚ñé        | 1/8 [00:01<00:07,  0.99it/s, v_num=kv4c]Epoch 1:  12%|‚ñà‚ñé        | 1/8 [00:01<00:07,  0.99it/s, v_num=kv4c]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:01<00:04,  1.31it/s, v_num=kv4c]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:01<00:04,  1.30it/s, v_num=kv4c]Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:02<00:03,  1.47it/s, v_num=kv4c]Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:02<00:03,  1.47it/s, v_num=kv4c]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:02<00:02,  1.59it/s, v_num=kv4c]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:02<00:02,  1.59it/s, v_num=kv4c]Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:03<00:02,  1.42it/s, v_num=kv4c]Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:03<00:02,  1.42it/s, v_num=kv4c]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:04<00:01,  1.47it/s, v_num=kv4c]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:04<00:01,  1.47it/s, v_num=kv4c]Epoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:04<00:00,  1.53it/s, v_num=kv4c]Epoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:04<00:00,  1.53it/s, v_num=kv4c]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:05<00:00,  1.55it/s, v_num=kv4c]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:05<00:00,  1.55it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.16it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.21it/s, v_num=kv4c]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.21it/s, v_num=kv4c]Epoch 1:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 2:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 2:  12%|‚ñà‚ñé        | 1/8 [00:01<00:11,  0.62it/s, v_num=kv4c]Epoch 2:  12%|‚ñà‚ñé        | 1/8 [00:01<00:11,  0.62it/s, v_num=kv4c]Epoch 2:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:02<00:08,  0.74it/s, v_num=kv4c]Epoch 2:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:02<00:08,  0.74it/s, v_num=kv4c]Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:06,  0.83it/s, v_num=kv4c]Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:06,  0.83it/s, v_num=kv4c]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:05,  0.72it/s, v_num=kv4c]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:05,  0.72it/s, v_num=kv4c]Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:03,  0.79it/s, v_num=kv4c]Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:03,  0.79it/s, v_num=kv4c]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:07<00:02,  0.79it/s, v_num=kv4c]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:07<00:02,  0.79it/s, v_num=kv4c]Epoch 2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:08<00:01,  0.78it/s, v_num=kv4c]Epoch 2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:08<00:01,  0.78it/s, v_num=kv4c]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  0.82it/s, v_num=kv4c]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  0.82it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.77it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  0.68it/s, v_num=kv4c]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  0.68it/s, v_num=kv4c]Epoch 2:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 3:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 3:  12%|‚ñà‚ñé        | 1/8 [00:02<00:14,  0.47it/s, v_num=kv4c]Epoch 3:  12%|‚ñà‚ñé        | 1/8 [00:02<00:14,  0.47it/s, v_num=kv4c]Epoch 3:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:03<00:10,  0.56it/s, v_num=kv4c]Epoch 3:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:03<00:10,  0.56it/s, v_num=kv4c]Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:04<00:08,  0.62it/s, v_num=kv4c]Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:04<00:08,  0.62it/s, v_num=kv4c]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:06,  0.61it/s, v_num=kv4c]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:06,  0.61it/s, v_num=kv4c]Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:07<00:04,  0.69it/s, v_num=kv4c]Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:07<00:04,  0.69it/s, v_num=kv4c]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:02,  0.72it/s, v_num=kv4c]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:02,  0.72it/s, v_num=kv4c]Epoch 3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:09<00:01,  0.73it/s, v_num=kv4c]Epoch 3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:09<00:01,  0.73it/s, v_num=kv4c]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:10<00:00,  0.77it/s, v_num=kv4c]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:10<00:00,  0.77it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.82it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:12<00:00,  0.65it/s, v_num=kv4c]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:12<00:00,  0.65it/s, v_num=kv4c]Epoch 3:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 4:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 4:  12%|‚ñà‚ñé        | 1/8 [00:02<00:20,  0.34it/s, v_num=kv4c]Epoch 4:  12%|‚ñà‚ñé        | 1/8 [00:02<00:20,  0.34it/s, v_num=kv4c]Epoch 4:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:04<00:12,  0.47it/s, v_num=kv4c]Epoch 4:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:04<00:12,  0.47it/s, v_num=kv4c]Epoch 4:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:09,  0.53it/s, v_num=kv4c]Epoch 4:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:09,  0.53it/s, v_num=kv4c]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:08<00:08,  0.49it/s, v_num=kv4c]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:08<00:08,  0.49it/s, v_num=kv4c]Epoch 4:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:09<00:05,  0.54it/s, v_num=kv4c]Epoch 4:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:09<00:05,  0.54it/s, v_num=kv4c]Epoch 4:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:10<00:03,  0.56it/s, v_num=kv4c]Epoch 4:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:10<00:03,  0.56it/s, v_num=kv4c]Epoch 4:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:11<00:01,  0.58it/s, v_num=kv4c]Epoch 4:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:11<00:01,  0.58it/s, v_num=kv4c]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:12<00:00,  0.62it/s, v_num=kv4c]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:12<00:00,  0.62it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.73it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:15<00:00,  0.53it/s, v_num=kv4c]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:15<00:00,  0.53it/s, v_num=kv4c]Epoch 4:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 5:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 5:  12%|‚ñà‚ñé        | 1/8 [00:04<00:28,  0.25it/s, v_num=kv4c]Epoch 5:  12%|‚ñà‚ñé        | 1/8 [00:04<00:28,  0.25it/s, v_num=kv4c]Epoch 5:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:16,  0.36it/s, v_num=kv4c]Epoch 5:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:16,  0.36it/s, v_num=kv4c]Epoch 5:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:10,  0.46it/s, v_num=kv4c]Epoch 5:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:10,  0.46it/s, v_num=kv4c]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:07<00:07,  0.53it/s, v_num=kv4c]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:07<00:07,  0.53it/s, v_num=kv4c]Epoch 5:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:08<00:05,  0.56it/s, v_num=kv4c]Epoch 5:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:08<00:05,  0.56it/s, v_num=kv4c]Epoch 5:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:10<00:03,  0.58it/s, v_num=kv4c]Epoch 5:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:10<00:03,  0.58it/s, v_num=kv4c]Epoch 5:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:11<00:01,  0.59it/s, v_num=kv4c]Epoch 5:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:11<00:01,  0.59it/s, v_num=kv4c]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:17<00:00,  0.47it/s, v_num=kv4c]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:17<00:00,  0.47it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.59it/s][A
                                                                      [AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:19<00:00,  0.41it/s, v_num=kv4c]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:19<00:00,  0.41it/s, v_num=kv4c]Epoch 5:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 6:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 6:  12%|‚ñà‚ñé        | 1/8 [00:02<00:15,  0.45it/s, v_num=kv4c]Epoch 6:  12%|‚ñà‚ñé        | 1/8 [00:02<00:15,  0.45it/s, v_num=kv4c]Epoch 6:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:03<00:10,  0.56it/s, v_num=kv4c]Epoch 6:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:03<00:10,  0.56it/s, v_num=kv4c]Epoch 6:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:04<00:07,  0.67it/s, v_num=kv4c]Epoch 6:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:04<00:07,  0.67it/s, v_num=kv4c]Epoch 6:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:05,  0.67it/s, v_num=kv4c]Epoch 6:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:05,  0.67it/s, v_num=kv4c]Epoch 6:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:07<00:04,  0.68it/s, v_num=kv4c]Epoch 6:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:07<00:04,  0.68it/s, v_num=kv4c]Epoch 6:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:02,  0.68it/s, v_num=kv4c]Epoch 6:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:02,  0.68it/s, v_num=kv4c]Epoch 6:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:14<00:02,  0.50it/s, v_num=kv4c]Epoch 6:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:14<00:02,  0.50it/s, v_num=kv4c]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:15<00:00,  0.52it/s, v_num=kv4c]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:15<00:00,  0.52it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.78it/s][A
                                                                      [AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:17<00:00,  0.46it/s, v_num=kv4c]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:17<00:00,  0.46it/s, v_num=kv4c]Epoch 6:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 7:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 7:  12%|‚ñà‚ñé        | 1/8 [00:02<00:19,  0.35it/s, v_num=kv4c]Epoch 7:  12%|‚ñà‚ñé        | 1/8 [00:02<00:19,  0.35it/s, v_num=kv4c]Epoch 7:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:04<00:12,  0.47it/s, v_num=kv4c]Epoch 7:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:04<00:12,  0.47it/s, v_num=kv4c]Epoch 7:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:04<00:08,  0.61it/s, v_num=kv4c]Epoch 7:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:04<00:08,  0.61it/s, v_num=kv4c]Epoch 7:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:06,  0.63it/s, v_num=kv4c]Epoch 7:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:06,  0.63it/s, v_num=kv4c]Epoch 7:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:07<00:04,  0.65it/s, v_num=kv4c]Epoch 7:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:07<00:04,  0.65it/s, v_num=kv4c]Epoch 7:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:09<00:03,  0.65it/s, v_num=kv4c]Epoch 7:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:09<00:03,  0.65it/s, v_num=kv4c]Epoch 7:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:10<00:01,  0.66it/s, v_num=kv4c]Epoch 7:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:10<00:01,  0.66it/s, v_num=kv4c]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  0.68it/s, v_num=kv4c]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  0.68it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.78it/s][A
                                                                      [AEpoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:13<00:00,  0.58it/s, v_num=kv4c]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:13<00:00,  0.58it/s, v_num=kv4c]Epoch 7:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 8:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 8:  12%|‚ñà‚ñé        | 1/8 [00:02<00:14,  0.49it/s, v_num=kv4c]Epoch 8:  12%|‚ñà‚ñé        | 1/8 [00:02<00:14,  0.49it/s, v_num=kv4c]Epoch 8:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:03<00:09,  0.65it/s, v_num=kv4c]Epoch 8:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:03<00:09,  0.65it/s, v_num=kv4c]Epoch 8:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:06,  0.79it/s, v_num=kv4c]Epoch 8:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:06,  0.79it/s, v_num=kv4c]Epoch 8:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:05,  0.76it/s, v_num=kv4c]Epoch 8:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:05,  0.76it/s, v_num=kv4c]Epoch 8:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:03,  0.75it/s, v_num=kv4c]Epoch 8:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:03,  0.75it/s, v_num=kv4c]Epoch 8:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:02,  0.73it/s, v_num=kv4c]Epoch 8:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:02,  0.73it/s, v_num=kv4c]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
Epoch 8:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:11<00:01,  0.63it/s, v_num=kv4c]Epoch 8:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:11<00:01,  0.63it/s, v_num=kv4c]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:12<00:00,  0.62it/s, v_num=kv4c]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:12<00:00,  0.62it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.62it/s][A
                                                                      [AEpoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:15<00:00,  0.52it/s, v_num=kv4c]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:15<00:00,  0.52it/s, v_num=kv4c]Epoch 8:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 9:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 9:  12%|‚ñà‚ñé        | 1/8 [00:02<00:18,  0.39it/s, v_num=kv4c]Epoch 9:  12%|‚ñà‚ñé        | 1/8 [00:02<00:18,  0.39it/s, v_num=kv4c]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
Epoch 9:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:06<00:20,  0.29it/s, v_num=kv4c]Epoch 9:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:06<00:20,  0.29it/s, v_num=kv4c]Epoch 9:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:08<00:13,  0.36it/s, v_num=kv4c]Epoch 9:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:08<00:13,  0.36it/s, v_num=kv4c]Epoch 9:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:13<00:13,  0.29it/s, v_num=kv4c]Epoch 9:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:13<00:13,  0.29it/s, v_num=kv4c]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
Epoch 9:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:15<00:09,  0.32it/s, v_num=kv4c]Epoch 9:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:15<00:09,  0.32it/s, v_num=kv4c]Epoch 9:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:17<00:05,  0.34it/s, v_num=kv4c]Epoch 9:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:17<00:05,  0.34it/s, v_num=kv4c]Epoch 9:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:20<00:02,  0.35it/s, v_num=kv4c]Epoch 9:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:20<00:02,  0.35it/s, v_num=kv4c]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:21<00:00,  0.38it/s, v_num=kv4c]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:21<00:00,  0.38it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.62it/s][A
                                                                      [AEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:24<00:00,  0.33it/s, v_num=kv4c]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:24<00:00,  0.33it/s, v_num=kv4c]Epoch 9:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 10:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
Epoch 10:  12%|‚ñà‚ñé        | 1/8 [00:04<00:31,  0.22it/s, v_num=kv4c]Epoch 10:  12%|‚ñà‚ñé        | 1/8 [00:04<00:31,  0.22it/s, v_num=kv4c]Epoch 10:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:06<00:19,  0.31it/s, v_num=kv4c]Epoch 10:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:06<00:19,  0.31it/s, v_num=kv4c]Epoch 10:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:08<00:13,  0.36it/s, v_num=kv4c]Epoch 10:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:08<00:13,  0.36it/s, v_num=kv4c]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:10<00:10,  0.40it/s, v_num=kv4c]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:10<00:10,  0.40it/s, v_num=kv4c]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
Epoch 10:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:12<00:07,  0.41it/s, v_num=kv4c]Epoch 10:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:12<00:07,  0.41it/s, v_num=kv4c]Epoch 10:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:13<00:04,  0.44it/s, v_num=kv4c]Epoch 10:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:13<00:04,  0.44it/s, v_num=kv4c]Epoch 10:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:15<00:02,  0.46it/s, v_num=kv4c]Epoch 10:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:15<00:02,  0.46it/s, v_num=kv4c]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:16<00:00,  0.50it/s, v_num=kv4c]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:16<00:00,  0.50it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.75it/s][A
                                                                      [AEpoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:18<00:00,  0.44it/s, v_num=kv4c]Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:18<00:00,  0.44it/s, v_num=kv4c]Epoch 10:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 11:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
Epoch 11:  12%|‚ñà‚ñé        | 1/8 [00:02<00:18,  0.37it/s, v_num=kv4c]Epoch 11:  12%|‚ñà‚ñé        | 1/8 [00:02<00:18,  0.37it/s, v_num=kv4c]Epoch 11:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:03<00:11,  0.50it/s, v_num=kv4c]Epoch 11:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:03<00:11,  0.50it/s, v_num=kv4c]Epoch 11:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:09,  0.51it/s, v_num=kv4c]Epoch 11:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:09,  0.51it/s, v_num=kv4c]Epoch 11:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:07<00:07,  0.54it/s, v_num=kv4c]Epoch 11:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:07<00:07,  0.54it/s, v_num=kv4c]Epoch 11:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:08<00:05,  0.58it/s, v_num=kv4c]Epoch 11:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:08<00:05,  0.58it/s, v_num=kv4c]Epoch 11:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:09<00:03,  0.63it/s, v_num=kv4c]Epoch 11:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:09<00:03,  0.63it/s, v_num=kv4c]Epoch 11:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:10<00:01,  0.66it/s, v_num=kv4c]Epoch 11:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:10<00:01,  0.66it/s, v_num=kv4c]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  0.71it/s, v_num=kv4c]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  0.71it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][Awandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.17it/s][A
                                                                      [AEpoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:16<00:00,  0.49it/s, v_num=kv4c]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:16<00:00,  0.49it/s, v_num=kv4c]Epoch 11:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 12:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 12:  12%|‚ñà‚ñé        | 1/8 [00:01<00:11,  0.61it/s, v_num=kv4c]Epoch 12:  12%|‚ñà‚ñé        | 1/8 [00:01<00:11,  0.61it/s, v_num=kv4c]Epoch 12:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:02<00:06,  0.92it/s, v_num=kv4c]Epoch 12:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:02<00:06,  0.92it/s, v_num=kv4c]Epoch 12:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:05,  0.96it/s, v_num=kv4c]Epoch 12:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:05,  0.96it/s, v_num=kv4c]Epoch 12:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:04<00:04,  0.98it/s, v_num=kv4c]Epoch 12:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:04<00:04,  0.98it/s, v_num=kv4c]Epoch 12:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:05<00:03,  0.98it/s, v_num=kv4c]Epoch 12:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:05<00:03,  0.98it/s, v_num=kv4c]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
Epoch 12:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:02,  0.96it/s, v_num=kv4c]Epoch 12:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:02,  0.96it/s, v_num=kv4c]Epoch 12:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:07<00:01,  0.98it/s, v_num=kv4c]Epoch 12:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:07<00:01,  0.98it/s, v_num=kv4c]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.03it/s, v_num=kv4c]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.03it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.17it/s][A
                                                                      [AEpoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  0.87it/s, v_num=kv4c]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  0.87it/s, v_num=kv4c]Epoch 12:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]        Epoch 13:   0%|          | 0/8 [00:00<?, ?it/s, v_num=kv4c]Epoch 13:  12%|‚ñà‚ñé        | 1/8 [00:01<00:08,  0.85it/s, v_num=kv4c]Epoch 13:  12%|‚ñà‚ñé        | 1/8 [00:01<00:08,  0.85it/s, v_num=kv4c]Epoch 13:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:02<00:06,  0.91it/s, v_num=kv4c]Epoch 13:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:02<00:06,  0.91it/s, v_num=kv4c]Epoch 13:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:05,  0.95it/s, v_num=kv4c]Epoch 13:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:05,  0.95it/s, v_num=kv4c]Epoch 13:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:04<00:04,  0.97it/s, v_num=kv4c]Epoch 13:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:04<00:04,  0.97it/s, v_num=kv4c]Epoch 13:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:05<00:03,  0.97it/s, v_num=kv4c]Epoch 13:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:05<00:03,  0.97it/s, v_num=kv4c]Epoch 13:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:02,  0.98it/s, v_num=kv4c]Epoch 13:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:02,  0.98it/s, v_num=kv4c]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
Epoch 13:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:07<00:01,  0.98it/s, v_num=kv4c]Epoch 13:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:07<00:01,  0.98it/s, v_num=kv4c]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.05it/s, v_num=kv4c]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.05it/s, v_num=kv4c]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.62it/s][A
                                                                      [AEpoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  0.91it/s, v_num=kv4c]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  0.91it/s, v_num=kv4c]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  0.90it/s, v_num=kv4c]Restoring states from the checkpoint path at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/6zblkv4c/checkpoints/epoch=5-val_loss=0.00-best_model.ckpt
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/6zblkv4c/checkpoints/epoch=5-val_loss=0.00-best_model.ckpt
/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.validate()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/finetune_weights
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/held_out_0.parquet

/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/finetune_weights
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/held_out_0.parquet
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/1 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.17it/s]Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.94it/s]Restoring states from the checkpoint path at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/6zblkv4c/checkpoints/epoch=5-val_loss=0.00-best_model.ckpt
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/6zblkv4c/checkpoints/epoch=5-val_loss=0.00-best_model.ckpt
/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          Validate metric                       DataLoader 0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             task_AUROC                             0.0
             task_loss                       145.73965454101562
           tuning_TTE_MSE                       33102.796875
          tuning_TTE_MSLE                    3.5725061893463135
         tuning_TTE_reg_NLL                  11.423803329467773
     tuning_event_label_cls_NLL              1.7377784252166748
 tuning_event_label_macro_accuracy           0.6889939308166504
 tuning_event_label_micro_accuracy           0.6889938712120056
 tuning_event_label_weighted_AUROC          0.45623138546943665
tuning_event_label_weighted_accuracy        0.46370774507522583
     tuning_event_type_cls_NLL              0.11557236313819885
  tuning_event_type_macro_accuracy                  0.0
  tuning_event_type_micro_accuracy                  0.0
  tuning_event_type_weighted_AUROC                  0.0
tuning_event_type_weighted_accuracy                 0.0
        tuning_feature_0_MSE                 6.941304683685303
      tuning_feature_0_reg_NLL               0.7168588042259216
       tuning_feature_10_MSE                 5.842156410217285
     tuning_feature_10_reg_NLL               1.7107876539230347
       tuning_feature_11_MSE                 5.862037181854248
     tuning_feature_11_reg_NLL               1.8400360345840454
       tuning_feature_12_MSE                 4.711791038513184
     tuning_feature_12_reg_NLL               1.7850803136825562
       tuning_feature_13_MSE                 7.613142490386963
     tuning_feature_13_reg_NLL               1.9818648099899292
       tuning_feature_14_MSE                 10.519128799438477
     tuning_feature_14_reg_NLL               1.986759066581726
       tuning_feature_15_MSE                 1.8260630369186401
     tuning_feature_15_reg_NLL               1.446951985359192
       tuning_feature_16_MSE                 1.4151300191879272
     tuning_feature_16_reg_NLL               1.3275071382522583
       tuning_feature_17_MSE                 1.7975071668624878
     tuning_feature_17_reg_NLL               1.4105262756347656
       tuning_feature_18_MSE                 5.221513271331787
     tuning_feature_18_reg_NLL               1.7681437730789185
       tuning_feature_19_MSE                 1.2922909259796143
     tuning_feature_19_reg_NLL               1.1710642576217651
        tuning_feature_1_MSE                 7.979642391204834
      tuning_feature_1_reg_NLL               1.0589417219161987
       tuning_feature_20_MSE                 1.0587801933288574
     tuning_feature_20_reg_NLL               1.0886396169662476
       tuning_feature_21_MSE                 1.417711853981018
     tuning_feature_21_reg_NLL               1.2594927549362183
       tuning_feature_22_MSE                 7.713705062866211
     tuning_feature_22_reg_NLL               5.444310188293457
       tuning_feature_23_MSE                 8.482938766479492
     tuning_feature_23_reg_NLL               1.8962502479553223
       tuning_feature_24_MSE                 18.172122955322266
     tuning_feature_24_reg_NLL               2.2286877632141113
        tuning_feature_2_MSE                 5.556295871734619
      tuning_feature_2_reg_NLL               1.6601821184158325
        tuning_feature_3_MSE                 6.625387191772461
      tuning_feature_3_reg_NLL               1.8111228942871094
        tuning_feature_4_MSE                 4.892604351043701
      tuning_feature_4_reg_NLL               1.5258746147155762
        tuning_feature_5_MSE                 2.2512738704681396
      tuning_feature_5_reg_NLL               1.1940420866012573
        tuning_feature_6_MSE                 2.1870620250701904
      tuning_feature_6_reg_NLL               1.4491530656814575
        tuning_feature_7_MSE                 3.515148878097534
      tuning_feature_7_reg_NLL               1.7673015594482422
        tuning_feature_8_MSE                 2.8497393131256104
      tuning_feature_8_reg_NLL               1.6298820972442627
        tuning_feature_9_MSE                 1.7180819511413574
      tuning_feature_9_reg_NLL               1.3724048137664795
            tuning_loss                      201.54867553710938
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.07it/s]Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.97it/s]wandb: Waiting for W&B process to finish... (success).
wandb: ERROR Error while calling W&B API: run 6zblkv4c was previously created and deleted; try a new run name (<Response [409]>)
wandb: ERROR Error while calling W&B API: run 6zblkv4c was previously created and deleted; try a new run name (<Response [409]>)
wandb: ERROR Error while calling W&B API: run 6zblkv4c was previously created and deleted; try a new run name (<Response [409]>)
wandb: ERROR Error while calling W&B API: run 6zblkv4c was previously created and deleted; try a new run name (<Response [409]>)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Error while calling W&B API: run 6zblkv4c was previously created and deleted; try a new run name (<Response [409]>)
wandb: ERROR Error while calling W&B API: run 6zblkv4c was previously created and deleted; try a new run name (<Response [409]>)
wandb: ERROR Error while calling W&B API: run 6zblkv4c was previously created and deleted; try a new run name (<Response [409]>)
wandb: ERROR Error while calling W&B API: run 6zblkv4c was previously created and deleted; try a new run name (<Response [409]>)
Thread SenderThread:
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/apis/normalize.py", line 41, in wrapper
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py", line 2007, in upsert_run
    response = self.gql(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py", line 338, in gql
    ret = self._retry_gql(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/lib/retry.py", line 131, in __call__
    result = self._call_fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py", line 366, in execute
    return self.client.execute(*args, **kwargs)  # type: ignore
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py", line 52, in execute
    result = self._get_result(document, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py", line 60, in _get_result
    return self.transport.execute(document, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/lib/gql_request.py", line 59, in execute
    request.raise_for_status()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://api.wandb.ai/graphql

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py", line 49, in run
    self._run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py", line 100, in _run
    self._process(record)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal.py", line 328, in _process
    self._sm.send(record)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 387, in send
    send_handler(record)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 409, in send_request
    send_handler(record)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 635, in send_request_defer
    self.debounce(final=True)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 538, in debounce
    self._maybe_update_config(always=final)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 515, in _maybe_update_config
    self._debounce_config()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 544, in _debounce_config
    self._api.upsert_run(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/apis/normalize.py", line 51, in wrapper
    raise CommError(message, error)
wandb.errors.CommError: run 6zblkv4c was previously created and deleted; try a new run name (Error 409: Conflict)
wandb: ERROR Internal wandb error: file data was not synced
2025-04-10 15:39:40,285 - wandb.wandb_agent - INFO - Cleaning up finished run: 6zblkv4c
[2025-04-10 15:39:40,285][wandb.wandb_agent][INFO] - Cleaning up finished run: 6zblkv4c
2025-04-10 15:39:40,734 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:39:40,734][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:39:40,734 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.37486875495503774
	config.input_dropout: 0.017898895183906416
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.426800864408163
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 63
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0024557772364067984
	optimization_config.init_lr: 0.0004472992755114919
	optimization_config.lr_decay_power: 1.2953986857162132
	optimization_config.lr_frac_warmup_steps: 0.012211731137277616
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00011119712498201629
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:39:40,734][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.37486875495503774
	config.input_dropout: 0.017898895183906416
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.426800864408163
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 63
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0024557772364067984
	optimization_config.init_lr: 0.0004472992755114919
	optimization_config.lr_decay_power: 1.2953986857162132
	optimization_config.lr_frac_warmup_steps: 0.012211731137277616
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00011119712498201629
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:39:40,741 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.37486875495503774 config.input_dropout=0.017898895183906416 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.426800864408163 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=63 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0024557772364067984 optimization_config.init_lr=0.0004472992755114919 optimization_config.lr_decay_power=1.2953986857162132 optimization_config.lr_frac_warmup_steps=0.012211731137277616 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00011119712498201629 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:39:40,741][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.37486875495503774 config.input_dropout=0.017898895183906416 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.426800864408163 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=63 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0024557772364067984 optimization_config.init_lr=0.0004472992755114919 optimization_config.lr_decay_power=1.2953986857162132 optimization_config.lr_frac_warmup_steps=0.012211731137277616 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00011119712498201629 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:39:45,751 - wandb.wandb_agent - INFO - Running runs: ['k5q8f1nf']
[2025-04-10 15:39:45,751][wandb.wandb_agent][INFO] - Running runs: ['k5q8f1nf']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_153951-k5q8f1nf
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/k5q8f1nf
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.37486875495503774
Overwriting input_dropout in config from 0.4494236115512016 to 0.017898895183906416
Overwriting resid_dropout in config from 0.4939188761966135 to 0.426800864408163
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.37486875495503774
Overwriting input_dropout in config from 0.4494236115512016 to 0.017898895183906416
Overwriting resid_dropout in config from 0.4939188761966135 to 0.426800864408163
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.36it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 158.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 132.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.03 GiB memory in use. Of the allocated memory 633.83 MiB is allocated by PyTorch, and 52.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 158.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 126.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.03 GiB memory in use. Of the allocated memory 632.88 MiB is allocated by PyTorch, and 57.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
EXCEPTION: CUDA out of memory. Tried to allocate 158.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 126.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.03 GiB memory in use. Of the allocated memory 632.88 MiB is allocated by PyTorch, and 57.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.37486875495503774', 'config.input_dropout=0.017898895183906416', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.426800864408163', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=cls', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=63', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.0024557772364067984', 'optimization_config.init_lr=0.0004472992755114919', 'optimization_config.lr_decay_power=1.2953986857162132', 'optimization_config.lr_frac_warmup_steps=0.012211731137277616', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.00011119712498201629', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 158.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 126.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.03 GiB memory in use. Of the allocated memory 632.88 MiB is allocated by PyTorch, and 57.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x73d6885ec1f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
[rank: 1] Child process with PID 1239902 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:40:16,778 - wandb.wandb_agent - INFO - Cleaning up finished run: k5q8f1nf
[2025-04-10 15:40:16,778][wandb.wandb_agent][INFO] - Cleaning up finished run: k5q8f1nf
2025-04-10 15:40:17,150 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:40:17,150][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:40:17,150 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.10386472453599148
	config.input_dropout: 0.10769795720137698
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.25468363871093064
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 61
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.02066454116254354
	optimization_config.init_lr: 0.0001748919663499442
	optimization_config.lr_decay_power: 4.912375873030162
	optimization_config.lr_frac_warmup_steps: 0.14447461437407516
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005896496455247276
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:40:17,150][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.10386472453599148
	config.input_dropout: 0.10769795720137698
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.25468363871093064
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 61
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.02066454116254354
	optimization_config.init_lr: 0.0001748919663499442
	optimization_config.lr_decay_power: 4.912375873030162
	optimization_config.lr_frac_warmup_steps: 0.14447461437407516
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005896496455247276
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:40:17,157 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.10386472453599148 config.input_dropout=0.10769795720137698 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.25468363871093064 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=61 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.02066454116254354 optimization_config.init_lr=0.0001748919663499442 optimization_config.lr_decay_power=4.912375873030162 optimization_config.lr_frac_warmup_steps=0.14447461437407516 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005896496455247276 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:40:17,157][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.10386472453599148 config.input_dropout=0.10769795720137698 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.25468363871093064 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=61 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.02066454116254354 optimization_config.init_lr=0.0001748919663499442 optimization_config.lr_decay_power=4.912375873030162 optimization_config.lr_frac_warmup_steps=0.14447461437407516 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005896496455247276 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:40:22,167 - wandb.wandb_agent - INFO - Running runs: ['4r8r2ws5']
[2025-04-10 15:40:22,167][wandb.wandb_agent][INFO] - Running runs: ['4r8r2ws5']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154026-4r8r2ws5
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/4r8r2ws5
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.10386472453599148
Overwriting input_dropout in config from 0.4494236115512016 to 0.10769795720137698
Overwriting resid_dropout in config from 0.4939188761966135 to 0.25468363871093064
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.10386472453599148
Overwriting input_dropout in config from 0.4494236115512016 to 0.10769795720137698
Overwriting resid_dropout in config from 0.4939188761966135 to 0.25468363871093064
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  0.27it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 154.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 146.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.01 GiB memory in use. Of the allocated memory 611.50 MiB is allocated by PyTorch, and 60.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 154.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 140.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.02 GiB memory in use. Of the allocated memory 611.50 MiB is allocated by PyTorch, and 64.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
EXCEPTION: CUDA out of memory. Tried to allocate 154.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 140.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.02 GiB memory in use. Of the allocated memory 611.50 MiB is allocated by PyTorch, and 64.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.10386472453599148', 'config.input_dropout=0.10769795720137698', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.25468363871093064', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=max', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=61', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.02066454116254354', 'optimization_config.init_lr=0.0001748919663499442', 'optimization_config.lr_decay_power=4.912375873030162', 'optimization_config.lr_frac_warmup_steps=0.14447461437407516', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.005896496455247276', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 154.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 140.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.02 GiB memory in use. Of the allocated memory 611.50 MiB is allocated by PyTorch, and 64.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x7356391d81f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
[rank: 1] Child process with PID 1241350 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:40:48,011 - wandb.wandb_agent - INFO - Cleaning up finished run: 4r8r2ws5
[2025-04-10 15:40:48,011][wandb.wandb_agent][INFO] - Cleaning up finished run: 4r8r2ws5
2025-04-10 15:40:48,432 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:40:48,432][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:40:48,432 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.024859621796715414
	config.input_dropout: 0.2085037956972659
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.20203235915256207
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 26
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.005661738501796953
	optimization_config.init_lr: 0.003513359608858107
	optimization_config.lr_decay_power: 1.3992232878679047
	optimization_config.lr_frac_warmup_steps: 1.2594939702782494e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0044551120322063
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:40:48,432][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.024859621796715414
	config.input_dropout: 0.2085037956972659
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.20203235915256207
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 26
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.005661738501796953
	optimization_config.init_lr: 0.003513359608858107
	optimization_config.lr_decay_power: 1.3992232878679047
	optimization_config.lr_frac_warmup_steps: 1.2594939702782494e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0044551120322063
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:40:48,440 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.024859621796715414 config.input_dropout=0.2085037956972659 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.20203235915256207 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=26 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.005661738501796953 optimization_config.init_lr=0.003513359608858107 optimization_config.lr_decay_power=1.3992232878679047 optimization_config.lr_frac_warmup_steps=1.2594939702782494e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0044551120322063 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:40:48,440][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.024859621796715414 config.input_dropout=0.2085037956972659 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.20203235915256207 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=26 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.005661738501796953 optimization_config.init_lr=0.003513359608858107 optimization_config.lr_decay_power=1.3992232878679047 optimization_config.lr_frac_warmup_steps=1.2594939702782494e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0044551120322063 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:40:53,451 - wandb.wandb_agent - INFO - Running runs: ['5fju43al']
[2025-04-10 15:40:53,451][wandb.wandb_agent][INFO] - Running runs: ['5fju43al']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154057-5fju43al
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/5fju43al
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.024859621796715414
Overwriting input_dropout in config from 0.4494236115512016 to 0.2085037956972659
Overwriting resid_dropout in config from 0.4939188761966135 to 0.20203235915256207
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.024859621796715414
Overwriting input_dropout in config from 0.4494236115512016 to 0.2085037956972659
Overwriting resid_dropout in config from 0.4939188761966135 to 0.20203235915256207
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.39it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/4 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s] Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 42.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.11 GiB memory in use. Of the allocated memory 683.43 MiB is allocated by PyTorch, and 90.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
EXCEPTION: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 42.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.11 GiB memory in use. Of the allocated memory 683.43 MiB is allocated by PyTorch, and 90.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
Error executing job with overrides: ['config.attention_dropout=0.024859621796715414', 'config.input_dropout=0.2085037956972659', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.20203235915256207', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=max', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=26', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.005661738501796953', 'optimization_config.init_lr=0.003513359608858107', 'optimization_config.lr_decay_power=1.3992232878679047', 'optimization_config.lr_frac_warmup_steps=1.2594939702782494e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.0044551120322063', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 6.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.15 GiB memory in use. Of the allocated memory 727.76 MiB is allocated by PyTorch, and 84.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 42.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.11 GiB memory in use. Of the allocated memory 683.43 MiB is allocated by PyTorch, and 90.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x7815088e01f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: | 0.000 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: üöÄ View run generative_event_stream_transformer at: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/5fju43al
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154057-5fju43al/logs
EXCEPTION: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 6.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.15 GiB memory in use. Of the allocated memory 727.76 MiB is allocated by PyTorch, and 84.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.024859621796715414', 'config.input_dropout=0.2085037956972659', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.20203235915256207', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=max', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=26', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.005661738501796953', 'optimization_config.init_lr=0.003513359608858107', 'optimization_config.lr_decay_power=1.3992232878679047', 'optimization_config.lr_frac_warmup_steps=1.2594939702782494e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.0044551120322063', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 6.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.15 GiB memory in use. Of the allocated memory 727.76 MiB is allocated by PyTorch, and 84.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[rank: 1] Child process with PID 1242703 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:41:19,332 - wandb.wandb_agent - INFO - Cleaning up finished run: 5fju43al
[2025-04-10 15:41:19,332][wandb.wandb_agent][INFO] - Cleaning up finished run: 5fju43al
2025-04-10 15:41:19,876 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:41:19,876][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:41:19,877 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.34150052424010546
	config.input_dropout: 0.28457384488699594
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.18386136328139813
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 32
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.014949874735388427
	optimization_config.init_lr: 0.02612790779410885
	optimization_config.lr_decay_power: 3.893877993025362
	optimization_config.lr_frac_warmup_steps: 0.002614738806206877
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.002810820491813567
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:41:19,877][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.34150052424010546
	config.input_dropout: 0.28457384488699594
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.18386136328139813
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 32
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.014949874735388427
	optimization_config.init_lr: 0.02612790779410885
	optimization_config.lr_decay_power: 3.893877993025362
	optimization_config.lr_frac_warmup_steps: 0.002614738806206877
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.002810820491813567
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:41:19,884 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.34150052424010546 config.input_dropout=0.28457384488699594 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.18386136328139813 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=32 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.014949874735388427 optimization_config.init_lr=0.02612790779410885 optimization_config.lr_decay_power=3.893877993025362 optimization_config.lr_frac_warmup_steps=0.002614738806206877 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.002810820491813567 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:41:19,884][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.34150052424010546 config.input_dropout=0.28457384488699594 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.18386136328139813 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=32 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.014949874735388427 optimization_config.init_lr=0.02612790779410885 optimization_config.lr_decay_power=3.893877993025362 optimization_config.lr_frac_warmup_steps=0.002614738806206877 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.002810820491813567 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:41:24,896 - wandb.wandb_agent - INFO - Running runs: ['1w8e2zhu']
[2025-04-10 15:41:24,896][wandb.wandb_agent][INFO] - Running runs: ['1w8e2zhu']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154129-1w8e2zhu
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/1w8e2zhu
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.34150052424010546
Overwriting input_dropout in config from 0.4494236115512016 to 0.28457384488699594
Overwriting resid_dropout in config from 0.4939188761966135 to 0.18386136328139813
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.34150052424010546
Overwriting input_dropout in config from 0.4494236115512016 to 0.28457384488699594
Overwriting resid_dropout in config from 0.4939188761966135 to 0.18386136328139813
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.42it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/3 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 76.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.08 GiB memory in use. Of the allocated memory 685.71 MiB is allocated by PyTorch, and 54.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
EXCEPTION: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 76.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.08 GiB memory in use. Of the allocated memory 685.71 MiB is allocated by PyTorch, and 54.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.34150052424010546', 'config.input_dropout=0.28457384488699594', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.18386136328139813', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=cls', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=32', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.014949874735388427', 'optimization_config.init_lr=0.02612790779410885', 'optimization_config.lr_decay_power=3.893877993025362', 'optimization_config.lr_frac_warmup_steps=0.002614738806206877', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.002810820491813567', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 26.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.13 GiB memory in use. Of the allocated memory 686.21 MiB is allocated by PyTorch, and 105.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 76.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.08 GiB memory in use. Of the allocated memory 685.71 MiB is allocated by PyTorch, and 54.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x787f9f5f41f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: üöÄ View run generative_event_stream_transformer at: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/1w8e2zhu
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154129-1w8e2zhu/logs
EXCEPTION: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 26.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.13 GiB memory in use. Of the allocated memory 686.21 MiB is allocated by PyTorch, and 105.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.34150052424010546', 'config.input_dropout=0.28457384488699594', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.18386136328139813', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=cls', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=32', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.014949874735388427', 'optimization_config.init_lr=0.02612790779410885', 'optimization_config.lr_decay_power=3.893877993025362', 'optimization_config.lr_frac_warmup_steps=0.002614738806206877', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.002810820491813567', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 26.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.13 GiB memory in use. Of the allocated memory 686.21 MiB is allocated by PyTorch, and 105.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2123: UserWarning: Run (1w8e2zhu) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stdout", data),
Epoch 0:   0%|          | 0/3 [00:07<?, ?it/s]2025-04-10 15:41:55,921 - wandb.wandb_agent - INFO - Cleaning up finished run: 1w8e2zhu
[2025-04-10 15:41:55,921][wandb.wandb_agent][INFO] - Cleaning up finished run: 1w8e2zhu
2025-04-10 15:41:56,488 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:41:56,488][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:41:56,489 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4395099932378856
	config.input_dropout: 0.19455696595810612
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.4258062144131766
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 58
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.7436196544589406
	optimization_config.init_lr: 0.00012764432370015067
	optimization_config.lr_decay_power: 3.8317363728741176
	optimization_config.lr_frac_warmup_steps: 1.627364239032912e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0036965609932397025
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:41:56,489][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4395099932378856
	config.input_dropout: 0.19455696595810612
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.4258062144131766
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 58
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.7436196544589406
	optimization_config.init_lr: 0.00012764432370015067
	optimization_config.lr_decay_power: 3.8317363728741176
	optimization_config.lr_frac_warmup_steps: 1.627364239032912e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0036965609932397025
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:41:56,495 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4395099932378856 config.input_dropout=0.19455696595810612 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.4258062144131766 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=58 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.7436196544589406 optimization_config.init_lr=0.00012764432370015067 optimization_config.lr_decay_power=3.8317363728741176 optimization_config.lr_frac_warmup_steps=1.627364239032912e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0036965609932397025 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:41:56,495][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4395099932378856 config.input_dropout=0.19455696595810612 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.4258062144131766 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=58 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.7436196544589406 optimization_config.init_lr=0.00012764432370015067 optimization_config.lr_decay_power=3.8317363728741176 optimization_config.lr_frac_warmup_steps=1.627364239032912e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0036965609932397025 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:42:01,505 - wandb.wandb_agent - INFO - Running runs: ['f10qio5v']
[2025-04-10 15:42:01,505][wandb.wandb_agent][INFO] - Running runs: ['f10qio5v']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154206-f10qio5v
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/f10qio5v
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4395099932378856
Overwriting input_dropout in config from 0.4494236115512016 to 0.19455696595810612
Overwriting resid_dropout in config from 0.4939188761966135 to 0.4258062144131766
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4395099932378856
Overwriting input_dropout in config from 0.4494236115512016 to 0.19455696595810612
Overwriting resid_dropout in config from 0.4939188761966135 to 0.4258062144131766
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.50it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 18.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.14 GiB memory in use. Of the allocated memory 700.35 MiB is allocated by PyTorch, and 97.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
EXCEPTION: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 18.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.14 GiB memory in use. Of the allocated memory 700.35 MiB is allocated by PyTorch, and 97.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.4395099932378856', 'config.input_dropout=0.19455696595810612', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.4258062144131766', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=mean', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=58', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.7436196544589406', 'optimization_config.init_lr=0.00012764432370015067', 'optimization_config.lr_decay_power=3.8317363728741176', 'optimization_config.lr_frac_warmup_steps=1.627364239032912e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.0036965609932397025', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 24.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.13 GiB memory in use. Of the allocated memory 700.35 MiB is allocated by PyTorch, and 93.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 18.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.14 GiB memory in use. Of the allocated memory 700.35 MiB is allocated by PyTorch, and 97.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x7170105f01f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'CUDA-capable device(s) is/are busy or unavailable'
[rank: 1] Child process with PID 1245849 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:42:32,523 - wandb.wandb_agent - INFO - Cleaning up finished run: f10qio5v
[2025-04-10 15:42:32,523][wandb.wandb_agent][INFO] - Cleaning up finished run: f10qio5v
2025-04-10 15:42:33,043 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:42:33,043][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:42:33,043 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4723848275046098
	config.input_dropout: 0.16161969765104744
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.08093574052401004
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 33
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.024186909500921127
	optimization_config.init_lr: 6.460042784627466e-08
	optimization_config.lr_decay_power: 2.3327177776760095
	optimization_config.lr_frac_warmup_steps: 2.5658772202362864e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004886134518119734
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:42:33,043][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4723848275046098
	config.input_dropout: 0.16161969765104744
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.08093574052401004
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 33
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.024186909500921127
	optimization_config.init_lr: 6.460042784627466e-08
	optimization_config.lr_decay_power: 2.3327177776760095
	optimization_config.lr_frac_warmup_steps: 2.5658772202362864e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004886134518119734
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:42:33,052 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4723848275046098 config.input_dropout=0.16161969765104744 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.08093574052401004 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=33 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.024186909500921127 optimization_config.init_lr=6.460042784627466e-08 optimization_config.lr_decay_power=2.3327177776760095 optimization_config.lr_frac_warmup_steps=2.5658772202362864e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004886134518119734 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:42:33,052][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4723848275046098 config.input_dropout=0.16161969765104744 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.08093574052401004 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=33 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.024186909500921127 optimization_config.init_lr=6.460042784627466e-08 optimization_config.lr_decay_power=2.3327177776760095 optimization_config.lr_frac_warmup_steps=2.5658772202362864e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004886134518119734 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:42:38,063 - wandb.wandb_agent - INFO - Running runs: ['u9bdfyzs']
[2025-04-10 15:42:38,063][wandb.wandb_agent][INFO] - Running runs: ['u9bdfyzs']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154242-u9bdfyzs
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/u9bdfyzs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4723848275046098
Overwriting input_dropout in config from 0.4494236115512016 to 0.16161969765104744
Overwriting resid_dropout in config from 0.4939188761966135 to 0.08093574052401004
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4723848275046098
Overwriting input_dropout in config from 0.4494236115512016 to 0.16161969765104744
Overwriting resid_dropout in config from 0.4939188761966135 to 0.08093574052401004
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.47it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/3 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 48.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.11 GiB memory in use. Of the allocated memory 706.92 MiB is allocated by PyTorch, and 61.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
EXCEPTION: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 48.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.11 GiB memory in use. Of the allocated memory 706.92 MiB is allocated by PyTorch, and 61.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.4723848275046098', 'config.input_dropout=0.16161969765104744', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.08093574052401004', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=cls', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=33', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.024186909500921127', 'optimization_config.init_lr=6.460042784627466e-08', 'optimization_config.lr_decay_power=2.3327177776760095', 'optimization_config.lr_frac_warmup_steps=2.5658772202362864e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.004886134518119734', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 78.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.08 GiB memory in use. Of the allocated memory 639.85 MiB is allocated by PyTorch, and 100.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 48.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.11 GiB memory in use. Of the allocated memory 706.92 MiB is allocated by PyTorch, and 61.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x73c548ee81f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
wandb: üöÄ View run generative_event_stream_transformer at: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/u9bdfyzs
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154242-u9bdfyzs/logs
EXCEPTION: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 78.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.08 GiB memory in use. Of the allocated memory 639.85 MiB is allocated by PyTorch, and 100.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.4723848275046098', 'config.input_dropout=0.16161969765104744', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.08093574052401004', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=cls', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=33', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.024186909500921127', 'optimization_config.init_lr=6.460042784627466e-08', 'optimization_config.lr_decay_power=2.3327177776760095', 'optimization_config.lr_frac_warmup_steps=2.5658772202362864e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.004886134518119734', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 78.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.08 GiB memory in use. Of the allocated memory 639.85 MiB is allocated by PyTorch, and 100.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[rank: 1] Child process with PID 1247356 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:43:09,100 - wandb.wandb_agent - INFO - Cleaning up finished run: u9bdfyzs
[2025-04-10 15:43:09,100][wandb.wandb_agent][INFO] - Cleaning up finished run: u9bdfyzs
2025-04-10 15:43:09,769 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:43:09,769][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:43:09,770 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4521436156542074
	config.input_dropout: 0.09899673563998256
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.22506178380971475
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 50
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0012856136986622868
	optimization_config.init_lr: 1.7590054161272963e-05
	optimization_config.lr_decay_power: 4.893254349182481
	optimization_config.lr_frac_warmup_steps: 0.19460192929002113
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0066930413846065075
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:43:09,770][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4521436156542074
	config.input_dropout: 0.09899673563998256
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.22506178380971475
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 50
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0012856136986622868
	optimization_config.init_lr: 1.7590054161272963e-05
	optimization_config.lr_decay_power: 4.893254349182481
	optimization_config.lr_frac_warmup_steps: 0.19460192929002113
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0066930413846065075
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:43:09,777 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4521436156542074 config.input_dropout=0.09899673563998256 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.22506178380971475 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=50 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0012856136986622868 optimization_config.init_lr=1.7590054161272963e-05 optimization_config.lr_decay_power=4.893254349182481 optimization_config.lr_frac_warmup_steps=0.19460192929002113 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0066930413846065075 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:43:09,777][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4521436156542074 config.input_dropout=0.09899673563998256 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.22506178380971475 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=50 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0012856136986622868 optimization_config.init_lr=1.7590054161272963e-05 optimization_config.lr_decay_power=4.893254349182481 optimization_config.lr_frac_warmup_steps=0.19460192929002113 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0066930413846065075 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:43:14,787 - wandb.wandb_agent - INFO - Running runs: ['2yye8rfd']
[2025-04-10 15:43:14,787][wandb.wandb_agent][INFO] - Running runs: ['2yye8rfd']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154319-2yye8rfd
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/2yye8rfd
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4521436156542074
Overwriting input_dropout in config from 0.4494236115512016 to 0.09899673563998256
Overwriting resid_dropout in config from 0.4939188761966135 to 0.22506178380971475
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4521436156542074
Overwriting input_dropout in config from 0.4494236115512016 to 0.09899673563998256
Overwriting resid_dropout in config from 0.4939188761966135 to 0.22506178380971475
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.51it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 122.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1236761 has 2.88 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.04 GiB memory in use. Of the allocated memory 607.68 MiB is allocated by PyTorch, and 88.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 112.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.04 GiB memory in use. Of the allocated memory 607.68 MiB is allocated by PyTorch, and 96.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
EXCEPTION: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 112.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.04 GiB memory in use. Of the allocated memory 607.68 MiB is allocated by PyTorch, and 96.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.4521436156542074', 'config.input_dropout=0.09899673563998256', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.22506178380971475', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=last', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=50', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.0012856136986622868', 'optimization_config.init_lr=1.7590054161272963e-05', 'optimization_config.lr_decay_power=4.893254349182481', 'optimization_config.lr_frac_warmup_steps=0.19460192929002113', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.0066930413846065075', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 112.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1237213 has 2.89 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Including non-PyTorch memory, this process has 1.04 GiB memory in use. Of the allocated memory 607.68 MiB is allocated by PyTorch, and 96.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x77b3461f41f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
[rank: 1] Child process with PID 1248954 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:43:40,631 - wandb.wandb_agent - INFO - Cleaning up finished run: 2yye8rfd
[2025-04-10 15:43:40,631][wandb.wandb_agent][INFO] - Cleaning up finished run: 2yye8rfd
2025-04-10 15:43:41,238 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:43:41,238][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:43:41,238 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.2146906286383312
	config.input_dropout: 0.11247248912710832
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.0895719289440633
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 12
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0005039240717537248
	optimization_config.init_lr: 1.1370862760791218e-06
	optimization_config.lr_decay_power: 2.4086728307389276
	optimization_config.lr_frac_warmup_steps: 3.864981361206759e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004378299254221504
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:43:41,238][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.2146906286383312
	config.input_dropout: 0.11247248912710832
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.0895719289440633
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 12
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0005039240717537248
	optimization_config.init_lr: 1.1370862760791218e-06
	optimization_config.lr_decay_power: 2.4086728307389276
	optimization_config.lr_frac_warmup_steps: 3.864981361206759e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004378299254221504
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:43:41,245 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2146906286383312 config.input_dropout=0.11247248912710832 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.0895719289440633 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=12 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0005039240717537248 optimization_config.init_lr=1.1370862760791218e-06 optimization_config.lr_decay_power=2.4086728307389276 optimization_config.lr_frac_warmup_steps=3.864981361206759e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004378299254221504 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:43:41,245][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2146906286383312 config.input_dropout=0.11247248912710832 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.0895719289440633 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=12 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0005039240717537248 optimization_config.init_lr=1.1370862760791218e-06 optimization_config.lr_decay_power=2.4086728307389276 optimization_config.lr_frac_warmup_steps=3.864981361206759e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004378299254221504 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:43:46,255 - wandb.wandb_agent - INFO - Running runs: ['5b180voh']
[2025-04-10 15:43:46,255][wandb.wandb_agent][INFO] - Running runs: ['5b180voh']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154350-5b180voh
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/5b180voh
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.2146906286383312
Overwriting input_dropout in config from 0.4494236115512016 to 0.11247248912710832
Overwriting resid_dropout in config from 0.4939188761966135 to 0.0895719289440633
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.2146906286383312
Overwriting input_dropout in config from 0.4494236115512016 to 0.11247248912710832
Overwriting resid_dropout in config from 0.4939188761966135 to 0.0895719289440633
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.46it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/7 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s] Epoch 0:  14%|‚ñà‚ñç        | 1/7 [00:02<00:12,  0.48it/s]Epoch 0:  14%|‚ñà‚ñç        | 1/7 [00:02<00:12,  0.48it/s, v_num=0voh]Epoch 0:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:04<00:11,  0.44it/s, v_num=0voh]Epoch 0:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:04<00:11,  0.44it/s, v_num=0voh]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:06<00:08,  0.46it/s, v_num=0voh]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:06<00:08,  0.46it/s, v_num=0voh]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:08<00:06,  0.47it/s, v_num=0voh]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:08<00:06,  0.47it/s, v_num=0voh]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:11<00:04,  0.43it/s, v_num=0voh]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:11<00:04,  0.43it/s, v_num=0voh]Epoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:13<00:02,  0.44it/s, v_num=0voh]Epoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:13<00:02,  0.44it/s, v_num=0voh]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:15<00:00,  0.46it/s, v_num=0voh]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:15<00:00,  0.45it/s, v_num=0voh]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.60it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.39it/s, v_num=0voh]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.39it/s, v_num=0voh]Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]        Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]Epoch 1:  14%|‚ñà‚ñç        | 1/7 [00:03<00:23,  0.25it/s, v_num=0voh]Epoch 1:  14%|‚ñà‚ñç        | 1/7 [00:03<00:23,  0.25it/s, v_num=0voh]Epoch 1:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:06<00:15,  0.32it/s, v_num=0voh]Epoch 1:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:06<00:15,  0.32it/s, v_num=0voh]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:08<00:10,  0.37it/s, v_num=0voh]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:08<00:10,  0.37it/s, v_num=0voh]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:10<00:07,  0.40it/s, v_num=0voh]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:10<00:07,  0.40it/s, v_num=0voh]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:12<00:04,  0.40it/s, v_num=0voh]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:12<00:04,  0.40it/s, v_num=0voh]Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:14<00:02,  0.41it/s, v_num=0voh]Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:14<00:02,  0.41it/s, v_num=0voh]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.42it/s, v_num=0voh]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.42it/s, v_num=0voh]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.55it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:19<00:00,  0.36it/s, v_num=0voh]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:19<00:00,  0.36it/s, v_num=0voh]Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]        Epoch 2:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]Epoch 2:  14%|‚ñà‚ñç        | 1/7 [00:02<00:16,  0.36it/s, v_num=0voh]Epoch 2:  14%|‚ñà‚ñç        | 1/7 [00:02<00:16,  0.36it/s, v_num=0voh]Epoch 2:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:04<00:11,  0.42it/s, v_num=0voh]Epoch 2:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:04<00:12,  0.42it/s, v_num=0voh]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:06<00:08,  0.45it/s, v_num=0voh]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:06<00:08,  0.45it/s, v_num=0voh]Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:08<00:06,  0.47it/s, v_num=0voh]Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:08<00:06,  0.47it/s, v_num=0voh]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:10<00:04,  0.46it/s, v_num=0voh]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:10<00:04,  0.46it/s, v_num=0voh]Epoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:12<00:02,  0.47it/s, v_num=0voh]Epoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:12<00:02,  0.47it/s, v_num=0voh]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:14<00:00,  0.47it/s, v_num=0voh]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:14<00:00,  0.47it/s, v_num=0voh]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.49it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:18<00:00,  0.38it/s, v_num=0voh]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:18<00:00,  0.38it/s, v_num=0voh]Epoch 2:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]        Epoch 3:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]Epoch 3:  14%|‚ñà‚ñç        | 1/7 [00:03<00:23,  0.26it/s, v_num=0voh]Epoch 3:  14%|‚ñà‚ñç        | 1/7 [00:03<00:23,  0.26it/s, v_num=0voh]Epoch 3:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:05<00:14,  0.35it/s, v_num=0voh]Epoch 3:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:05<00:14,  0.35it/s, v_num=0voh]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:07<00:10,  0.38it/s, v_num=0voh]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:07<00:10,  0.38it/s, v_num=0voh]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:11<00:08,  0.36it/s, v_num=0voh]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:11<00:08,  0.36it/s, v_num=0voh]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:13<00:05,  0.38it/s, v_num=0voh]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:13<00:05,  0.38it/s, v_num=0voh]Epoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.38it/s, v_num=0voh]Epoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.38it/s, v_num=0voh]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.40it/s, v_num=0voh]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.40it/s, v_num=0voh]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.49it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.35it/s, v_num=0voh]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.35it/s, v_num=0voh]Epoch 3:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]        Epoch 4:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]Epoch 4:  14%|‚ñà‚ñç        | 1/7 [00:02<00:16,  0.37it/s, v_num=0voh]Epoch 4:  14%|‚ñà‚ñç        | 1/7 [00:02<00:16,  0.37it/s, v_num=0voh]Epoch 4:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:04<00:10,  0.46it/s, v_num=0voh]Epoch 4:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:04<00:10,  0.46it/s, v_num=0voh]Epoch 4:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:06<00:09,  0.43it/s, v_num=0voh]Epoch 4:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:06<00:09,  0.43it/s, v_num=0voh]Epoch 4:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:09<00:07,  0.41it/s, v_num=0voh]Epoch 4:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:09<00:07,  0.41it/s, v_num=0voh]Epoch 4:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:11<00:04,  0.43it/s, v_num=0voh]Epoch 4:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:11<00:04,  0.43it/s, v_num=0voh]Epoch 4:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.38it/s, v_num=0voh]Epoch 4:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.38it/s, v_num=0voh]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.39it/s, v_num=0voh]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.39it/s, v_num=0voh]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.62it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.34it/s, v_num=0voh]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.34it/s, v_num=0voh]Epoch 4:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]        Epoch 5:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]Epoch 5:  14%|‚ñà‚ñç        | 1/7 [00:02<00:14,  0.41it/s, v_num=0voh]Epoch 5:  14%|‚ñà‚ñç        | 1/7 [00:02<00:14,  0.41it/s, v_num=0voh]Epoch 5:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:05<00:13,  0.37it/s, v_num=0voh]Epoch 5:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:05<00:13,  0.37it/s, v_num=0voh]Epoch 5:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:07<00:10,  0.39it/s, v_num=0voh]Epoch 5:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:07<00:10,  0.39it/s, v_num=0voh]Epoch 5:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:09<00:07,  0.42it/s, v_num=0voh]Epoch 5:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:09<00:07,  0.42it/s, v_num=0voh]Epoch 5:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:12<00:05,  0.39it/s, v_num=0voh]Epoch 5:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:12<00:05,  0.39it/s, v_num=0voh]Epoch 5:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.39it/s, v_num=0voh]Epoch 5:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.39it/s, v_num=0voh]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.40it/s, v_num=0voh]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.40it/s, v_num=0voh]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.63it/s][A
                                                                      [AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:19<00:00,  0.35it/s, v_num=0voh]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:19<00:00,  0.35it/s, v_num=0voh]Epoch 5:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]        Epoch 6:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]Epoch 6:  14%|‚ñà‚ñç        | 1/7 [00:03<00:22,  0.27it/s, v_num=0voh]Epoch 6:  14%|‚ñà‚ñç        | 1/7 [00:03<00:22,  0.27it/s, v_num=0voh]Epoch 6:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:05<00:13,  0.36it/s, v_num=0voh]Epoch 6:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:05<00:13,  0.36it/s, v_num=0voh]Epoch 6:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:07<00:10,  0.38it/s, v_num=0voh]Epoch 6:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:07<00:10,  0.38it/s, v_num=0voh]Epoch 6:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:10<00:07,  0.39it/s, v_num=0voh]Epoch 6:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:10<00:07,  0.39it/s, v_num=0voh]Epoch 6:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:12<00:04,  0.41it/s, v_num=0voh]Epoch 6:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:12<00:04,  0.41it/s, v_num=0voh]Epoch 6:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:14<00:02,  0.43it/s, v_num=0voh]Epoch 6:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:14<00:02,  0.43it/s, v_num=0voh]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.43it/s, v_num=0voh]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.43it/s, v_num=0voh]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.57it/s][A
                                                                      [AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:18<00:00,  0.37it/s, v_num=0voh]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:18<00:00,  0.37it/s, v_num=0voh]Epoch 6:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]        Epoch 7:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]Epoch 7:  14%|‚ñà‚ñç        | 1/7 [00:02<00:12,  0.48it/s, v_num=0voh]Epoch 7:  14%|‚ñà‚ñç        | 1/7 [00:02<00:12,  0.48it/s, v_num=0voh]Epoch 7:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:05<00:14,  0.35it/s, v_num=0voh]Epoch 7:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:05<00:14,  0.35it/s, v_num=0voh]Epoch 7:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:07<00:10,  0.39it/s, v_num=0voh]Epoch 7:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:07<00:10,  0.39it/s, v_num=0voh]Epoch 7:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:09<00:07,  0.41it/s, v_num=0voh]Epoch 7:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:09<00:07,  0.41it/s, v_num=0voh]Epoch 7:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:11<00:04,  0.42it/s, v_num=0voh]Epoch 7:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:11<00:04,  0.42it/s, v_num=0voh]Epoch 7:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:13<00:02,  0.46it/s, v_num=0voh]Epoch 7:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:13<00:02,  0.46it/s, v_num=0voh]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:14<00:00,  0.48it/s, v_num=0voh]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:14<00:00,  0.48it/s, v_num=0voh]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.76it/s][A
                                                                      [AEpoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.43it/s, v_num=0voh]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.43it/s, v_num=0voh]Epoch 7:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]        Epoch 8:   0%|          | 0/7 [00:00<?, ?it/s, v_num=0voh]Epoch 8:  14%|‚ñà‚ñç        | 1/7 [00:02<00:15,  0.38it/s, v_num=0voh]Epoch 8:  14%|‚ñà‚ñç        | 1/7 [00:02<00:15,  0.38it/s, v_num=0voh]Epoch 8:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:04<00:10,  0.48it/s, v_num=0voh]Epoch 8:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:04<00:10,  0.48it/s, v_num=0voh]Epoch 8:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:05<00:07,  0.51it/s, v_num=0voh]Epoch 8:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:05<00:07,  0.51it/s, v_num=0voh]Epoch 8:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:07<00:05,  0.52it/s, v_num=0voh]Epoch 8:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:07<00:05,  0.52it/s, v_num=0voh]Epoch 8:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:08<00:03,  0.57it/s, v_num=0voh]Epoch 8:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:08<00:03,  0.57it/s, v_num=0voh]Epoch 8:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:13<00:02,  0.43it/s, v_num=0voh]Epoch 8:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:13<00:02,  0.43it/s, v_num=0voh]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:15<00:00,  0.46it/s, v_num=0voh]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:15<00:00,  0.46it/s, v_num=0voh]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.94it/s][A
                                                                      [AEpoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.40it/s, v_num=0voh]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.40it/s, v_num=0voh]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:17<00:00,  0.40it/s, v_num=0voh]Restoring states from the checkpoint path at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/5b180voh/checkpoints/epoch=0-val_loss=0.00-best_model.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/5b180voh/checkpoints/epoch=0-val_loss=0.00-best_model.ckpt
/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.validate()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.

/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/finetune_weights
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/held_out_0.parquet
/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/finetune_weights
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/held_out_0.parquet
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/1 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.82it/s]Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.39it/s]Restoring states from the checkpoint path at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/5b180voh/checkpoints/epoch=0-val_loss=0.00-best_model.ckpt
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/5b180voh/checkpoints/epoch=0-val_loss=0.00-best_model.ckpt
/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          Validate metric                       DataLoader 0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             task_AUROC                             0.0
             task_loss                        855.27490234375
           tuning_TTE_MSE                      36098.8828125
          tuning_TTE_MSLE                    8.156837463378906
         tuning_TTE_reg_NLL                  5.358717441558838
     tuning_event_label_cls_NLL             0.16583353281021118
 tuning_event_label_macro_accuracy           0.921269416809082
 tuning_event_label_micro_accuracy           0.921269416809082
 tuning_event_label_weighted_AUROC           0.7613412141799927
tuning_event_label_weighted_accuracy         0.7114335298538208
     tuning_event_type_cls_NLL              0.009486105293035507
  tuning_event_type_macro_accuracy                  0.0
  tuning_event_type_micro_accuracy                  0.0
  tuning_event_type_weighted_AUROC                  0.0
tuning_event_type_weighted_accuracy                 0.0
        tuning_feature_0_MSE                0.12230707705020905
      tuning_feature_0_reg_NLL              -0.4200229346752167
       tuning_feature_10_MSE                 0.7474196553230286
     tuning_feature_10_reg_NLL               0.8131342530250549
       tuning_feature_11_MSE                 1.763713002204895
     tuning_feature_11_reg_NLL               1.1876286268234253
       tuning_feature_12_MSE                 1.5896884202957153
     tuning_feature_12_reg_NLL               1.133163571357727
       tuning_feature_13_MSE                 2.1716127395629883
     tuning_feature_13_reg_NLL               1.460384488105774
       tuning_feature_14_MSE                 0.7433865070343018
     tuning_feature_14_reg_NLL               0.7707972526550293
       tuning_feature_15_MSE                 1.3810161352157593
     tuning_feature_15_reg_NLL               1.2632070779800415
       tuning_feature_16_MSE                 1.1026792526245117
     tuning_feature_16_reg_NLL               1.1502047777175903
       tuning_feature_17_MSE                 1.2567230463027954
     tuning_feature_17_reg_NLL               1.1925145387649536
       tuning_feature_18_MSE                 1.2441905736923218
     tuning_feature_18_reg_NLL               0.9693799614906311
       tuning_feature_19_MSE                 0.5215485095977783
     tuning_feature_19_reg_NLL               0.6361297369003296
        tuning_feature_1_MSE                 0.5664187073707581
      tuning_feature_1_reg_NLL               0.7642605900764465
       tuning_feature_20_MSE                0.47147753834724426
     tuning_feature_20_reg_NLL               0.5560283064842224
       tuning_feature_21_MSE                 0.469176322221756
     tuning_feature_21_reg_NLL               0.5335638523101807
       tuning_feature_22_MSE                 7.024417877197266
     tuning_feature_22_reg_NLL               5.805043697357178
       tuning_feature_23_MSE                 6.571743965148926
     tuning_feature_23_reg_NLL               1.1999934911727905
       tuning_feature_24_MSE                 15.910649299621582
     tuning_feature_24_reg_NLL               2.648545026779175
        tuning_feature_2_MSE                 1.6620211601257324
      tuning_feature_2_reg_NLL              0.30280840396881104
        tuning_feature_3_MSE                 2.0005156993865967
      tuning_feature_3_reg_NLL               0.4042462408542633
        tuning_feature_4_MSE                 1.5736560821533203
      tuning_feature_4_reg_NLL              -0.11943087726831436
        tuning_feature_5_MSE                 0.6397882103919983
      tuning_feature_5_reg_NLL              0.07887361198663712
        tuning_feature_6_MSE                 0.9839319586753845
      tuning_feature_6_reg_NLL               1.0308457612991333
        tuning_feature_7_MSE                 1.4668653011322021
      tuning_feature_7_reg_NLL               1.3764476776123047
        tuning_feature_8_MSE                 1.411266565322876
      tuning_feature_8_reg_NLL               1.2862062454223633
        tuning_feature_9_MSE                 1.5872124433517456
      tuning_feature_9_reg_NLL               1.2816276550292969
            tuning_loss                       888.114501953125
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.37it/s]Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.35it/s]wandb: Waiting for W&B process to finish... (success).
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.044 MB uploaded (0.000 MB deduped)wandb: | 0.000 MB of 0.044 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                  epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà
wandb:                       held_out_TTE_MSE ‚ñÅ
wandb:                      held_out_TTE_MSLE ‚ñÅ
wandb:                   held_out_TTE_reg_NLL ‚ñÅ
wandb:           held_out_event_label_cls_NLL ‚ñÅ
wandb:    held_out_event_label_macro_accuracy ‚ñÅ
wandb:    held_out_event_label_micro_accuracy ‚ñÅ
wandb:    held_out_event_label_weighted_AUROC ‚ñÅ
wandb: held_out_event_label_weighted_accuracy ‚ñÅ
wandb:            held_out_event_type_cls_NLL ‚ñÅ
wandb:     held_out_event_type_macro_accuracy ‚ñÅ
wandb:     held_out_event_type_micro_accuracy ‚ñÅ
wandb:     held_out_event_type_weighted_AUROC ‚ñÅ
wandb:  held_out_event_type_weighted_accuracy ‚ñÅ
wandb:                 held_out_feature_0_MSE ‚ñÅ
wandb:             held_out_feature_0_reg_NLL ‚ñÅ
wandb:                held_out_feature_10_MSE ‚ñÅ
wandb:            held_out_feature_10_reg_NLL ‚ñÅ
wandb:                held_out_feature_11_MSE ‚ñÅ
wandb:            held_out_feature_11_reg_NLL ‚ñÅ
wandb:                held_out_feature_12_MSE ‚ñÅ
wandb:            held_out_feature_12_reg_NLL ‚ñÅ
wandb:                held_out_feature_13_MSE ‚ñÅ
wandb:            held_out_feature_13_reg_NLL ‚ñÅ
wandb:                held_out_feature_14_MSE ‚ñÅ
wandb:            held_out_feature_14_reg_NLL ‚ñÅ
wandb:                held_out_feature_15_MSE ‚ñÅ
wandb:            held_out_feature_15_reg_NLL ‚ñÅ
wandb:                held_out_feature_16_MSE ‚ñÅ
wandb:            held_out_feature_16_reg_NLL ‚ñÅ
wandb:                held_out_feature_17_MSE ‚ñÅ
wandb:            held_out_feature_17_reg_NLL ‚ñÅ
wandb:                held_out_feature_18_MSE ‚ñÅ
wandb:            held_out_feature_18_reg_NLL ‚ñÅ
wandb:                held_out_feature_19_MSE ‚ñÅ
wandb:            held_out_feature_19_reg_NLL ‚ñÅ
wandb:                 held_out_feature_1_MSE ‚ñÅ
wandb:             held_out_feature_1_reg_NLL ‚ñÅ
wandb:                held_out_feature_20_MSE ‚ñÅ
wandb:            held_out_feature_20_reg_NLL ‚ñÅ
wandb:                held_out_feature_21_MSE ‚ñÅ
wandb:            held_out_feature_21_reg_NLL ‚ñÅ
wandb:                held_out_feature_22_MSE ‚ñÅ
wandb:            held_out_feature_22_reg_NLL ‚ñÅ
wandb:                held_out_feature_23_MSE ‚ñÅ
wandb:            held_out_feature_23_reg_NLL ‚ñÅ
wandb:                held_out_feature_24_MSE ‚ñÅ
wandb:            held_out_feature_24_reg_NLL ‚ñÅ
wandb:                 held_out_feature_2_MSE ‚ñÅ
wandb:             held_out_feature_2_reg_NLL ‚ñÅ
wandb:                 held_out_feature_3_MSE ‚ñÅ
wandb:             held_out_feature_3_reg_NLL ‚ñÅ
wandb:                 held_out_feature_4_MSE ‚ñÅ
wandb:             held_out_feature_4_reg_NLL ‚ñÅ
wandb:                 held_out_feature_5_MSE ‚ñÅ
wandb:             held_out_feature_5_reg_NLL ‚ñÅ
wandb:                 held_out_feature_6_MSE ‚ñÅ
wandb:             held_out_feature_6_reg_NLL ‚ñÅ
wandb:                 held_out_feature_7_MSE ‚ñÅ
wandb:             held_out_feature_7_reg_NLL ‚ñÅ
wandb:                 held_out_feature_8_MSE ‚ñÅ
wandb:             held_out_feature_8_reg_NLL ‚ñÅ
wandb:                 held_out_feature_9_MSE ‚ñÅ
wandb:             held_out_feature_9_reg_NLL ‚ñÅ
wandb:                          held_out_loss ‚ñÅ
wandb:                               lr-AdamW ‚ñÅ
wandb:                             task_AUROC ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                              task_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:                             train_loss ‚ñÅ
wandb:                    trainer/global_step ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà
wandb:                         tuning_TTE_MSE ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                        tuning_TTE_MSLE ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÑ
wandb:                     tuning_TTE_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:             tuning_event_label_cls_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:      tuning_event_label_macro_accuracy ‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñà
wandb:      tuning_event_label_micro_accuracy ‚ñà‚ñà‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñà
wandb:      tuning_event_label_weighted_AUROC ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:   tuning_event_label_weighted_accuracy ‚ñà‚ñà‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñà
wandb:              tuning_event_type_cls_NLL ‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñà
wandb:       tuning_event_type_macro_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       tuning_event_type_micro_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       tuning_event_type_weighted_AUROC ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    tuning_event_type_weighted_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                   tuning_feature_0_MSE ‚ñÅ‚ñà‚ñá‚ñà‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÅ
wandb:               tuning_feature_0_reg_NLL ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:                  tuning_feature_10_MSE ‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñá‚ñÉ‚ñá‚ñÑ‚ñà
wandb:              tuning_feature_10_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:                  tuning_feature_11_MSE ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñá‚ñà‚ñÉ‚ñá‚ñá‚ñÉ
wandb:              tuning_feature_11_reg_NLL ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:                  tuning_feature_12_MSE ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÑ‚ñÖ
wandb:              tuning_feature_12_reg_NLL ‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñà
wandb:                  tuning_feature_13_MSE ‚ñÇ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÇ
wandb:              tuning_feature_13_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:                  tuning_feature_14_MSE ‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÑ
wandb:              tuning_feature_14_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:                  tuning_feature_15_MSE ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñà
wandb:              tuning_feature_15_reg_NLL ‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñá
wandb:                  tuning_feature_16_MSE ‚ñÑ‚ñá‚ñà‚ñÑ‚ñÇ‚ñá‚ñÜ‚ñÅ‚ñÖ‚ñÑ
wandb:              tuning_feature_16_reg_NLL ‚ñÅ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÅ
wandb:                  tuning_feature_17_MSE ‚ñá‚ñÜ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÇ‚ñá
wandb:              tuning_feature_17_reg_NLL ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñà
wandb:                  tuning_feature_18_MSE ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÑ
wandb:              tuning_feature_18_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:                  tuning_feature_19_MSE ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñà
wandb:              tuning_feature_19_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:                   tuning_feature_1_MSE ‚ñá‚ñÑ‚ñÜ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñà‚ñá‚ñá
wandb:               tuning_feature_1_reg_NLL ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ
wandb:                  tuning_feature_20_MSE ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÖ
wandb:              tuning_feature_20_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:                  tuning_feature_21_MSE ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÖ‚ñÅ‚ñá‚ñÉ
wandb:              tuning_feature_21_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:                  tuning_feature_22_MSE ‚ñá‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñá
wandb:              tuning_feature_22_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:                  tuning_feature_23_MSE ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñà‚ñá‚ñÑ‚ñÜ‚ñÅ‚ñÉ
wandb:              tuning_feature_23_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÅ
wandb:                  tuning_feature_24_MSE ‚ñà‚ñà‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÇ‚ñÅ‚ñà
wandb:              tuning_feature_24_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÅ
wandb:                   tuning_feature_2_MSE ‚ñÖ‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÖ
wandb:               tuning_feature_2_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:                   tuning_feature_3_MSE ‚ñà‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÇ‚ñÑ‚ñà
wandb:               tuning_feature_3_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:                   tuning_feature_4_MSE ‚ñÜ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñà‚ñÑ‚ñÜ
wandb:               tuning_feature_4_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:                   tuning_feature_5_MSE ‚ñá‚ñÉ‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñá
wandb:               tuning_feature_5_reg_NLL ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:                   tuning_feature_6_MSE ‚ñà‚ñá‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñà
wandb:               tuning_feature_6_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:                   tuning_feature_7_MSE ‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÅ
wandb:               tuning_feature_7_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:                   tuning_feature_8_MSE ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÅ
wandb:               tuning_feature_8_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:                   tuning_feature_9_MSE ‚ñÑ‚ñÇ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÅ‚ñÑ
wandb:               tuning_feature_9_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:                            tuning_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                  epoch 9
wandb:                       held_out_TTE_MSE 55482.08203
wandb:                      held_out_TTE_MSLE 9.82566
wandb:                   held_out_TTE_reg_NLL 5.43632
wandb:           held_out_event_label_cls_NLL 0.17517
wandb:    held_out_event_label_macro_accuracy 0.94524
wandb:    held_out_event_label_micro_accuracy 0.94524
wandb:    held_out_event_label_weighted_AUROC 0.86177
wandb: held_out_event_label_weighted_accuracy 0.84584
wandb:            held_out_event_type_cls_NLL 0.01683
wandb:     held_out_event_type_macro_accuracy 0.0
wandb:     held_out_event_type_micro_accuracy 0.0
wandb:     held_out_event_type_weighted_AUROC 0.0
wandb:  held_out_event_type_weighted_accuracy 0.0
wandb:                 held_out_feature_0_MSE 1.16395
wandb:             held_out_feature_0_reg_NLL -0.50211
wandb:                held_out_feature_10_MSE 1.20213
wandb:            held_out_feature_10_reg_NLL 1.57291
wandb:                held_out_feature_11_MSE 2.44064
wandb:            held_out_feature_11_reg_NLL 1.64714
wandb:                held_out_feature_12_MSE 2.92429
wandb:            held_out_feature_12_reg_NLL 1.92765
wandb:                held_out_feature_13_MSE 2.7796
wandb:            held_out_feature_13_reg_NLL 2.48119
wandb:                held_out_feature_14_MSE 2.7747
wandb:            held_out_feature_14_reg_NLL 0.98661
wandb:                held_out_feature_15_MSE 1.42044
wandb:            held_out_feature_15_reg_NLL 1.30385
wandb:                held_out_feature_16_MSE 1.30406
wandb:            held_out_feature_16_reg_NLL 1.23689
wandb:                held_out_feature_17_MSE 1.37373
wandb:            held_out_feature_17_reg_NLL 1.24538
wandb:                held_out_feature_18_MSE 2.16545
wandb:            held_out_feature_18_reg_NLL 1.4358
wandb:                held_out_feature_19_MSE 0.88882
wandb:            held_out_feature_19_reg_NLL 0.80674
wandb:                 held_out_feature_1_MSE 1.85967
wandb:             held_out_feature_1_reg_NLL 0.6685
wandb:                held_out_feature_20_MSE 0.80707
wandb:            held_out_feature_20_reg_NLL 0.82115
wandb:                held_out_feature_21_MSE 0.82563
wandb:            held_out_feature_21_reg_NLL 0.78731
wandb:                held_out_feature_22_MSE 0.3539
wandb:            held_out_feature_22_reg_NLL 0.41444
wandb:                held_out_feature_23_MSE 1.96857
wandb:            held_out_feature_23_reg_NLL 0.97267
wandb:                held_out_feature_24_MSE 1.36729
wandb:            held_out_feature_24_reg_NLL 0.43808
wandb:                 held_out_feature_2_MSE 1.71407
wandb:             held_out_feature_2_reg_NLL 1.51246
wandb:                 held_out_feature_3_MSE 1.34904
wandb:             held_out_feature_3_reg_NLL 1.06891
wandb:                 held_out_feature_4_MSE 1.5706
wandb:             held_out_feature_4_reg_NLL 0.95903
wandb:                 held_out_feature_5_MSE 1.71455
wandb:             held_out_feature_5_reg_NLL 2.58849
wandb:                 held_out_feature_6_MSE 0.80851
wandb:             held_out_feature_6_reg_NLL 0.92131
wandb:                 held_out_feature_7_MSE 1.86934
wandb:             held_out_feature_7_reg_NLL 1.42606
wandb:                 held_out_feature_8_MSE 1.43888
wandb:             held_out_feature_8_reg_NLL 1.25876
wandb:                 held_out_feature_9_MSE 1.83643
wandb:             held_out_feature_9_reg_NLL 1.33704
wandb:                          held_out_loss 1027.76672
wandb:                               lr-AdamW 0.0
wandb:                             task_AUROC 0.0
wandb:                              task_loss 992.82208
wandb:                             train_loss 1182.7583
wandb:                    trainer/global_step 63
wandb:                         tuning_TTE_MSE 36098.88281
wandb:                        tuning_TTE_MSLE 8.15684
wandb:                     tuning_TTE_reg_NLL 5.35872
wandb:             tuning_event_label_cls_NLL 0.16583
wandb:      tuning_event_label_macro_accuracy 0.92127
wandb:      tuning_event_label_micro_accuracy 0.92127
wandb:      tuning_event_label_weighted_AUROC 0.76134
wandb:   tuning_event_label_weighted_accuracy 0.71143
wandb:              tuning_event_type_cls_NLL 0.00949
wandb:       tuning_event_type_macro_accuracy 0.0
wandb:       tuning_event_type_micro_accuracy 0.0
wandb:       tuning_event_type_weighted_AUROC 0.0
wandb:    tuning_event_type_weighted_accuracy 0.0
wandb:                   tuning_feature_0_MSE 0.12231
wandb:               tuning_feature_0_reg_NLL -0.42002
wandb:                  tuning_feature_10_MSE 0.74742
wandb:              tuning_feature_10_reg_NLL 0.81313
wandb:                  tuning_feature_11_MSE 1.76371
wandb:              tuning_feature_11_reg_NLL 1.18763
wandb:                  tuning_feature_12_MSE 1.58969
wandb:              tuning_feature_12_reg_NLL 1.13316
wandb:                  tuning_feature_13_MSE 2.17161
wandb:              tuning_feature_13_reg_NLL 1.46038
wandb:                  tuning_feature_14_MSE 0.74339
wandb:              tuning_feature_14_reg_NLL 0.7708
wandb:                  tuning_feature_15_MSE 1.38102
wandb:              tuning_feature_15_reg_NLL 1.26321
wandb:                  tuning_feature_16_MSE 1.10268
wandb:              tuning_feature_16_reg_NLL 1.1502
wandb:                  tuning_feature_17_MSE 1.25672
wandb:              tuning_feature_17_reg_NLL 1.19251
wandb:                  tuning_feature_18_MSE 1.24419
wandb:              tuning_feature_18_reg_NLL 0.96938
wandb:                  tuning_feature_19_MSE 0.52155
wandb:              tuning_feature_19_reg_NLL 0.63613
wandb:                   tuning_feature_1_MSE 0.56642
wandb:               tuning_feature_1_reg_NLL 0.76426
wandb:                  tuning_feature_20_MSE 0.47148
wandb:              tuning_feature_20_reg_NLL 0.55603
wandb:                  tuning_feature_21_MSE 0.46918
wandb:              tuning_feature_21_reg_NLL 0.53356
wandb:                  tuning_feature_22_MSE 7.02442
wandb:              tuning_feature_22_reg_NLL 5.80504
wandb:                  tuning_feature_23_MSE 6.57174
wandb:              tuning_feature_23_reg_NLL 1.19999
wandb:                  tuning_feature_24_MSE 15.91065
wandb:              tuning_feature_24_reg_NLL 2.64855
wandb:                   tuning_feature_2_MSE 1.66202
wandb:               tuning_feature_2_reg_NLL 0.30281
wandb:                   tuning_feature_3_MSE 2.00052
wandb:               tuning_feature_3_reg_NLL 0.40425
wandb:                   tuning_feature_4_MSE 1.57366
wandb:               tuning_feature_4_reg_NLL -0.11943
wandb:                   tuning_feature_5_MSE 0.63979
wandb:               tuning_feature_5_reg_NLL 0.07887
wandb:                   tuning_feature_6_MSE 0.98393
wandb:               tuning_feature_6_reg_NLL 1.03085
wandb:                   tuning_feature_7_MSE 1.46687
wandb:               tuning_feature_7_reg_NLL 1.37645
wandb:                   tuning_feature_8_MSE 1.41127
wandb:               tuning_feature_8_reg_NLL 1.28621
wandb:                   tuning_feature_9_MSE 1.58721
wandb:               tuning_feature_9_reg_NLL 1.28163
wandb:                            tuning_loss 888.1145
wandb: 
wandb: üöÄ View run generative_event_stream_transformer at: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/5b180voh
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154350-5b180voh/logs

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             Test metric                           DataLoader 0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
           held_out_TTE_MSE                       55482.08203125
          held_out_TTE_MSLE                     9.825660705566406
         held_out_TTE_reg_NLL                   5.436321258544922
     held_out_event_label_cls_NLL              0.17516927421092987
 held_out_event_label_macro_accuracy            0.9452396035194397
 held_out_event_label_micro_accuracy            0.9452396035194397
 held_out_event_label_weighted_AUROC            0.8617737293243408
held_out_event_label_weighted_accuracy          0.8458359241485596
     held_out_event_type_cls_NLL               0.016830729320645332
  held_out_event_type_macro_accuracy                   0.0
  held_out_event_type_micro_accuracy                   0.0
  held_out_event_type_weighted_AUROC                   0.0
held_out_event_type_weighted_accuracy                  0.0
        held_out_feature_0_MSE                  1.1639498472213745
      held_out_feature_0_reg_NLL               -0.5021083354949951
       held_out_feature_10_MSE                  1.2021297216415405
     held_out_feature_10_reg_NLL                1.5729097127914429
       held_out_feature_11_MSE                  2.4406392574310303
     held_out_feature_11_reg_NLL                1.647137999534607
       held_out_feature_12_MSE                  2.9242889881134033
     held_out_feature_12_reg_NLL                1.9276479482650757
       held_out_feature_13_MSE                  2.779595136642456
     held_out_feature_13_reg_NLL                2.4811947345733643
       held_out_feature_14_MSE                  2.7747018337249756
     held_out_feature_14_reg_NLL                0.9866122603416443
       held_out_feature_15_MSE                  1.420437216758728
     held_out_feature_15_reg_NLL                1.303850531578064
       held_out_feature_16_MSE                  1.3040560483932495
     held_out_feature_16_reg_NLL                1.2368850708007812
       held_out_feature_17_MSE                  1.3737318515777588
     held_out_feature_17_reg_NLL                1.2453774213790894
       held_out_feature_18_MSE                   2.16544771194458
     held_out_feature_18_reg_NLL                1.4357975721359253
       held_out_feature_19_MSE                  0.8888169527053833
     held_out_feature_19_reg_NLL                0.8067416548728943
        held_out_feature_1_MSE                  1.859668493270874
      held_out_feature_1_reg_NLL                0.6685001254081726
       held_out_feature_20_MSE                  0.8070661425590515
     held_out_feature_20_reg_NLL                0.8211514949798584
       held_out_feature_21_MSE                  0.8256275653839111
     held_out_feature_21_reg_NLL                0.7873145937919617
       held_out_feature_22_MSE                  0.3538963794708252
     held_out_feature_22_reg_NLL                0.4144355356693268
       held_out_feature_23_MSE                  1.9685657024383545
     held_out_feature_23_reg_NLL                0.9726737141609192
       held_out_feature_24_MSE                  1.3672866821289062
     held_out_feature_24_reg_NLL               0.43808475136756897
        held_out_feature_2_MSE                  1.7140693664550781
      held_out_feature_2_reg_NLL                1.5124579668045044
        held_out_feature_3_MSE                  1.3490358591079712
      held_out_feature_3_reg_NLL                1.0689133405685425
        held_out_feature_4_MSE                  1.5706006288528442
      held_out_feature_4_reg_NLL                0.9590303301811218
        held_out_feature_5_MSE                  1.7145525217056274
      held_out_feature_5_reg_NLL                2.5884928703308105
        held_out_feature_6_MSE                  0.8085119724273682
      held_out_feature_6_reg_NLL                0.9213111996650696
        held_out_feature_7_MSE                  1.8693444728851318
      held_out_feature_7_reg_NLL                1.4260560274124146
        held_out_feature_8_MSE                  1.4388842582702637
      held_out_feature_8_reg_NLL                1.2587618827819824
        held_out_feature_9_MSE                  1.8364313840866089
      held_out_feature_9_reg_NLL                1.3370414972305298
            held_out_loss                       1027.7667236328125
              task_AUROC                               0.0
              task_loss                         992.8220825195312
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Saving final metrics...
2025-04-10 15:47:13,441 - wandb.wandb_agent - INFO - Cleaning up finished run: 5b180voh
[2025-04-10 15:47:13,441][wandb.wandb_agent][INFO] - Cleaning up finished run: 5b180voh
2025-04-10 15:47:13,949 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:47:13,949][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:47:13,949 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3485960669261471
	config.input_dropout: 0.10536062399920182
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.2533324944718771
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 37
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0320120187915143
	optimization_config.init_lr: 1.6501174914375195e-06
	optimization_config.lr_decay_power: 4.379671042851925
	optimization_config.lr_frac_warmup_steps: 2.6767411280448934e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009928617975069736
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:47:13,949][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3485960669261471
	config.input_dropout: 0.10536062399920182
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.2533324944718771
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 37
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0320120187915143
	optimization_config.init_lr: 1.6501174914375195e-06
	optimization_config.lr_decay_power: 4.379671042851925
	optimization_config.lr_frac_warmup_steps: 2.6767411280448934e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009928617975069736
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:47:13,956 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3485960669261471 config.input_dropout=0.10536062399920182 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.2533324944718771 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=37 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0320120187915143 optimization_config.init_lr=1.6501174914375195e-06 optimization_config.lr_decay_power=4.379671042851925 optimization_config.lr_frac_warmup_steps=2.6767411280448934e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009928617975069736 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:47:13,956][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3485960669261471 config.input_dropout=0.10536062399920182 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.2533324944718771 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=37 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0320120187915143 optimization_config.init_lr=1.6501174914375195e-06 optimization_config.lr_decay_power=4.379671042851925 optimization_config.lr_frac_warmup_steps=2.6767411280448934e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009928617975069736 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:47:18,966 - wandb.wandb_agent - INFO - Running runs: ['lj6e6y4w']
[2025-04-10 15:47:18,966][wandb.wandb_agent][INFO] - Running runs: ['lj6e6y4w']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154723-lj6e6y4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/lj6e6y4w
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.3485960669261471
Overwriting input_dropout in config from 0.4494236115512016 to 0.10536062399920182
Overwriting resid_dropout in config from 0.4939188761966135 to 0.2533324944718771
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.3485960669261471
Overwriting input_dropout in config from 0.4494236115512016 to 0.10536062399920182
Overwriting resid_dropout in config from 0.4939188761966135 to 0.2533324944718771
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.44it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/3 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 64.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Process 1256504 has 2.53 GiB memory in use. Including non-PyTorch memory, this process has 1.45 GiB memory in use. Of the allocated memory 1.02 GiB is allocated by PyTorch, and 72.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
EXCEPTION: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 64.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Process 1256504 has 2.53 GiB memory in use. Including non-PyTorch memory, this process has 1.45 GiB memory in use. Of the allocated memory 1.02 GiB is allocated by PyTorch, and 72.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.3485960669261471', 'config.input_dropout=0.10536062399920182', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.2533324944718771', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=max', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=37', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.0320120187915143', 'optimization_config.init_lr=1.6501174914375195e-06', 'optimization_config.lr_decay_power=4.379671042851925', 'optimization_config.lr_frac_warmup_steps=2.6767411280448934e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.009928617975069736', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 54.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Process 1256120 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 1.44 GiB memory in use. Of the allocated memory 1.02 GiB is allocated by PyTorch, and 64.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights, mask_value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 64.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Process 1256504 has 2.53 GiB memory in use. Including non-PyTorch memory, this process has 1.45 GiB memory in use. Of the allocated memory 1.02 GiB is allocated by PyTorch, and 72.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x7992086d41f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
[rank: 1] Child process with PID 1257705 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:47:44,885 - wandb.wandb_agent - INFO - Cleaning up finished run: lj6e6y4w
[2025-04-10 15:47:44,885][wandb.wandb_agent][INFO] - Cleaning up finished run: lj6e6y4w
2025-04-10 15:47:45,496 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:47:45,496][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:47:45,496 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.020726879619759553
	config.input_dropout: 0.4218322408876842
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.01259916246679088
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 47
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00029367410938215075
	optimization_config.init_lr: 1.1542124916352969e-08
	optimization_config.lr_decay_power: 1.5268189460004795
	optimization_config.lr_frac_warmup_steps: 3.355499036706863e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0009091387764981072
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:47:45,496][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.020726879619759553
	config.input_dropout: 0.4218322408876842
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.01259916246679088
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 47
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00029367410938215075
	optimization_config.init_lr: 1.1542124916352969e-08
	optimization_config.lr_decay_power: 1.5268189460004795
	optimization_config.lr_frac_warmup_steps: 3.355499036706863e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0009091387764981072
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:47:45,502 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.020726879619759553 config.input_dropout=0.4218322408876842 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.01259916246679088 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=47 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00029367410938215075 optimization_config.init_lr=1.1542124916352969e-08 optimization_config.lr_decay_power=1.5268189460004795 optimization_config.lr_frac_warmup_steps=3.355499036706863e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0009091387764981072 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:47:45,502][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.020726879619759553 config.input_dropout=0.4218322408876842 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.01259916246679088 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=47 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00029367410938215075 optimization_config.init_lr=1.1542124916352969e-08 optimization_config.lr_decay_power=1.5268189460004795 optimization_config.lr_frac_warmup_steps=3.355499036706863e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0009091387764981072 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:47:50,512 - wandb.wandb_agent - INFO - Running runs: ['fxms0zjh']
[2025-04-10 15:47:50,512][wandb.wandb_agent][INFO] - Running runs: ['fxms0zjh']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154755-fxms0zjh
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/fxms0zjh
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.020726879619759553
Overwriting input_dropout in config from 0.4494236115512016 to 0.4218322408876842
Overwriting resid_dropout in config from 0.4939188761966135 to 0.01259916246679088
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.020726879619759553
Overwriting input_dropout in config from 0.4494236115512016 to 0.4218322408876842
Overwriting resid_dropout in config from 0.4939188761966135 to 0.01259916246679088
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.52it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 60.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Process 1256504 has 2.53 GiB memory in use. Including non-PyTorch memory, this process has 1.46 GiB memory in use. Of the allocated memory 996.31 MiB is allocated by PyTorch, and 127.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
EXCEPTION: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 60.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Process 1256504 has 2.53 GiB memory in use. Including non-PyTorch memory, this process has 1.46 GiB memory in use. Of the allocated memory 996.31 MiB is allocated by PyTorch, and 127.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.020726879619759553', 'config.input_dropout=0.4218322408876842', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.01259916246679088', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=mean', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=47', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.00029367410938215075', 'optimization_config.init_lr=1.1542124916352969e-08', 'optimization_config.lr_decay_power=1.5268189460004795', 'optimization_config.lr_frac_warmup_steps=3.355499036706863e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.0009091387764981072', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 30.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Process 1256120 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 1.47 GiB memory in use. Of the allocated memory 996.31 MiB is allocated by PyTorch, and 139.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 60.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Process 1256504 has 2.53 GiB memory in use. Including non-PyTorch memory, this process has 1.46 GiB memory in use. Of the allocated memory 996.31 MiB is allocated by PyTorch, and 127.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x71b6a7de81f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: üöÄ View run generative_event_stream_transformer at: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/fxms0zjh
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154755-fxms0zjh/logs
EXCEPTION: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 30.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Process 1256120 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 1.47 GiB memory in use. Of the allocated memory 996.31 MiB is allocated by PyTorch, and 139.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.020726879619759553', 'config.input_dropout=0.4218322408876842', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.01259916246679088', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=mean', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=47', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.00029367410938215075', 'optimization_config.init_lr=1.1542124916352969e-08', 'optimization_config.lr_decay_power=1.5268189460004795', 'optimization_config.lr_frac_warmup_steps=3.355499036706863e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.0009091387764981072', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 329, in forward
    encoded = self.encoder(batch, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 886, in forward
    outputs = block(**kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 439, in forward
    attn_outputs = self.attn(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 350, in forward
    return self.attention(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 272, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/transformer.py", line 191, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 30.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Process 1256120 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 1.47 GiB memory in use. Of the allocated memory 996.31 MiB is allocated by PyTorch, and 139.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[rank: 1] Child process with PID 1259130 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:48:21,537 - wandb.wandb_agent - INFO - Cleaning up finished run: fxms0zjh
[2025-04-10 15:48:21,537][wandb.wandb_agent][INFO] - Cleaning up finished run: fxms0zjh
2025-04-10 15:48:22,174 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:48:22,174][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:48:22,175 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.45143889280289784
	config.input_dropout: 0.1810833778754647
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.12178398178780649
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 28
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.1435512007225153
	optimization_config.init_lr: 0.0003860460367133558
	optimization_config.lr_decay_power: 1.291421302532168
	optimization_config.lr_frac_warmup_steps: 0.000718360994598567
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005777018017546932
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:48:22,175][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.45143889280289784
	config.input_dropout: 0.1810833778754647
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.12178398178780649
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 28
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.1435512007225153
	optimization_config.init_lr: 0.0003860460367133558
	optimization_config.lr_decay_power: 1.291421302532168
	optimization_config.lr_frac_warmup_steps: 0.000718360994598567
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005777018017546932
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:48:22,182 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.45143889280289784 config.input_dropout=0.1810833778754647 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.12178398178780649 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=28 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.1435512007225153 optimization_config.init_lr=0.0003860460367133558 optimization_config.lr_decay_power=1.291421302532168 optimization_config.lr_frac_warmup_steps=0.000718360994598567 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005777018017546932 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:48:22,182][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.45143889280289784 config.input_dropout=0.1810833778754647 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.12178398178780649 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=28 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.1435512007225153 optimization_config.init_lr=0.0003860460367133558 optimization_config.lr_decay_power=1.291421302532168 optimization_config.lr_frac_warmup_steps=0.000718360994598567 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005777018017546932 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:48:27,192 - wandb.wandb_agent - INFO - Running runs: ['nmzlfn7z']
[2025-04-10 15:48:27,192][wandb.wandb_agent][INFO] - Running runs: ['nmzlfn7z']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154831-nmzlfn7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/nmzlfn7z
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.45143889280289784
Overwriting input_dropout in config from 0.4494236115512016 to 0.1810833778754647
Overwriting resid_dropout in config from 0.4939188761966135 to 0.12178398178780649
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.45143889280289784
Overwriting input_dropout in config from 0.4494236115512016 to 0.1810833778754647
Overwriting resid_dropout in config from 0.4939188761966135 to 0.12178398178780649
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.69it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/3 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1097, in backward
    loss.backward(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 10.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Process 1256504 has 2.53 GiB memory in use. Including non-PyTorch memory, this process has 1.50 GiB memory in use. Of the allocated memory 991.73 MiB is allocated by PyTorch, and 174.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
EXCEPTION: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 10.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Process 1256504 has 2.53 GiB memory in use. Including non-PyTorch memory, this process has 1.50 GiB memory in use. Of the allocated memory 991.73 MiB is allocated by PyTorch, and 174.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error executing job with overrides: ['config.attention_dropout=0.45143889280289784', 'config.input_dropout=0.1810833778754647', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.12178398178780649', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=max', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=28', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.1435512007225153', 'optimization_config.init_lr=0.0003860460367133558', 'optimization_config.lr_decay_power=1.291421302532168', 'optimization_config.lr_frac_warmup_steps=0.000718360994598567', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.005777018017546932', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1097, in backward
    loss.backward(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 20.50 MiB is free. Process 1228759 has 2.49 GiB memory in use. Process 1237656 has 3.09 GiB memory in use. Process 1256120 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.01 GiB is allocated by PyTorch, and 107.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1097, in backward
    loss.backward(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 1 has a total capacity of 9.67 GiB of which 10.25 MiB is free. Process 1228994 has 2.50 GiB memory in use. Process 1238109 has 3.09 GiB memory in use. Process 1256504 has 2.53 GiB memory in use. Including non-PyTorch memory, this process has 1.50 GiB memory in use. Of the allocated memory 991.73 MiB is allocated by PyTorch, and 174.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _destroy_dist_connection at 0x7be6bc0d41f0>
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 317, in _destroy_dist_connection
    torch.distributed.destroy_process_group()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2146, in destroy_process_group
    _shutdown_backend(pg_to_shutdown)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1815, in _shutdown_backend
    backend._shutdown()
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
[rank: 1] Child process with PID 1260704 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:48:53,036 - wandb.wandb_agent - INFO - Cleaning up finished run: nmzlfn7z
[2025-04-10 15:48:53,036][wandb.wandb_agent][INFO] - Cleaning up finished run: nmzlfn7z
2025-04-10 15:48:53,658 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:48:53,658][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:48:53,659 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4181276155632077
	config.input_dropout: 0.057820826139199144
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.23075366090719768
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 19
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0014598388512978638
	optimization_config.init_lr: 4.719087488662878e-08
	optimization_config.lr_decay_power: 2.8135852616008346
	optimization_config.lr_frac_warmup_steps: 3.0643961617304683e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0033090316035055303
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:48:53,659][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4181276155632077
	config.input_dropout: 0.057820826139199144
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.23075366090719768
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 19
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0014598388512978638
	optimization_config.init_lr: 4.719087488662878e-08
	optimization_config.lr_decay_power: 2.8135852616008346
	optimization_config.lr_frac_warmup_steps: 3.0643961617304683e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0033090316035055303
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:48:53,665 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4181276155632077 config.input_dropout=0.057820826139199144 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.23075366090719768 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=19 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0014598388512978638 optimization_config.init_lr=4.719087488662878e-08 optimization_config.lr_decay_power=2.8135852616008346 optimization_config.lr_frac_warmup_steps=3.0643961617304683e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0033090316035055303 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:48:53,665][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4181276155632077 config.input_dropout=0.057820826139199144 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.23075366090719768 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=19 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0014598388512978638 optimization_config.init_lr=4.719087488662878e-08 optimization_config.lr_decay_power=2.8135852616008346 optimization_config.lr_frac_warmup_steps=3.0643961617304683e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0033090316035055303 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:48:58,675 - wandb.wandb_agent - INFO - Running runs: ['flrbnaei']
[2025-04-10 15:48:58,675][wandb.wandb_agent][INFO] - Running runs: ['flrbnaei']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154903-flrbnaei
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/flrbnaei
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4181276155632077
Overwriting input_dropout in config from 0.4494236115512016 to 0.057820826139199144
Overwriting resid_dropout in config from 0.4939188761966135 to 0.23075366090719768
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4181276155632077
Overwriting input_dropout in config from 0.4494236115512016 to 0.057820826139199144
Overwriting resid_dropout in config from 0.4939188761966135 to 0.23075366090719768
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.42it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/5 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/5 [00:00<?, ?it/s] Epoch 0:  20%|‚ñà‚ñà        | 1/5 [00:03<00:12,  0.32it/s]Epoch 0:  20%|‚ñà‚ñà        | 1/5 [00:03<00:12,  0.32it/s, v_num=naei]Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:12<00:19,  0.16it/s, v_num=naei]Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:12<00:19,  0.16it/s, v_num=naei]Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:16<00:11,  0.18it/s, v_num=naei]Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:16<00:11,  0.18it/s, v_num=naei]Epoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:20<00:05,  0.20it/s, v_num=naei]Epoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:20<00:05,  0.20it/s, v_num=naei]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:21<00:00,  0.23it/s, v_num=naei]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:21<00:00,  0.23it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.68it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:23<00:00,  0.21it/s, v_num=naei]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:23<00:00,  0.21it/s, v_num=naei]Epoch 0:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]        Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]Epoch 1:  20%|‚ñà‚ñà        | 1/5 [00:03<00:15,  0.25it/s, v_num=naei]Epoch 1:  20%|‚ñà‚ñà        | 1/5 [00:03<00:15,  0.25it/s, v_num=naei]Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:06<00:09,  0.31it/s, v_num=naei]Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:06<00:09,  0.31it/s, v_num=naei]Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:08<00:05,  0.34it/s, v_num=naei]Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:08<00:05,  0.34it/s, v_num=naei]Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:10<00:02,  0.37it/s, v_num=naei]Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:10<00:02,  0.37it/s, v_num=naei]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  0.42it/s, v_num=naei]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  0.42it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.73it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  0.36it/s, v_num=naei]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  0.36it/s, v_num=naei]Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]        Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]Epoch 2:  20%|‚ñà‚ñà        | 1/5 [00:03<00:14,  0.28it/s, v_num=naei]Epoch 2:  20%|‚ñà‚ñà        | 1/5 [00:03<00:14,  0.28it/s, v_num=naei]Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:06<00:09,  0.32it/s, v_num=naei]Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:06<00:09,  0.32it/s, v_num=naei]Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:11<00:07,  0.27it/s, v_num=naei]Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:11<00:07,  0.27it/s, v_num=naei]Epoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:13<00:03,  0.30it/s, v_num=naei]Epoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:13<00:03,  0.30it/s, v_num=naei]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  0.35it/s, v_num=naei]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  0.35it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.88it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:16<00:00,  0.31it/s, v_num=naei]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:16<00:00,  0.31it/s, v_num=naei]Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]        Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]Epoch 3:  20%|‚ñà‚ñà        | 1/5 [00:02<00:09,  0.41it/s, v_num=naei]Epoch 3:  20%|‚ñà‚ñà        | 1/5 [00:02<00:09,  0.41it/s, v_num=naei]Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:06<00:09,  0.31it/s, v_num=naei]Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:06<00:09,  0.31it/s, v_num=naei]Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:09<00:06,  0.31it/s, v_num=naei]Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:09<00:06,  0.31it/s, v_num=naei]Epoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:12<00:03,  0.31it/s, v_num=naei]Epoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:12<00:03,  0.31it/s, v_num=naei]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  0.36it/s, v_num=naei]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  0.36it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.67it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:16<00:00,  0.30it/s, v_num=naei]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:16<00:00,  0.30it/s, v_num=naei]Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]        Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]Epoch 4:  20%|‚ñà‚ñà        | 1/5 [00:07<00:29,  0.13it/s, v_num=naei]Epoch 4:  20%|‚ñà‚ñà        | 1/5 [00:07<00:29,  0.13it/s, v_num=naei]Epoch 4:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:11<00:17,  0.17it/s, v_num=naei]Epoch 4:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:11<00:17,  0.17it/s, v_num=naei]Epoch 4:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:14<00:09,  0.20it/s, v_num=naei]Epoch 4:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:14<00:09,  0.20it/s, v_num=naei]Epoch 4:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:18<00:04,  0.22it/s, v_num=naei]Epoch 4:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:18<00:04,  0.22it/s, v_num=naei]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:20<00:00,  0.25it/s, v_num=naei]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:20<00:00,  0.25it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.53it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:22<00:00,  0.22it/s, v_num=naei]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:22<00:00,  0.22it/s, v_num=naei]Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]        Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]Epoch 5:  20%|‚ñà‚ñà        | 1/5 [00:05<00:20,  0.19it/s, v_num=naei]Epoch 5:  20%|‚ñà‚ñà        | 1/5 [00:05<00:20,  0.19it/s, v_num=naei]Epoch 5:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:09<00:14,  0.20it/s, v_num=naei]Epoch 5:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:09<00:14,  0.20it/s, v_num=naei]Epoch 5:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:12<00:08,  0.24it/s, v_num=naei]Epoch 5:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:12<00:08,  0.24it/s, v_num=naei]Epoch 5:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:15<00:03,  0.25it/s, v_num=naei]Epoch 5:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:15<00:03,  0.25it/s, v_num=naei]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:17<00:00,  0.29it/s, v_num=naei]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:17<00:00,  0.29it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.67it/s][A
                                                                      [AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:19<00:00,  0.26it/s, v_num=naei]Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:19<00:00,  0.26it/s, v_num=naei]Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]        Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]Epoch 6:  20%|‚ñà‚ñà        | 1/5 [00:04<00:16,  0.24it/s, v_num=naei]Epoch 6:  20%|‚ñà‚ñà        | 1/5 [00:04<00:16,  0.24it/s, v_num=naei]Epoch 6:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:08<00:13,  0.23it/s, v_num=naei]Epoch 6:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:08<00:13,  0.23it/s, v_num=naei]Epoch 6:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:11<00:07,  0.25it/s, v_num=naei]Epoch 6:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:11<00:07,  0.25it/s, v_num=naei]Epoch 6:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:15<00:03,  0.26it/s, v_num=naei]Epoch 6:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:15<00:03,  0.26it/s, v_num=naei]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:16<00:00,  0.31it/s, v_num=naei]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:16<00:00,  0.31it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.64it/s][A
                                                                      [AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:18<00:00,  0.27it/s, v_num=naei]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:18<00:00,  0.27it/s, v_num=naei]Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]        Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]Epoch 7:  20%|‚ñà‚ñà        | 1/5 [00:07<00:31,  0.13it/s, v_num=naei]Epoch 7:  20%|‚ñà‚ñà        | 1/5 [00:07<00:31,  0.13it/s, v_num=naei]Epoch 7:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:10<00:16,  0.18it/s, v_num=naei]Epoch 7:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:10<00:16,  0.18it/s, v_num=naei]Epoch 7:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:15<00:10,  0.20it/s, v_num=naei]Epoch 7:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:15<00:10,  0.20it/s, v_num=naei]Epoch 7:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:18<00:04,  0.21it/s, v_num=naei]Epoch 7:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:18<00:04,  0.21it/s, v_num=naei]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:20<00:00,  0.25it/s, v_num=naei]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:20<00:00,  0.25it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.36it/s][A
                                                                      [AEpoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:24<00:00,  0.21it/s, v_num=naei]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:24<00:00,  0.21it/s, v_num=naei]Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]        Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]Epoch 8:  20%|‚ñà‚ñà        | 1/5 [00:09<00:39,  0.10it/s, v_num=naei]Epoch 8:  20%|‚ñà‚ñà        | 1/5 [00:09<00:39,  0.10it/s, v_num=naei]Epoch 8:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:15<00:23,  0.13it/s, v_num=naei]Epoch 8:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:15<00:23,  0.13it/s, v_num=naei]Epoch 8:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:18<00:12,  0.16it/s, v_num=naei]Epoch 8:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:18<00:12,  0.16it/s, v_num=naei]Epoch 8:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:23<00:05,  0.17it/s, v_num=naei]Epoch 8:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:23<00:05,  0.17it/s, v_num=naei]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:24<00:00,  0.20it/s, v_num=naei]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:24<00:00,  0.20it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.59it/s][A
                                                                      [AEpoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:26<00:00,  0.19it/s, v_num=naei]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:26<00:00,  0.19it/s, v_num=naei]Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]        Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s, v_num=naei]Epoch 9:  20%|‚ñà‚ñà        | 1/5 [00:04<00:17,  0.23it/s, v_num=naei]Epoch 9:  20%|‚ñà‚ñà        | 1/5 [00:04<00:17,  0.23it/s, v_num=naei]Epoch 9:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:06<00:10,  0.29it/s, v_num=naei]Epoch 9:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:06<00:10,  0.29it/s, v_num=naei]Epoch 9:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:12<00:08,  0.24it/s, v_num=naei]Epoch 9:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:12<00:08,  0.24it/s, v_num=naei]Epoch 9:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:15<00:03,  0.25it/s, v_num=naei]Epoch 9:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:15<00:03,  0.25it/s, v_num=naei]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:17<00:00,  0.29it/s, v_num=naei]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:17<00:00,  0.29it/s, v_num=naei]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.51it/s][A
                                                                      [AEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:19<00:00,  0.25it/s, v_num=naei]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:19<00:00,  0.25it/s, v_num=naei]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:19<00:00,  0.25it/s, v_num=naei]Restoring states from the checkpoint path at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/flrbnaei/checkpoints/epoch=1-val_loss=0.00-best_model.ckpt
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/flrbnaei/checkpoints/epoch=1-val_loss=0.00-best_model.ckpt
/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.validate()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/finetune_weights
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/held_out_0.parquet

/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/finetune_weights
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/held_out_0.parquet
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/1 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.53it/s]Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.46it/s]Restoring states from the checkpoint path at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/flrbnaei/checkpoints/epoch=1-val_loss=0.00-best_model.ckpt
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/eneryield_ft_sweep/flrbnaei/checkpoints/epoch=1-val_loss=0.00-best_model.ckpt
/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          Validate metric                       DataLoader 0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             task_AUROC                             0.0
             task_loss                        855.43408203125
           tuning_TTE_MSE                       36153.765625
          tuning_TTE_MSLE                    8.172929763793945
         tuning_TTE_reg_NLL                  5.3571648597717285
     tuning_event_label_cls_NLL             0.16578711569309235
 tuning_event_label_macro_accuracy           0.921319842338562
 tuning_event_label_micro_accuracy           0.9213199019432068
 tuning_event_label_weighted_AUROC           0.7612841129302979
tuning_event_label_weighted_accuracy         0.7116814851760864
     tuning_event_type_cls_NLL              0.009485996328294277
  tuning_event_type_macro_accuracy                  0.0
  tuning_event_type_micro_accuracy                  0.0
  tuning_event_type_weighted_AUROC                  0.0
tuning_event_type_weighted_accuracy                 0.0
        tuning_feature_0_MSE                0.12240942567586899
      tuning_feature_0_reg_NLL              -0.42026737332344055
       tuning_feature_10_MSE                 0.7483469247817993
     tuning_feature_10_reg_NLL               0.8133440613746643
       tuning_feature_11_MSE                 1.766588807106018
     tuning_feature_11_reg_NLL               1.1870008707046509
       tuning_feature_12_MSE                 1.5924315452575684
     tuning_feature_12_reg_NLL               1.1328083276748657
       tuning_feature_13_MSE                 2.1731700897216797
     tuning_feature_13_reg_NLL               1.4595392942428589
       tuning_feature_14_MSE                 0.7452930808067322
     tuning_feature_14_reg_NLL               0.7714992165565491
       tuning_feature_15_MSE                 1.3814414739608765
     tuning_feature_15_reg_NLL               1.2632701396942139
       tuning_feature_16_MSE                 1.1025108098983765
     tuning_feature_16_reg_NLL               1.1501784324645996
       tuning_feature_17_MSE                 1.2570431232452393
     tuning_feature_17_reg_NLL               1.1925406455993652
       tuning_feature_18_MSE                 1.2459003925323486
     tuning_feature_18_reg_NLL               0.9697883725166321
       tuning_feature_19_MSE                 0.521572470664978
     tuning_feature_19_reg_NLL               0.6347994804382324
        tuning_feature_1_MSE                 0.5665619373321533
      tuning_feature_1_reg_NLL               0.761890709400177
       tuning_feature_20_MSE                 0.4712510108947754
     tuning_feature_20_reg_NLL               0.5545766949653625
       tuning_feature_21_MSE                 0.4692358374595642
     tuning_feature_21_reg_NLL               0.5325902700424194
       tuning_feature_22_MSE                 7.024022579193115
     tuning_feature_22_reg_NLL               5.809607982635498
       tuning_feature_23_MSE                 6.572465419769287
     tuning_feature_23_reg_NLL               1.2000455856323242
       tuning_feature_24_MSE                 15.912240028381348
     tuning_feature_24_reg_NLL               2.643648386001587
        tuning_feature_2_MSE                 1.6626224517822266
      tuning_feature_2_reg_NLL               0.3038087785243988
        tuning_feature_3_MSE                 2.0014548301696777
      tuning_feature_3_reg_NLL              0.40539035201072693
        tuning_feature_4_MSE                 1.5745282173156738
      tuning_feature_4_reg_NLL              -0.11811038106679916
        tuning_feature_5_MSE                 0.6397171020507812
      tuning_feature_5_reg_NLL              0.07900045067071915
        tuning_feature_6_MSE                 0.9837509393692017
      tuning_feature_6_reg_NLL               1.0305299758911133
        tuning_feature_7_MSE                 1.466652750968933
      tuning_feature_7_reg_NLL               1.3758498430252075
        tuning_feature_8_MSE                 1.4127318859100342
      tuning_feature_8_reg_NLL               1.286118984222412
        tuning_feature_9_MSE                 1.5871330499649048
      tuning_feature_9_reg_NLL               1.2815624475479126
            tuning_loss                      888.2674560546875
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  0.32it/s]Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  0.29it/s]wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                  epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                       held_out_TTE_MSE ‚ñÅ
wandb:                      held_out_TTE_MSLE ‚ñÅ
wandb:                   held_out_TTE_reg_NLL ‚ñÅ
wandb:           held_out_event_label_cls_NLL ‚ñÅ
wandb:    held_out_event_label_macro_accuracy ‚ñÅ
wandb:    held_out_event_label_micro_accuracy ‚ñÅ
wandb:    held_out_event_label_weighted_AUROC ‚ñÅ
wandb: held_out_event_label_weighted_accuracy ‚ñÅ
wandb:            held_out_event_type_cls_NLL ‚ñÅ
wandb:     held_out_event_type_macro_accuracy ‚ñÅ
wandb:     held_out_event_type_micro_accuracy ‚ñÅ
wandb:     held_out_event_type_weighted_AUROC ‚ñÅ
wandb:  held_out_event_type_weighted_accuracy ‚ñÅ
wandb:                 held_out_feature_0_MSE ‚ñÅ
wandb:             held_out_feature_0_reg_NLL ‚ñÅ
wandb:                held_out_feature_10_MSE ‚ñÅ
wandb:            held_out_feature_10_reg_NLL ‚ñÅ
wandb:                held_out_feature_11_MSE ‚ñÅ
wandb:            held_out_feature_11_reg_NLL ‚ñÅ
wandb:                held_out_feature_12_MSE ‚ñÅ
wandb:            held_out_feature_12_reg_NLL ‚ñÅ
wandb:                held_out_feature_13_MSE ‚ñÅ
wandb:            held_out_feature_13_reg_NLL ‚ñÅ
wandb:                held_out_feature_14_MSE ‚ñÅ
wandb:            held_out_feature_14_reg_NLL ‚ñÅ
wandb:                held_out_feature_15_MSE ‚ñÅ
wandb:            held_out_feature_15_reg_NLL ‚ñÅ
wandb:                held_out_feature_16_MSE ‚ñÅ
wandb:            held_out_feature_16_reg_NLL ‚ñÅ
wandb:                held_out_feature_17_MSE ‚ñÅ
wandb:            held_out_feature_17_reg_NLL ‚ñÅ
wandb:                held_out_feature_18_MSE ‚ñÅ
wandb:            held_out_feature_18_reg_NLL ‚ñÅ
wandb:                held_out_feature_19_MSE ‚ñÅ
wandb:            held_out_feature_19_reg_NLL ‚ñÅ
wandb:                 held_out_feature_1_MSE ‚ñÅ
wandb:             held_out_feature_1_reg_NLL ‚ñÅ
wandb:                held_out_feature_20_MSE ‚ñÅ
wandb:            held_out_feature_20_reg_NLL ‚ñÅ
wandb:                held_out_feature_21_MSE ‚ñÅ
wandb:            held_out_feature_21_reg_NLL ‚ñÅ
wandb:                held_out_feature_22_MSE ‚ñÅ
wandb:            held_out_feature_22_reg_NLL ‚ñÅ
wandb:                held_out_feature_23_MSE ‚ñÅ
wandb:            held_out_feature_23_reg_NLL ‚ñÅ
wandb:                held_out_feature_24_MSE ‚ñÅ
wandb:            held_out_feature_24_reg_NLL ‚ñÅ
wandb:                 held_out_feature_2_MSE ‚ñÅ
wandb:             held_out_feature_2_reg_NLL ‚ñÅ
wandb:                 held_out_feature_3_MSE ‚ñÅ
wandb:             held_out_feature_3_reg_NLL ‚ñÅ
wandb:                 held_out_feature_4_MSE ‚ñÅ
wandb:             held_out_feature_4_reg_NLL ‚ñÅ
wandb:                 held_out_feature_5_MSE ‚ñÅ
wandb:             held_out_feature_5_reg_NLL ‚ñÅ
wandb:                 held_out_feature_6_MSE ‚ñÅ
wandb:             held_out_feature_6_reg_NLL ‚ñÅ
wandb:                 held_out_feature_7_MSE ‚ñÅ
wandb:             held_out_feature_7_reg_NLL ‚ñÅ
wandb:                 held_out_feature_8_MSE ‚ñÅ
wandb:             held_out_feature_8_reg_NLL ‚ñÅ
wandb:                 held_out_feature_9_MSE ‚ñÅ
wandb:             held_out_feature_9_reg_NLL ‚ñÅ
wandb:                          held_out_loss ‚ñÅ
wandb:                               lr-AdamW ‚ñÅ
wandb:                             task_AUROC ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                              task_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:                             train_loss ‚ñÅ
wandb:                    trainer/global_step ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                         tuning_TTE_MSE ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÅ
wandb:                        tuning_TTE_MSLE ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÇ
wandb:                     tuning_TTE_reg_NLL ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñÅ
wandb:             tuning_event_label_cls_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÇ
wandb:      tuning_event_label_macro_accuracy ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:      tuning_event_label_micro_accuracy ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:      tuning_event_label_weighted_AUROC ‚ñÉ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ
wandb:   tuning_event_label_weighted_accuracy ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:              tuning_event_type_cls_NLL ‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:       tuning_event_type_macro_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       tuning_event_type_micro_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       tuning_event_type_weighted_AUROC ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    tuning_event_type_weighted_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                   tuning_feature_0_MSE ‚ñÅ‚ñà‚ñá‚ñà‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÅ
wandb:               tuning_feature_0_reg_NLL ‚ñà‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ
wandb:                  tuning_feature_10_MSE ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñà‚ñÜ
wandb:              tuning_feature_10_reg_NLL ‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñá
wandb:                  tuning_feature_11_MSE ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñá‚ñà‚ñÑ‚ñá‚ñà‚ñà‚ñÉ
wandb:              tuning_feature_11_reg_NLL ‚ñÇ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñà‚ñÅ
wandb:                  tuning_feature_12_MSE ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÖ
wandb:              tuning_feature_12_reg_NLL ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÜ
wandb:                  tuning_feature_13_MSE ‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñà‚ñÇ
wandb:              tuning_feature_13_reg_NLL ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñà‚ñÅ
wandb:                  tuning_feature_14_MSE ‚ñÉ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ
wandb:              tuning_feature_14_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñá
wandb:                  tuning_feature_15_MSE ‚ñà‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÑ‚ñà
wandb:              tuning_feature_15_reg_NLL ‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÇ‚ñÉ‚ñÅ‚ñá
wandb:                  tuning_feature_16_MSE ‚ñÑ‚ñá‚ñà‚ñÑ‚ñÇ‚ñá‚ñÜ‚ñÅ‚ñÖ‚ñá‚ñÑ
wandb:              tuning_feature_16_reg_NLL ‚ñá‚ñá‚ñá‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÅ‚ñÉ‚ñÉ‚ñá
wandb:                  tuning_feature_17_MSE ‚ñÜ‚ñÜ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÜ
wandb:              tuning_feature_17_reg_NLL ‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñà
wandb:                  tuning_feature_18_MSE ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÑ‚ñÑ
wandb:              tuning_feature_18_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñá
wandb:                  tuning_feature_19_MSE ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñà
wandb:              tuning_feature_19_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÇ
wandb:                   tuning_feature_1_MSE ‚ñá‚ñÑ‚ñÜ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñà‚ñá‚ñÉ‚ñá
wandb:               tuning_feature_1_reg_NLL ‚ñà‚ñÇ‚ñá‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                  tuning_feature_20_MSE ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÖ
wandb:              tuning_feature_20_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÇ
wandb:                  tuning_feature_21_MSE ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÅ‚ñá‚ñÖ‚ñÉ
wandb:              tuning_feature_21_reg_NLL ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÇ
wandb:                  tuning_feature_22_MSE ‚ñá‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÖ‚ñá
wandb:              tuning_feature_22_reg_NLL ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb:                  tuning_feature_23_MSE ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñà‚ñá‚ñÑ‚ñÜ‚ñÅ‚ñÅ‚ñÉ
wandb:              tuning_feature_23_reg_NLL ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÇ
wandb:                  tuning_feature_24_MSE ‚ñà‚ñà‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñà
wandb:              tuning_feature_24_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÇ
wandb:                   tuning_feature_2_MSE ‚ñÖ‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñá‚ñÖ
wandb:               tuning_feature_2_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñá
wandb:                   tuning_feature_3_MSE ‚ñà‚ñÅ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñÜ‚ñà
wandb:               tuning_feature_3_reg_NLL ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñà
wandb:                   tuning_feature_4_MSE ‚ñÜ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñà‚ñÑ‚ñÉ‚ñÜ
wandb:               tuning_feature_4_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñá
wandb:                   tuning_feature_5_MSE ‚ñá‚ñÉ‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñá
wandb:               tuning_feature_5_reg_NLL ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñá
wandb:                   tuning_feature_6_MSE ‚ñà‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñà
wandb:               tuning_feature_6_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÇ
wandb:                   tuning_feature_7_MSE ‚ñÅ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñÅ
wandb:               tuning_feature_7_reg_NLL ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñà‚ñÉ
wandb:                   tuning_feature_8_MSE ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÅ
wandb:               tuning_feature_8_reg_NLL ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÇ
wandb:                   tuning_feature_9_MSE ‚ñÑ‚ñÇ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñÅ‚ñÖ‚ñÑ
wandb:               tuning_feature_9_reg_NLL ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÇ
wandb:                            tuning_loss ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                  epoch 10
wandb:                       held_out_TTE_MSE 55615.07031
wandb:                      held_out_TTE_MSLE 9.84146
wandb:                   held_out_TTE_reg_NLL 5.43611
wandb:           held_out_event_label_cls_NLL 0.17515
wandb:    held_out_event_label_macro_accuracy 0.94524
wandb:    held_out_event_label_micro_accuracy 0.94524
wandb:    held_out_event_label_weighted_AUROC 0.86184
wandb: held_out_event_label_weighted_accuracy 0.84584
wandb:            held_out_event_type_cls_NLL 0.01683
wandb:     held_out_event_type_macro_accuracy 0.0
wandb:     held_out_event_type_micro_accuracy 0.0
wandb:     held_out_event_type_weighted_AUROC 0.0
wandb:  held_out_event_type_weighted_accuracy 0.0
wandb:                 held_out_feature_0_MSE 1.16432
wandb:             held_out_feature_0_reg_NLL -0.50097
wandb:                held_out_feature_10_MSE 1.20363
wandb:            held_out_feature_10_reg_NLL 1.57234
wandb:                held_out_feature_11_MSE 2.44445
wandb:            held_out_feature_11_reg_NLL 1.64599
wandb:                held_out_feature_12_MSE 2.92813
wandb:            held_out_feature_12_reg_NLL 1.92632
wandb:                held_out_feature_13_MSE 2.78141
wandb:            held_out_feature_13_reg_NLL 2.47879
wandb:                held_out_feature_14_MSE 2.77747
wandb:            held_out_feature_14_reg_NLL 0.98616
wandb:                held_out_feature_15_MSE 1.42082
wandb:            held_out_feature_15_reg_NLL 1.30391
wandb:                held_out_feature_16_MSE 1.30369
wandb:            held_out_feature_16_reg_NLL 1.237
wandb:                held_out_feature_17_MSE 1.37408
wandb:            held_out_feature_17_reg_NLL 1.24549
wandb:                held_out_feature_18_MSE 2.16785
wandb:            held_out_feature_18_reg_NLL 1.43571
wandb:                held_out_feature_19_MSE 0.88918
wandb:            held_out_feature_19_reg_NLL 0.80666
wandb:                 held_out_feature_1_MSE 1.86001
wandb:             held_out_feature_1_reg_NLL 0.66749
wandb:                held_out_feature_20_MSE 0.80699
wandb:            held_out_feature_20_reg_NLL 0.82084
wandb:                held_out_feature_21_MSE 0.82576
wandb:            held_out_feature_21_reg_NLL 0.78697
wandb:                held_out_feature_22_MSE 0.35365
wandb:            held_out_feature_22_reg_NLL 0.4141
wandb:                held_out_feature_23_MSE 1.96936
wandb:            held_out_feature_23_reg_NLL 0.97194
wandb:                held_out_feature_24_MSE 1.36956
wandb:            held_out_feature_24_reg_NLL 0.43966
wandb:                 held_out_feature_2_MSE 1.71489
wandb:             held_out_feature_2_reg_NLL 1.51181
wandb:                 held_out_feature_3_MSE 1.35027
wandb:             held_out_feature_3_reg_NLL 1.06937
wandb:                 held_out_feature_4_MSE 1.57173
wandb:             held_out_feature_4_reg_NLL 0.95865
wandb:                 held_out_feature_5_MSE 1.71422
wandb:             held_out_feature_5_reg_NLL 2.58949
wandb:                 held_out_feature_6_MSE 0.80828
wandb:             held_out_feature_6_reg_NLL 0.92121
wandb:                 held_out_feature_7_MSE 1.87058
wandb:             held_out_feature_7_reg_NLL 1.4262
wandb:                 held_out_feature_8_MSE 1.44002
wandb:             held_out_feature_8_reg_NLL 1.25876
wandb:                 held_out_feature_9_MSE 1.83637
wandb:             held_out_feature_9_reg_NLL 1.33709
wandb:                          held_out_loss 1027.88049
wandb:                               lr-AdamW 0.0
wandb:                             task_AUROC 0.0
wandb:                              task_loss 992.94141
wandb:                             train_loss 757.60938
wandb:                    trainer/global_step 50
wandb:                         tuning_TTE_MSE 36153.76562
wandb:                        tuning_TTE_MSLE 8.17293
wandb:                     tuning_TTE_reg_NLL 5.35716
wandb:             tuning_event_label_cls_NLL 0.16579
wandb:      tuning_event_label_macro_accuracy 0.92132
wandb:      tuning_event_label_micro_accuracy 0.92132
wandb:      tuning_event_label_weighted_AUROC 0.76128
wandb:   tuning_event_label_weighted_accuracy 0.71168
wandb:              tuning_event_type_cls_NLL 0.00949
wandb:       tuning_event_type_macro_accuracy 0.0
wandb:       tuning_event_type_micro_accuracy 0.0
wandb:       tuning_event_type_weighted_AUROC 0.0
wandb:    tuning_event_type_weighted_accuracy 0.0
wandb:                   tuning_feature_0_MSE 0.12241
wandb:               tuning_feature_0_reg_NLL -0.42027
wandb:                  tuning_feature_10_MSE 0.74835
wandb:              tuning_feature_10_reg_NLL 0.81334
wandb:                  tuning_feature_11_MSE 1.76659
wandb:              tuning_feature_11_reg_NLL 1.187
wandb:                  tuning_feature_12_MSE 1.59243
wandb:              tuning_feature_12_reg_NLL 1.13281
wandb:                  tuning_feature_13_MSE 2.17317
wandb:              tuning_feature_13_reg_NLL 1.45954
wandb:                  tuning_feature_14_MSE 0.74529
wandb:              tuning_feature_14_reg_NLL 0.7715
wandb:                  tuning_feature_15_MSE 1.38144
wandb:              tuning_feature_15_reg_NLL 1.26327
wandb:                  tuning_feature_16_MSE 1.10251
wandb:              tuning_feature_16_reg_NLL 1.15018
wandb:                  tuning_feature_17_MSE 1.25704
wandb:              tuning_feature_17_reg_NLL 1.19254
wandb:                  tuning_feature_18_MSE 1.2459
wandb:              tuning_feature_18_reg_NLL 0.96979
wandb:                  tuning_feature_19_MSE 0.52157
wandb:              tuning_feature_19_reg_NLL 0.6348
wandb:                   tuning_feature_1_MSE 0.56656
wandb:               tuning_feature_1_reg_NLL 0.76189
wandb:                  tuning_feature_20_MSE 0.47125
wandb:              tuning_feature_20_reg_NLL 0.55458
wandb:                  tuning_feature_21_MSE 0.46924
wandb:              tuning_feature_21_reg_NLL 0.53259
wandb:                  tuning_feature_22_MSE 7.02402
wandb:              tuning_feature_22_reg_NLL 5.80961
wandb:                  tuning_feature_23_MSE 6.57247
wandb:              tuning_feature_23_reg_NLL 1.20005
wandb:                  tuning_feature_24_MSE 15.91224
wandb:              tuning_feature_24_reg_NLL 2.64365
wandb:                   tuning_feature_2_MSE 1.66262
wandb:               tuning_feature_2_reg_NLL 0.30381
wandb:                   tuning_feature_3_MSE 2.00145
wandb:               tuning_feature_3_reg_NLL 0.40539
wandb:                   tuning_feature_4_MSE 1.57453
wandb:               tuning_feature_4_reg_NLL -0.11811
wandb:                   tuning_feature_5_MSE 0.63972
wandb:               tuning_feature_5_reg_NLL 0.079
wandb:                   tuning_feature_6_MSE 0.98375
wandb:               tuning_feature_6_reg_NLL 1.03053
wandb:                   tuning_feature_7_MSE 1.46665
wandb:               tuning_feature_7_reg_NLL 1.37585
wandb:                   tuning_feature_8_MSE 1.41273
wandb:               tuning_feature_8_reg_NLL 1.28612
wandb:                   tuning_feature_9_MSE 1.58713
wandb:               tuning_feature_9_reg_NLL 1.28156
wandb:                            tuning_loss 888.26746
wandb: 
wandb: üöÄ View run generative_event_stream_transformer at: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/flrbnaei
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_154903-flrbnaei/logs

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             Test metric                           DataLoader 0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
           held_out_TTE_MSE                       55615.0703125
          held_out_TTE_MSLE                     9.841461181640625
         held_out_TTE_reg_NLL                   5.436107635498047
     held_out_event_label_cls_NLL               0.1751469075679779
 held_out_event_label_macro_accuracy            0.9452396035194397
 held_out_event_label_micro_accuracy            0.9452396035194397
 held_out_event_label_weighted_AUROC            0.8618412613868713
held_out_event_label_weighted_accuracy          0.8458359241485596
     held_out_event_type_cls_NLL               0.01683291234076023
  held_out_event_type_macro_accuracy                   0.0
  held_out_event_type_micro_accuracy                   0.0
  held_out_event_type_weighted_AUROC                   0.0
held_out_event_type_weighted_accuracy                  0.0
        held_out_feature_0_MSE                  1.1643246412277222
      held_out_feature_0_reg_NLL               -0.5009706616401672
       held_out_feature_10_MSE                  1.2036341428756714
     held_out_feature_10_reg_NLL                1.572339653968811
       held_out_feature_11_MSE                  2.4444451332092285
     held_out_feature_11_reg_NLL                1.645986795425415
       held_out_feature_12_MSE                  2.9281277656555176
     held_out_feature_12_reg_NLL                1.9263229370117188
       held_out_feature_13_MSE                  2.781414031982422
     held_out_feature_13_reg_NLL                2.4787914752960205
       held_out_feature_14_MSE                  2.777474880218506
     held_out_feature_14_reg_NLL                0.9861553311347961
       held_out_feature_15_MSE                  1.4208208322525024
     held_out_feature_15_reg_NLL                1.3039072751998901
       held_out_feature_16_MSE                  1.303694248199463
     held_out_feature_16_reg_NLL                1.2370012998580933
       held_out_feature_17_MSE                  1.374081015586853
     held_out_feature_17_reg_NLL                1.2454887628555298
       held_out_feature_18_MSE                  2.167850971221924
     held_out_feature_18_reg_NLL                1.4357051849365234
       held_out_feature_19_MSE                  0.8891777396202087
     held_out_feature_19_reg_NLL                0.8066573739051819
        held_out_feature_1_MSE                  1.8600132465362549
      held_out_feature_1_reg_NLL                0.6674857139587402
       held_out_feature_20_MSE                  0.806989848613739
     held_out_feature_20_reg_NLL                0.8208429217338562
       held_out_feature_21_MSE                  0.8257582783699036
     held_out_feature_21_reg_NLL                0.7869726419448853
       held_out_feature_22_MSE                  0.3536505401134491
     held_out_feature_22_reg_NLL               0.41409745812416077
       held_out_feature_23_MSE                  1.9693584442138672
     held_out_feature_23_reg_NLL                0.9719377756118774
       held_out_feature_24_MSE                  1.3695613145828247
     held_out_feature_24_reg_NLL               0.43965601921081543
        held_out_feature_2_MSE                  1.7148929834365845
      held_out_feature_2_reg_NLL                1.5118058919906616
        held_out_feature_3_MSE                  1.3502659797668457
      held_out_feature_3_reg_NLL                1.0693700313568115
        held_out_feature_4_MSE                  1.5717326402664185
      held_out_feature_4_reg_NLL                0.9586504697799683
        held_out_feature_5_MSE                  1.7142153978347778
      held_out_feature_5_reg_NLL                2.5894880294799805
        held_out_feature_6_MSE                  0.8082754611968994
      held_out_feature_6_reg_NLL                0.9212056398391724
        held_out_feature_7_MSE                  1.8705792427062988
      held_out_feature_7_reg_NLL                1.4262018203735352
        held_out_feature_8_MSE                  1.4400173425674438
      held_out_feature_8_reg_NLL                1.2587581872940063
        held_out_feature_9_MSE                  1.8363707065582275
      held_out_feature_9_reg_NLL                1.3370872735977173
            held_out_loss                       1027.8804931640625
              task_AUROC                               0.0
              task_loss                            992.94140625
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Saving final metrics...
2025-04-10 15:53:01,688 - wandb.wandb_agent - INFO - Cleaning up finished run: flrbnaei
[2025-04-10 15:53:01,688][wandb.wandb_agent][INFO] - Cleaning up finished run: flrbnaei
2025-04-10 15:53:02,209 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:53:02,209][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:53:02,210 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4334623935879534
	config.input_dropout: 0.1821731921499698
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.2718275479576162
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 13
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.05180662438183981
	optimization_config.init_lr: 1.1031322330455999e-07
	optimization_config.lr_decay_power: 0.808566576337054
	optimization_config.lr_frac_warmup_steps: 3.0040035009789075e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009566790436369007
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:53:02,210][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4334623935879534
	config.input_dropout: 0.1821731921499698
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.2718275479576162
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 13
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.05180662438183981
	optimization_config.init_lr: 1.1031322330455999e-07
	optimization_config.lr_decay_power: 0.808566576337054
	optimization_config.lr_frac_warmup_steps: 3.0040035009789075e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009566790436369007
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:53:02,219 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4334623935879534 config.input_dropout=0.1821731921499698 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.2718275479576162 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=13 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.05180662438183981 optimization_config.init_lr=1.1031322330455999e-07 optimization_config.lr_decay_power=0.808566576337054 optimization_config.lr_frac_warmup_steps=3.0040035009789075e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009566790436369007 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:53:02,219][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4334623935879534 config.input_dropout=0.1821731921499698 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.2718275479576162 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=13 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.05180662438183981 optimization_config.init_lr=1.1031322330455999e-07 optimization_config.lr_decay_power=0.808566576337054 optimization_config.lr_frac_warmup_steps=3.0040035009789075e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009566790436369007 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:53:07,232 - wandb.wandb_agent - INFO - Running runs: ['7k15vy7l']
[2025-04-10 15:53:07,232][wandb.wandb_agent][INFO] - Running runs: ['7k15vy7l']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_155310-7k15vy7l
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/7k15vy7l
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4334623935879534
Overwriting input_dropout in config from 0.4494236115512016 to 0.1821731921499698
Overwriting resid_dropout in config from 0.4939188761966135 to 0.2718275479576162
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.4334623935879534
Overwriting input_dropout in config from 0.4494236115512016 to 0.1821731921499698
Overwriting resid_dropout in config from 0.4939188761966135 to 0.2718275479576162
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.70it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/7 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s] Epoch 0:  14%|‚ñà‚ñç        | 1/7 [00:05<00:35,  0.17it/s]Epoch 0:  14%|‚ñà‚ñç        | 1/7 [00:05<00:35,  0.17it/s, v_num=vy7l]Epoch 0:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:08<00:20,  0.25it/s, v_num=vy7l]Epoch 0:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:08<00:20,  0.25it/s, v_num=vy7l]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:10<00:13,  0.30it/s, v_num=vy7l]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:10<00:13,  0.30it/s, v_num=vy7l]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:12<00:09,  0.32it/s, v_num=vy7l]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:12<00:09,  0.32it/s, v_num=vy7l]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:14<00:05,  0.35it/s, v_num=vy7l]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:14<00:05,  0.35it/s, v_num=vy7l]Epoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:16<00:02,  0.36it/s, v_num=vy7l]Epoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:16<00:02,  0.36it/s, v_num=vy7l]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:18<00:00,  0.37it/s, v_num=vy7l]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:18<00:00,  0.37it/s, v_num=vy7l]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.52it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  0.33it/s, v_num=vy7l]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  0.33it/s, v_num=vy7l]Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l]        Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l]Epoch 1:  14%|‚ñà‚ñç        | 1/7 [00:06<00:41,  0.14it/s, v_num=vy7l]Epoch 1:  14%|‚ñà‚ñç        | 1/7 [00:06<00:41,  0.14it/s, v_num=vy7l]Epoch 1:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:09<00:23,  0.22it/s, v_num=vy7l]Epoch 1:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:09<00:23,  0.22it/s, v_num=vy7l]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:11<00:15,  0.26it/s, v_num=vy7l]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:11<00:15,  0.26it/s, v_num=vy7l]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:14<00:10,  0.28it/s, v_num=vy7l]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:14<00:10,  0.28it/s, v_num=vy7l]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:17<00:07,  0.28it/s, v_num=vy7l]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:17<00:07,  0.28it/s, v_num=vy7l]Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:19<00:03,  0.30it/s, v_num=vy7l]Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:19<00:03,  0.30it/s, v_num=vy7l]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.34it/s, v_num=vy7l]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.34it/s, v_num=vy7l]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.61it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  0.29it/s, v_num=vy7l]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  0.29it/s, v_num=vy7l]Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l]        Epoch 2:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l]Epoch 2:  14%|‚ñà‚ñç        | 1/7 [00:03<00:18,  0.33it/s, v_num=vy7l]Epoch 2:  14%|‚ñà‚ñç        | 1/7 [00:03<00:18,  0.33it/s, v_num=vy7l]Epoch 2:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:06<00:16,  0.31it/s, v_num=vy7l]Epoch 2:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:06<00:16,  0.31it/s, v_num=vy7l]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:08<00:11,  0.34it/s, v_num=vy7l]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:08<00:11,  0.34it/s, v_num=vy7l]Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:11<00:08,  0.35it/s, v_num=vy7l]Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:11<00:08,  0.35it/s, v_num=vy7l]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:13<00:05,  0.38it/s, v_num=vy7l]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:13<00:05,  0.38it/s, v_num=vy7l]Epoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.39it/s, v_num=vy7l]Epoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.39it/s, v_num=vy7l]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.42it/s, v_num=vy7l]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.42it/s, v_num=vy7l]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.38it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.35it/s, v_num=vy7l]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.35it/s, v_num=vy7l]Epoch 2:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l]        Epoch 3:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l]Epoch 3:  14%|‚ñà‚ñç        | 1/7 [00:03<00:19,  0.31it/s, v_num=vy7l]Epoch 3:  14%|‚ñà‚ñç        | 1/7 [00:03<00:19,  0.31it/s, v_num=vy7l]Epoch 3:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:06<00:16,  0.31it/s, v_num=vy7l]Epoch 3:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:06<00:16,  0.31it/s, v_num=vy7l]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:08<00:11,  0.33it/s, v_num=vy7l]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:08<00:11,  0.33it/s, v_num=vy7l]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:11<00:08,  0.35it/s, v_num=vy7l]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:11<00:08,  0.35it/s, v_num=vy7l]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:13<00:05,  0.37it/s, v_num=vy7l]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:13<00:05,  0.37it/s, v_num=vy7l]Epoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.38it/s, v_num=vy7l]Epoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:15<00:02,  0.38it/s, v_num=vy7l]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.42it/s, v_num=vy7l]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  0.42it/s, v_num=vy7l]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.60it/s][A
                                                                      [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.35it/s, v_num=vy7l]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  0.35it/s, v_num=vy7l]Epoch 3:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l]        Epoch 4:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l]Epoch 4:  14%|‚ñà‚ñç        | 1/7 [00:02<00:16,  0.37it/s, v_num=vy7l]Epoch 4:  14%|‚ñà‚ñç        | 1/7 [00:02<00:16,  0.37it/s, v_num=vy7l]Epoch 4:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:06<00:15,  0.33it/s, v_num=vy7l]Epoch 4:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:06<00:15,  0.33it/s, v_num=vy7l]Epoch 4:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:08<00:10,  0.37it/s, v_num=vy7l]Epoch 4:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:08<00:10,  0.37it/s, v_num=vy7l]Epoch 4:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:12<00:09,  0.32it/s, v_num=vy7l]Epoch 4:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:12<00:09,  0.32it/s, v_num=vy7l]Epoch 4:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:19<00:07,  0.26it/s, v_num=vy7l]Epoch 4:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:19<00:07,  0.26it/s, v_num=vy7l]Epoch 4:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:22<00:03,  0.26it/s, v_num=vy7l]Epoch 4:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:22<00:03,  0.26it/s, v_num=vy7l]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  0.29it/s, v_num=vy7l]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  0.29it/s, v_num=vy7l]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.72it/s][A
                                                                      [AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  0.27it/s, v_num=vy7l]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  0.27it/s, v_num=vy7l]Epoch 4:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l]        Epoch 5:   0%|          | 0/7 [00:00<?, ?it/s, v_num=vy7l][rank: 1] Child process with PID 1270294 terminated with code -9. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 15:55:21,639 - wandb.wandb_agent - INFO - Cleaning up finished run: 7k15vy7l
[2025-04-10 15:55:21,639][wandb.wandb_agent][INFO] - Cleaning up finished run: 7k15vy7l
2025-04-10 15:55:22,131 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 15:55:22,131][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 15:55:22,131 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.07953876474545629
	config.input_dropout: 0.2392782879767836
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.17413580146888963
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 25
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0003065931399677365
	optimization_config.init_lr: 0.001880542204028268
	optimization_config.lr_decay_power: 3.2260586299577403
	optimization_config.lr_frac_warmup_steps: 1.230119625872728e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0084796979849496
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 15:55:22,131][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.07953876474545629
	config.input_dropout: 0.2392782879767836
	config.is_cls_dist: True
	config.is_event_classification: False
	config.resid_dropout: 0.17413580146888963
	config.save_metrics: False
	config.save_metrics_fp: /home/filip-marcus/resutls/eneryield/test
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 25
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0003065931399677365
	optimization_config.init_lr: 0.001880542204028268
	optimization_config.lr_decay_power: 3.2260586299577403
	optimization_config.lr_frac_warmup_steps: 1.230119625872728e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0084796979849496
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_class_dist
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 15:55:22,138 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.07953876474545629 config.input_dropout=0.2392782879767836 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.17413580146888963 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=25 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0003065931399677365 optimization_config.init_lr=0.001880542204028268 optimization_config.lr_decay_power=3.2260586299577403 optimization_config.lr_frac_warmup_steps=1.230119625872728e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0084796979849496 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 15:55:22,138][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.07953876474545629 config.input_dropout=0.2392782879767836 config.is_cls_dist=True config.is_event_classification=False config.resid_dropout=0.17413580146888963 config.save_metrics=False config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=25 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0003065931399677365 optimization_config.init_lr=0.001880542204028268 optimization_config.lr_decay_power=3.2260586299577403 optimization_config.lr_frac_warmup_steps=1.230119625872728e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0084796979849496 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_class_dist trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 15:55:27,148 - wandb.wandb_agent - INFO - Running runs: ['p0u57d93']
[2025-04-10 15:55:27,148][wandb.wandb_agent][INFO] - Running runs: ['p0u57d93']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/wandb/run-20250410_155529-p0u57d93
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/rl522wkb
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/p0u57d93
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.5 M  | eval 
---------------------------------------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
6.148     Total estimated model params size (MB)
92        Modules in train mode
197       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.07953876474545629
Overwriting input_dropout in config from 0.4494236115512016 to 0.2392782879767836
Overwriting resid_dropout in config from 0.4939188761966135 to 0.17413580146888963
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_class_dist/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=256, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_class_dist', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.004563095202856138 to 0.07953876474545629
Overwriting input_dropout in config from 0.4494236115512016 to 0.2392782879767836
Overwriting resid_dropout in config from 0.4939188761966135 to 0.17413580146888963
Overwriting is_cls_dist in config from False to True
Overwriting is_event_classification in config from False to False
Overwriting save_metrics in config from True to False
Overwriting save_metrics_fp in config from /home/filip-marcus/results/eneryield/pretrain_10_04_2025 to /home/filip-marcus/resutls/eneryield/test
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/train_0.parquet
Re-loading task data for task_df_eneryield_class_dist from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_class_dist/tuning_0.parquet
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  0.48it/s]                                                                           /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/4 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s] Epoch 0:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:09<00:29,  0.10it/s]Epoch 0:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:09<00:29,  0.10it/s, v_num=7d93]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:24<00:24,  0.08it/s, v_num=7d93]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:24<00:24,  0.08it/s, v_num=7d93]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:28<00:09,  0.11it/s, v_num=7d93]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:28<00:09,  0.11it/s, v_num=7d93]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:30<00:00,  0.13it/s, v_num=7d93]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:30<00:00,  0.13it/s, v_num=7d93]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.62it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:32<00:00,  0.12it/s, v_num=7d93]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:32<00:00,  0.12it/s, v_num=7d93]Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s, v_num=7d93]        Epoch 1:   0%|          | 0/4 [00:00<?, ?it/s, v_num=7d93]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:05<00:15,  0.20it/s, v_num=7d93]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:05<00:15,  0.20it/s, v_num=7d93]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:09<00:09,  0.21it/s, v_num=7d93]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:09<00:09,  0.21it/s, v_num=7d93]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:14<00:04,  0.20it/s, v_num=7d93]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:14<00:04,  0.20it/s, v_num=7d93]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:16<00:00,  0.24it/s, v_num=7d93]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:16<00:00,  0.24it/s, v_num=7d93]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.54it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:18<00:00,  0.21it/s, v_num=7d93]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:18<00:00,  0.21it/s, v_num=7d93]Epoch 1:   0%|          | 0/4 [00:00<?, ?it/s, v_num=7d93]        Epoch 2:   0%|          | 0/4 [00:00<?, ?it/s, v_num=7d93]Epoch 2:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:05<00:15,  0.20it/s, v_num=7d93]Epoch 2:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:05<00:15,  0.20it/s, v_num=7d93]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:08<00:08,  0.23it/s, v_num=7d93]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:08<00:08,  0.23it/s, v_num=7d93]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:12<00:04,  0.24it/s, v_num=7d93]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:12<00:04,  0.24it/s, v_num=7d93]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:14<00:00,  0.28it/s, v_num=7d93]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:14<00:00,  0.28it/s, v_num=7d93]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.63it/s][A
                                                                      [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:16<00:00,  0.24it/s, v_num=7d93]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:16<00:00,  0.24it/s, v_num=7d93]Epoch 2:   0%|          | 0/4 [00:00<?, ?it/s, v_num=7d93]        Epoch 3:   0%|          | 0/4 [00:00<?, ?it/s, v_num=7d93]Epoch 3:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:05<00:15,  0.19it/s, v_num=7d93]Epoch 3:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:05<00:15,  0.19it/s, v_num=7d93][rank: 0] Received SIGTERM: 15
[rank: 1] Received SIGTERM: 15
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 330, in forward
    output = self.output_layer(batch, encoded.last_hidden_state, is_generation=is_generation)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 132, in forward
    task_loss, accuracy, auroc_score, mse, f1_score, event_label_preds, event_label_labels = self.get_task_outputs(
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/model_output.py", line -1, in get_task_outputs
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1277296) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 69, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1035, in _teardown
    self.strategy.teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 419, in teardown
    super().teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/parallel.py", line 134, in teardown
    super().teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 532, in teardown
    _optimizers_to_device(self.optimizers, torch.device("cpu"))
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/optimizer.py", line 27, in _optimizers_to_device
    _optimizer_to_device(opt, device)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/optimizer.py", line 41, in _optimizer_to_device
    v[key] = move_data_to_device(val, device)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/apply_func.py", line 110, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py", line 66, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/apply_func.py", line 104, in batch_to
    data_output = data.to(device, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1277412) is killed by signal: Terminated. 
None
wandb: Waiting for W&B process to finish... (success).
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 330, in forward
    output = self.output_layer(batch, encoded.last_hidden_state, is_generation=is_generation)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 132, in forward
    task_loss, accuracy, auroc_score, mse, f1_score, event_label_preds, event_label_labels = self.get_task_outputs(
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/model_output.py", line -1, in get_task_outputs
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1277260) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 69, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1035, in _teardown
    self.strategy.teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 419, in teardown
    super().teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/parallel.py", line 134, in teardown
    super().teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 532, in teardown
    _optimizers_to_device(self.optimizers, torch.device("cpu"))
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/optimizer.py", line 27, in _optimizers_to_device
    _optimizer_to_device(opt, device)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/optimizer.py", line 41, in _optimizer_to_device
    v[key] = move_data_to_device(val, device)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/apply_func.py", line 110, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py", line 66, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/apply_func.py", line 104, in batch_to
    data_output = data.to(device, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1277393) is killed by signal: Terminated. 
None
EXCEPTION: DataLoader worker (pid 1277393) is killed by signal: Terminated. 
Error executing job with overrides: ['config.attention_dropout=0.07953876474545629', 'config.input_dropout=0.2392782879767836', 'config.is_cls_dist=True', 'config.is_event_classification=False', 'config.resid_dropout=0.17413580146888963', 'config.save_metrics=False', 'config.save_metrics_fp=/home/filip-marcus/resutls/eneryield/test', 'config.task_specific_params.pooling_method=max', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=25', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.0003065931399677365', 'optimization_config.init_lr=0.001880542204028268', 'optimization_config.lr_decay_power=3.2260586299577403', 'optimization_config.lr_frac_warmup_steps=1.230119625872728e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.0084796979849496', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_class_dist', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    loss = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 488, in training_step
    out_tuple = self.model(batch)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 330, in forward
    output = self.output_layer(batch, encoded.last_hidden_state, is_generation=is_generation)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/conditionally_independent_model.py", line 132, in forward
    task_loss, accuracy, auroc_score, mse, f1_score, event_label_preds, event_label_labels = self.get_task_outputs(
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/model_output.py", line -1, in get_task_outputs
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1277260) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 388, in train
    trainer.fit(model=LM, train_dataloaders=train_dataloader, val_dataloaders=tuning_dataloader)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 69, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1035, in _teardown
    self.strategy.teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 419, in teardown
    super().teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/parallel.py", line 134, in teardown
    super().teardown()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 532, in teardown
    _optimizers_to_device(self.optimizers, torch.device("cpu"))
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/optimizer.py", line 27, in _optimizers_to_device
    _optimizer_to_device(opt, device)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/optimizer.py", line 41, in _optimizer_to_device
    v[key] = move_data_to_device(val, device)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/apply_func.py", line 110, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py", line 66, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/lightning/fabric/utilities/apply_func.py", line 104, in batch_to
    data_output = data.to(device, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1277393) is killed by signal: Terminated. 

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.041 MB uploaded (0.000 MB deduped)[rank: 1] Child process with PID 1274847 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
