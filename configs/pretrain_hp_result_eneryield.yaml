defaults:
  - pretrain_config
  - _self_
 
do_overwrite: True

config:
  cohort_name: eneryield_combined
  do_use_learnable_sinusoidal_ATE: true
  event_types_idxmap:
    disturbance: 1
  
  do_split_embeddings: false
  categorical_embedding_dim: null
  numerical_embedding_dim: null
  static_embedding_mode: drop
  static_embedding_weight: 0.9990428176000288
  dynamic_embedding_weight: 0.5
  categorical_embedding_weight: 0.9461132616555276
  numerical_embedding_weight: 0.5
  do_normalize_by_measurement_index: false
  structured_event_processing_mode: conditionally_independent
  num_hidden_layers: 8
  seq_attention_types: global
  
  dep_graph_attention_types: null
  dep_graph_attention_layers: null
  seq_window_size: 52
  dep_graph_window_size: null
  TTE_generation_layer_type: log_normal_mixture
  TTE_lognormal_generation_num_components: 12
  mean_log_inter_event_time_min: -0.7011383917089885
  std_log_inter_event_time_min: 2.459817443333912
  init_std: 0.02
  max_seq_len: 43
  
  measurements_per_dep_graph_level: null
  vocab_size: 17
  head_dim: 16
  hidden_size: 160
  num_attention_heads: 10
  attention_dropout: 0.4937673071195996
  input_dropout: 0.45719475969563056
  resid_dropout: 0.35101455310044205
  intermediate_size: 128
  layer_norm_epsilon: 1.0e-05
  activation_function: gelu
  do_full_block_in_seq_attention: null
  do_full_block_in_dep_graph_attention: null
  use_cache: true
  return_dict: true
  output_hidden_states: false
  output_attentions: false
  torchscript: false
  torch_dtype: null
  use_bfloat16: false
  tf_legacy_loss: false
  pruned_heads: {}
  tie_word_embeddings: true
  chunk_size_feed_forward: 0
  is_encoder_decoder: false
  is_decoder: false
  cross_attention_hidden_size: null
  add_cross_attention: false
  tie_encoder_decoder: false
  max_length: 20
  min_length: 0
  do_sample: false
  early_stopping: false
  num_beams: 1
  num_beam_groups: 1
  diversity_penalty: 0.0
  temperature: 1.0
  top_k: 50
  top_p: 1.0
  typical_p: 1.0
  repetition_penalty: 1.0
  length_penalty: 1.0
  no_repeat_ngram_size: 0
  encoder_no_repeat_ngram_size: 0

  num_return_sequences: 1
  output_scores: false
  return_dict_in_generate: false

  remove_invalid_values: false


experiment_dir: "/home/filip-marcus/models/ESGPT_new/EventStreamGPT"

#   TTE_generation_layer_type:
#     desc: null
#     value: log_normal_mixture

#   TTE_lognormal_generation_num_components:
#     desc: null
#     value: 11

#   attention_dropout:
#     desc: null
#     value: 0.4937673071195996

#   categorical_embedding_dim:
#     desc: null
#     value: 88

#   categorical_embedding_weight:
#     desc: null
#     value: 0.9461132616555276

#   do_full_block_in_dep_graph_attention:
#     desc: null
#     value: false
#   do_full_block_in_seq_attention:
#     desc: null
#     value: false
#   do_normalize_by_measurement_index:
#     desc: null
#     value: false
#   do_split_embeddings:
#     desc: null
#     value: false
#   do_use_learnable_sinusoidal_ATE:
#     desc: null
#     value: true
#   head_dim:
#     desc: null
#     value: 8
#   input_dropout:
#     desc: null
#     value: 0.45719475969563056
#   intermediate_size:
#     desc: null
#     value: 18
#   num_attention_heads:
#     desc: null
#     value: 8
#   num_hidden_layers:
#     desc: null
#     value: 6
#   numerical_embedding_dim:
#     desc: null
#     value: 66
#   resid_dropout:
#     desc: null
#     value: 0.35101455310044205
#   seq_attention_types:
#     desc: null
#     value: global
#   seq_window_size:
#     desc: null
#     value: 52
#   static_embedding_mode:
#     desc: null
#     value: drop
#   static_embedding_weight:
#     desc: null
#     value: 0.9990428176000288
#   structured_event_processing_mode:
#     desc: null
#     value: conditionally_independent
data_config:
  max_seq_len: 43
  min_seq_len: 20
  save_dir: /home/filip-marcus/models/ESGPT_new/EventStreamGPT/data/processed/eneryield_combined
  train_subset_seed: 1
  train_subset_size: FULL


optimization_config:
  batch_size: 16
  validation_batch_size: 32
  lr_num_warmup_steps: 0
  max_training_steps: 600
  weight_decay: 0.8029083482607132
  patience: null
  gradient_accumulation: null
  end_lr: null
  end_lr_frac_of_init_lr: 0.0011385375796989063
  init_lr: 0.2 #0.002010117041050921
  lr_decay_power: 5 #2.623723207863515
  lr_frac_warmup_steps: 0.00015286611200084077
  max_epochs: 1000
  num_dataloader_workers: 5

#   patience:
#     desc: null
#     value: 10

#   weight_decay:
#     desc: null
#     value: 0.8029083482607132
# pretraining_metrics_config:
#   do_skip_all_metrics:
#     desc: null
#     value: false
# trainer_config:
#   detect_anomaly:
#     desc: null
#     value: false
#   log_every_n_steps:
#     desc: null
#     value: 50
# wandb_logger_kwargs:
#   do_log_graph:
#     desc: null
#     value: false

#   log_model:
#     desc: null
#     value: false

#   name:
#     desc: null
#     value: generative_event_stream_transformer
#   project:
#     desc: null
#     value: Hyperparameter_sweep_eneryield_full

# save_dir: /home/filip-marcus/models/ESGPT_new/EventStreamGPT/pretrain/


