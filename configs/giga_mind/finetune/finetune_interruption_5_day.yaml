defaults:
  - finetune_config
  - _self_

do_overwrite: true
seed: 1

# # Strategy:  (interruption 5 day)
# load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/
# task_df_name: task_df_giga_mind_interruption_5_day
# pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/pretrained_weights
# config:
#   is_cls_dist: false
#   is_event_classification: false
#   save_metrics: true
#   is_interruption_forecast: true
#   save_metrics_fp: /home/filip-marcus/results/giga_mind/interruption_5_day_baseline
#   attention_dropout: 0.4302132542357813
#   input_dropout: 0.4131171234562605
#   resid_dropout: 0.20482647659344977
#   task_specific_params:
#     pooling_method: last



# # Strategy:  (class dist --> event label --> interruption in sequence --> interruption 3 day --> interruption 5 day) 
# load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_interruption_3_day
# task_df_name: task_df_giga_mind_interruption_5_day
# pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_interruption_3_day/finetune_weights
# config:
#   is_cls_dist: false
#   is_event_classification: false
#   save_metrics: true
#   save_metrics_fp: /home/filip-marcus/results/giga_mind/class_dist_event_label_interruption_in_seq_interruption3_interruption_5_day
#   is_pretrain: false
#   is_interruption_forecast: true
#   attention_dropout: 0.4302132542357813
#   input_dropout: 0.4131171234562605
#   resid_dropout: 0.20482647659344977
#   task_specific_params:
#     pooling_method: last


# # Strategy:  (class dist --> interruption in sequence --> event label --> interruption 3 day --> interruption 5 day) 
# load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_interruption_3_day
# task_df_name: task_df_giga_mind_interruption_5_day
# pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_interruption_3_day/finetune_weights
# config:
#   is_cls_dist: false
#   is_event_classification: false
#   save_metrics: true
#   save_metrics_fp: /home/filip-marcus/results/giga_mind/class_dist_interruption_in_seq_event_label_interruption3_interruption5
#   is_pretrain: false
#   is_interruption_forecast: true
#   attention_dropout: 0.4302132542357813
#   input_dropout: 0.4131171234562605
#   resid_dropout: 0.20482647659344977
#   task_specific_params:
#     pooling_method: last


# Strategy:  (event label --> class dist --> interruption in sequence --> interruption 3 day --> interruption 5 day) 
load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_interruption_3_day
task_df_name: task_df_giga_mind_interruption_5_day
pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_interruption_3_day/finetune_weights
config:
  is_cls_dist: false
  is_event_classification: false
  save_metrics: true
  save_metrics_fp: /home/filip-marcus/results/giga_mind/event_label_class_dist_interruption_in_seq_interruption3_interruption5
  is_pretrain: false
  is_interruption_forecast: true
  attention_dropout: 0.4302132542357813
  input_dropout: 0.4131171234562605
  resid_dropout: 0.20482647659344977
  task_specific_params:
    pooling_method: last

# # Strategy:  (event label --> interruption in sequence --> class dist --> interruption 3 day --> interruption 5 day) 
# load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_interruption_3_day
# task_df_name: task_df_giga_mind_interruption_5_day
# pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_interruption_3_day/finetune_weights
# config:
#   is_cls_dist: false
#   is_event_classification: false
#   save_metrics: true
#   save_metrics_fp: /home/filip-marcus/results/giga_mind/event_label_interruption_in_seq_class_dist_interruption3_interruption5
#   is_pretrain: false
#   is_interruption_forecast: true
#   attention_dropout: 0.28165702824699057
#   input_dropout: 0.3422534010637496
#   resid_dropout: 0.299695252905951
#   task_specific_params:
#     pooling_method: mean


# # Strategy:  (interruption in seq --> class dist --> event label --> interruption 3 day --> interruption 5 day) 
# load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_interruption_3_day
# task_df_name: task_df_giga_mind_interruption_5_day
# pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_interruption_3_day/finetune_weights
# config:
#   is_cls_dist: false
#   is_event_classification: false
#   save_metrics: true
#   save_metrics_fp: /home/filip-marcus/results/giga_mind/interruption_in_seq_class_dist_event_label_interruption3_interruption5
#   is_pretrain: false
#   is_interruption_forecast: true
#   attention_dropout: 0.28165702824699057
#   input_dropout: 0.3422534010637496
#   resid_dropout: 0.299695252905951
#   task_specific_params:
#     pooling_method: mean

# # Strategy:  (interruption in seq --> event label --> class dist --> interruption 3 day --> interruption 5 day) 
# load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_interruption_3_day
# task_df_name: task_df_giga_mind_interruption_5_day
# pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/giga_mind/finetuning/task_df_giga_mind_interruption_in_seq/finetuning/task_df_giga_mind_event_label/finetuning/task_df_giga_mind_class_dist/finetuning/task_df_giga_mind_interruption_3_day/finetune_weights
# config:
#   is_cls_dist: false
#   is_event_classification: false
#   save_metrics: true
#   save_metrics_fp: /home/filip-marcus/results/giga_mind/interruption_in_seq_event_label_class_dist_interruption3_interruption5
#   is_pretrain: false
#   is_interruption_forecast: true
#   attention_dropout: 0.28165702824699057
#   input_dropout: 0.3422534010637496
#   resid_dropout: 0.299695252905951
#   task_specific_params:
#     pooling_method: mean


data_config:
  save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/giga_mind
  seq_padding_side: left
  max_seq_len: 256

optimization_config:
  init_lr: 2.212109299517262e-05
  end_lr: 8.140627999939582e-06
  end_lr_frac_of_init_lr: 0.3680029735292046
  max_epochs: 70
  batch_size: 23
  validation_batch_size: 32
  lr_frac_warmup_steps: 7.028983870970934e-05
  lr_num_warmup_steps: 0
  max_training_steps: 2600
  lr_decay_power: 3.4320321099193114
  weight_decay: 0.16099739482584516
  patience: 15
  gradient_accumulation: 1
  gradient_clip_val: 1
  num_dataloader_workers: 5