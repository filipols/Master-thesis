defaults:
  - pretrain_config
  - _self_

experiment_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT
do_overwrite: True
do_final_validation_on_metrics: true



data_config:
  save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/synthetic_mega
  max_seq_len: 360
  min_seq_len: 5
  train_subset_seed: 1
  train_subset_size: FULL

optimization_config:
  num_dataloader_workers: 5
  max_epochs: 100
  batch_size: 128
  end_lr_frac_of_init_lr: 0.049841511155020007
  init_lr: 0.005047916971952921
  end_lr: null
  lr_decay_power: 2.9127738195373194
  lr_frac_warmup_steps: 0.13570177012064244
  patience: 15
  weight_decay: 0.7615238238755272



# denna används för att spara ner output från pretraining till samma mapp, istället för att göra ny mapp varje körning
config:
  hidden_size: 22
  structured_event_processing_mode: conditionally_independent
  max_seq_length: 256
  cohort_name: synthetic_mega
  TTE_generation_layer_type: exponential
  attention_dropout: 0.09101829291333348
  categorical_embedding_dim: 21
  categorical_embedding_weight: 0.9553590131645292
  do_full_block_in_dep_graph_attention: false
  do_full_block_in_seq_attention: false
  do_normalize_by_measurement_index: false
  do_split_embeddings: false
  do_use_learnable_sinusoidal_ATE: true
  head_dim: 11
  input_dropout: 0.3298678884662117
  intermediate_size: 9
  num_attention_heads: 2
  num_hidden_layers: 3
  numerical_embedding_dim: 109
  resid_dropout: 0.3639805391510391
  seq_attention_types: global
  seq_window_size: 2
  static_embedding_mode: drop
  static_embedding_weight: 0.47018931018313426
  