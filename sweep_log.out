nohup: ignoring input
Running sweep with config: FT_hp_sweep_event_label
2025-04-09 16:39:35,460 - wandb.wandb_agent - INFO - Running runs: []
Create sweep with ID: o9qlrb7q
Sweep URL: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
[2025-04-09 16:39:35,460][wandb.wandb_agent][INFO] - Running runs: []
2025-04-09 16:39:35,788 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 16:39:35,788][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 16:39:35,789 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3599957198849081
	config.input_dropout: 0.04856905687804608
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.4338678452902355
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 14
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.29836729791916106
	optimization_config.init_lr: 0.0015768754809196662
	optimization_config.lr_decay_power: 2.9018862558271774
	optimization_config.lr_frac_warmup_steps: 4.1904600521101823e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00948743051945399
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 16:39:35,789][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3599957198849081
	config.input_dropout: 0.04856905687804608
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.4338678452902355
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 14
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.29836729791916106
	optimization_config.init_lr: 0.0015768754809196662
	optimization_config.lr_decay_power: 2.9018862558271774
	optimization_config.lr_frac_warmup_steps: 4.1904600521101823e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00948743051945399
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 16:39:35,799 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3599957198849081 config.input_dropout=0.04856905687804608 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.4338678452902355 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=14 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.29836729791916106 optimization_config.init_lr=0.0015768754809196662 optimization_config.lr_decay_power=2.9018862558271774 optimization_config.lr_frac_warmup_steps=4.1904600521101823e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00948743051945399 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 16:39:35,799][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3599957198849081 config.input_dropout=0.04856905687804608 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.4338678452902355 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=14 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.29836729791916106 optimization_config.init_lr=0.0015768754809196662 optimization_config.lr_decay_power=2.9018862558271774 optimization_config.lr_frac_warmup_steps=4.1904600521101823e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00948743051945399 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 16:39:40,809 - wandb.wandb_agent - INFO - Running runs: ['d62a6uha']
[2025-04-09 16:39:40,809][wandb.wandb_agent][INFO] - Running runs: ['d62a6uha']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_163942-d62a6uha
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/d62a6uha
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3599957198849081
Overwriting input_dropout in config from 0.25407516893041604 to 0.04856905687804608
Overwriting resid_dropout in config from 0.4697133384759005 to 0.4338678452902355
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3599957198849081
Overwriting input_dropout in config from 0.25407516893041604 to 0.04856905687804608
Overwriting resid_dropout in config from 0.4697133384759005 to 0.4338678452902355
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  0.78it/s][rank0]:[E409 17:09:53.116821958 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800019 milliseconds before timing out.
[rank0]:[E409 17:09:53.117052715 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 17:09:53.117072513 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 17:09:53.117089565 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 17:09:53.117096322 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 17:09:53.118601046 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800019 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79bcaa26c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x79bc577fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x79bc578007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79bc578016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x79bcc84ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x79bccce94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x79bcccf26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800019 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79bcaa26c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x79bc577fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x79bc578007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79bc578016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x79bcc84ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x79bccce94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x79bcccf26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79bcaa26c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x79bc5745c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x79bcc84ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x79bccce94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x79bcccf26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E409 17:09:53.197091638 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800097 milliseconds before timing out.
[rank1]:[E409 17:09:53.197415167 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 17:09:53.197442816 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 17:09:53.197456928 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 17:09:53.197470222 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 17:09:53.200464628 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800097 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x758e9096c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x758e3ddfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x758e3de007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x758e3de016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x758eaed2d5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x758eb3694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x758eb3726850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800097 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x758e9096c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x758e3ddfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x758e3de007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x758e3de016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x758eaed2d5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x758eb3694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x758eb3726850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x758e9096c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x758e3da5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x758eaed2d5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x758eb3694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x758eb3726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 17:09:54,650 - wandb.wandb_agent - INFO - Cleaning up finished run: d62a6uha
[2025-04-09 17:09:54,650][wandb.wandb_agent][INFO] - Cleaning up finished run: d62a6uha
wandb: Waiting for W&B process to finish... (failed -6). Press Control-C to abort syncing.
wandb: - 0.000 MB of 0.026 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.026 MB uploaded (0.000 MB deduped)wandb: | 0.026 MB of 0.026 MB uploaded (0.000 MB deduped)wandb: üöÄ View run generative_event_stream_transformer at: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/d62a6uha
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_163942-d62a6uha/logs
2025-04-09 17:09:59,919 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 17:09:59,919][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 17:09:59,920 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3777534775465129
	config.input_dropout: 0.07942013522160385
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.18891516238023728
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 48
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0014373599150293587
	optimization_config.init_lr: 7.437546802291538e-07
	optimization_config.lr_decay_power: 4.985067498938046
	optimization_config.lr_frac_warmup_steps: 0.0001050630854656223
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.006865281242814038
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 17:09:59,920][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3777534775465129
	config.input_dropout: 0.07942013522160385
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.18891516238023728
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 48
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0014373599150293587
	optimization_config.init_lr: 7.437546802291538e-07
	optimization_config.lr_decay_power: 4.985067498938046
	optimization_config.lr_frac_warmup_steps: 0.0001050630854656223
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.006865281242814038
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 17:09:59,930 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3777534775465129 config.input_dropout=0.07942013522160385 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.18891516238023728 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=48 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0014373599150293587 optimization_config.init_lr=7.437546802291538e-07 optimization_config.lr_decay_power=4.985067498938046 optimization_config.lr_frac_warmup_steps=0.0001050630854656223 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.006865281242814038 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 17:09:59,930][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3777534775465129 config.input_dropout=0.07942013522160385 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.18891516238023728 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=48 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0014373599150293587 optimization_config.init_lr=7.437546802291538e-07 optimization_config.lr_decay_power=4.985067498938046 optimization_config.lr_frac_warmup_steps=0.0001050630854656223 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.006865281242814038 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 17:10:04,943 - wandb.wandb_agent - INFO - Running runs: ['bttac8qo']
[2025-04-09 17:10:04,943][wandb.wandb_agent][INFO] - Running runs: ['bttac8qo']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_171007-bttac8qo
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/bttac8qo
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3777534775465129
Overwriting input_dropout in config from 0.25407516893041604 to 0.07942013522160385
Overwriting resid_dropout in config from 0.4697133384759005 to 0.18891516238023728
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3777534775465129
Overwriting input_dropout in config from 0.25407516893041604 to 0.07942013522160385
Overwriting resid_dropout in config from 0.4697133384759005 to 0.18891516238023728
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.59it/s][rank1]:[E409 17:40:15.676113469 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800091 milliseconds before timing out.
[rank1]:[E409 17:40:15.676464691 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 17:40:15.676489622 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 17:40:15.676502549 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 17:40:15.676514903 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 17:40:15.677061958 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800092 milliseconds before timing out.
[rank0]:[E409 17:40:15.677360711 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 17:40:15.677384287 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 17:40:15.677396261 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 17:40:15.677408274 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 17:40:15.679488460 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800091 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79833036c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7982dd7fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7982dd8007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7982dd8016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x79834decd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x798352e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x798352f26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank0]:[E409 17:40:15.680457268 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800092 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fee9626c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fee437fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fee438007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fee438016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7feeb44ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7feeb8e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7feeb8f26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800091 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79833036c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7982dd7fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7982dd8007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7982dd8016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x79834decd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x798352e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x798352f26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79833036c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7982dd45c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x79834decd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x798352e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x798352f26850 in /lib/x86_64-linux-gnu/libc.so.6)

  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800092 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fee9626c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fee437fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fee438007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fee438016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7feeb44ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7feeb8e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7feeb8f26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fee9626c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7fee4345c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7feeb44ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7feeb8e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7feeb8f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 17:40:21,725 - wandb.wandb_agent - INFO - Cleaning up finished run: bttac8qo
[2025-04-09 17:40:21,725][wandb.wandb_agent][INFO] - Cleaning up finished run: bttac8qo
2025-04-09 17:40:22,155 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 17:40:22,155][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 17:40:22,156 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.18209099804663423
	config.input_dropout: 0.020642250563500383
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.018020066242478483
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 64
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0004639066142441561
	optimization_config.init_lr: 9.777010086169966e-05
	optimization_config.lr_decay_power: 2.704778404772055
	optimization_config.lr_frac_warmup_steps: 0.001013131650577491
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009650727471682764
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 17:40:22,156][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.18209099804663423
	config.input_dropout: 0.020642250563500383
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.018020066242478483
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 64
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0004639066142441561
	optimization_config.init_lr: 9.777010086169966e-05
	optimization_config.lr_decay_power: 2.704778404772055
	optimization_config.lr_frac_warmup_steps: 0.001013131650577491
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009650727471682764
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 17:40:22,166 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.18209099804663423 config.input_dropout=0.020642250563500383 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.018020066242478483 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=64 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0004639066142441561 optimization_config.init_lr=9.777010086169966e-05 optimization_config.lr_decay_power=2.704778404772055 optimization_config.lr_frac_warmup_steps=0.001013131650577491 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009650727471682764 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 17:40:22,166][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.18209099804663423 config.input_dropout=0.020642250563500383 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.018020066242478483 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=64 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0004639066142441561 optimization_config.init_lr=9.777010086169966e-05 optimization_config.lr_decay_power=2.704778404772055 optimization_config.lr_frac_warmup_steps=0.001013131650577491 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009650727471682764 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 17:40:27,179 - wandb.wandb_agent - INFO - Running runs: ['iypqv8kv']
[2025-04-09 17:40:27,179][wandb.wandb_agent][INFO] - Running runs: ['iypqv8kv']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_174029-iypqv8kv
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/iypqv8kv
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.18209099804663423
Overwriting input_dropout in config from 0.25407516893041604 to 0.020642250563500383
Overwriting resid_dropout in config from 0.4697133384759005 to 0.018020066242478483
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.18209099804663423
Overwriting input_dropout in config from 0.25407516893041604 to 0.020642250563500383
Overwriting resid_dropout in config from 0.4697133384759005 to 0.018020066242478483
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.64it/s][rank0]:[E409 18:10:38.233949832 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800038 milliseconds before timing out.
[rank0]:[E409 18:10:38.234337511 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 18:10:38.234366317 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 18:10:38.234379284 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 18:10:38.234392015 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 18:10:38.237395005 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800038 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71f785f6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x71f7333fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x71f7334007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71f7334016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x71f7a3a5c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x71f7a8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x71f7a8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800038 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71f785f6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x71f7333fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x71f7334007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71f7334016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x71f7a3a5c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x71f7a8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x71f7a8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71f785f6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x71f73305c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x71f7a3a5c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x71f7a8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x71f7a8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E409 18:10:38.250829004 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800054 milliseconds before timing out.
[rank1]:[E409 18:10:38.250979227 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 18:10:38.250987977 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 18:10:38.250998377 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 18:10:38.251008914 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 18:10:38.253737499 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800054 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73b88276c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x73b82fbfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x73b82fc007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73b82fc016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x73b8a02cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x73b8a5294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x73b8a5326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800054 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73b88276c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x73b82fbfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x73b82fc007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73b82fc016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x73b8a02cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x73b8a5294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x73b8a5326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73b88276c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x73b82f85c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x73b8a02cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x73b8a5294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x73b8a5326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 18:10:43,308 - wandb.wandb_agent - INFO - Cleaning up finished run: iypqv8kv
[2025-04-09 18:10:43,308][wandb.wandb_agent][INFO] - Cleaning up finished run: iypqv8kv
2025-04-09 18:10:43,861 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 18:10:43,861][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 18:10:43,862 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.2882662233066449
	config.input_dropout: 0.06466822830399466
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3505071986150453
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 62
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.006347868042229152
	optimization_config.init_lr: 0.0010718099755882271
	optimization_config.lr_decay_power: 0.6038845853060564
	optimization_config.lr_frac_warmup_steps: 0.02802350277243462
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00380281231656682
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 18:10:43,862][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.2882662233066449
	config.input_dropout: 0.06466822830399466
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3505071986150453
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 62
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.006347868042229152
	optimization_config.init_lr: 0.0010718099755882271
	optimization_config.lr_decay_power: 0.6038845853060564
	optimization_config.lr_frac_warmup_steps: 0.02802350277243462
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00380281231656682
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 18:10:43,870 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2882662233066449 config.input_dropout=0.06466822830399466 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3505071986150453 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=62 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.006347868042229152 optimization_config.init_lr=0.0010718099755882271 optimization_config.lr_decay_power=0.6038845853060564 optimization_config.lr_frac_warmup_steps=0.02802350277243462 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00380281231656682 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 18:10:43,870][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2882662233066449 config.input_dropout=0.06466822830399466 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3505071986150453 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=62 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.006347868042229152 optimization_config.init_lr=0.0010718099755882271 optimization_config.lr_decay_power=0.6038845853060564 optimization_config.lr_frac_warmup_steps=0.02802350277243462 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00380281231656682 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-09 18:10:48,882 - wandb.wandb_agent - INFO - Running runs: ['c55ple5h']
[2025-04-09 18:10:48,882][wandb.wandb_agent][INFO] - Running runs: ['c55ple5h']
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_181051-c55ple5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/c55ple5h
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.2882662233066449
Overwriting input_dropout in config from 0.25407516893041604 to 0.06466822830399466
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3505071986150453
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.2882662233066449
Overwriting input_dropout in config from 0.25407516893041604 to 0.06466822830399466
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3505071986150453
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.48it/s][rank1]:[E409 18:40:59.802854647 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800008 milliseconds before timing out.
[rank0]:[E409 18:40:59.803055391 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800009 milliseconds before timing out.
[rank1]:[E409 18:40:59.803220341 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 18:40:59.803242123 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 18:40:59.803253646 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 18:40:59.803263162 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 18:40:59.803497207 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 18:40:59.803529215 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 18:40:59.803547866 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 18:40:59.803565249 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 18:40:59.806339640 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800008 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f839d86c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f834adfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f834ae007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f834ae016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f83bb4785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f83c0494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f83c0526850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800008 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f839d86c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f834adfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f834ae007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f834ae016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f83bb4785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f83c0494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f83c0526850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f839d86c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f834aa5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f83bb4785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f83c0494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7f83c0526850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E409 18:40:59.807859642 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800009 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x790cf586c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x790ca2dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x790ca2e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x790ca2e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x790d13aea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x790d18494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x790d18526850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800009 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x790cf586c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x790ca2dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x790ca2e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x790ca2e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x790d13aea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x790d18494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x790d18526850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x790cf586c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x790ca2a5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x790d13aea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x790d18494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x790d18526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 18:41:04,600 - wandb.wandb_agent - INFO - Cleaning up finished run: c55ple5h
[2025-04-09 18:41:04,600][wandb.wandb_agent][INFO] - Cleaning up finished run: c55ple5h
2025-04-09 18:41:05,117 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 18:41:05,117][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 18:41:05,118 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3447553353328916
	config.input_dropout: 0.3114161648310419
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.33044543245125785
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 36
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.000133386211816606
	optimization_config.init_lr: 0.00027407452761374964
	optimization_config.lr_decay_power: 0.5734987787815391
	optimization_config.lr_frac_warmup_steps: 0.004652138806585106
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00915177511183601
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 18:41:05,118][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3447553353328916
	config.input_dropout: 0.3114161648310419
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.33044543245125785
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 36
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.000133386211816606
	optimization_config.init_lr: 0.00027407452761374964
	optimization_config.lr_decay_power: 0.5734987787815391
	optimization_config.lr_frac_warmup_steps: 0.004652138806585106
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00915177511183601
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 18:41:05,128 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3447553353328916 config.input_dropout=0.3114161648310419 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.33044543245125785 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=36 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.000133386211816606 optimization_config.init_lr=0.00027407452761374964 optimization_config.lr_decay_power=0.5734987787815391 optimization_config.lr_frac_warmup_steps=0.004652138806585106 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00915177511183601 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 18:41:05,128][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3447553353328916 config.input_dropout=0.3114161648310419 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.33044543245125785 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=36 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.000133386211816606 optimization_config.init_lr=0.00027407452761374964 optimization_config.lr_decay_power=0.5734987787815391 optimization_config.lr_frac_warmup_steps=0.004652138806585106 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00915177511183601 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-09 18:41:10,142 - wandb.wandb_agent - INFO - Running runs: ['ucnl3atd']
[2025-04-09 18:41:10,142][wandb.wandb_agent][INFO] - Running runs: ['ucnl3atd']
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_184112-ucnl3atd
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/ucnl3atd
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3447553353328916
Overwriting input_dropout in config from 0.25407516893041604 to 0.3114161648310419
Overwriting resid_dropout in config from 0.4697133384759005 to 0.33044543245125785
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3447553353328916
Overwriting input_dropout in config from 0.25407516893041604 to 0.3114161648310419
Overwriting resid_dropout in config from 0.4697133384759005 to 0.33044543245125785
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.54it/s][rank1]:[E409 19:11:21.207701623 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800010 milliseconds before timing out.
[rank1]:[E409 19:11:21.207940418 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 19:11:21.207954996 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 19:11:21.207962844 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 19:11:21.207969905 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 19:11:21.209384636 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800010 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75ffcdf6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x75ff7b3fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x75ff7b4007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75ff7b4016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x75ffebacd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x75fff0a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x75fff0b26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800010 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75ffcdf6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x75ff7b3fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x75ff7b4007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75ff7b4016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x75ffebacd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x75fff0a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x75fff0b26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75ffcdf6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x75ff7b05c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x75ffebacd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x75fff0a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x75fff0b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E409 19:11:21.227302455 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800030 milliseconds before timing out.
[rank0]:[E409 19:11:21.227592883 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 19:11:21.227614201 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 19:11:21.227624400 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 19:11:21.227635056 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 19:11:21.230595634 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800030 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73e3dd56c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x73e38a9fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x73e38aa007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73e38aa016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x73e3facee5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x73e400094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x73e400126850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800030 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73e3dd56c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x73e38a9fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x73e38aa007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73e38aa016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x73e3facee5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x73e400094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x73e400126850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73e3dd56c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x73e38a65c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x73e3facee5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x73e400094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x73e400126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 19:11:24,608 - wandb.wandb_agent - INFO - Cleaning up finished run: ucnl3atd
[2025-04-09 19:11:24,608][wandb.wandb_agent][INFO] - Cleaning up finished run: ucnl3atd
2025-04-09 19:11:25,133 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 19:11:25,133][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 19:11:25,140 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.11991697005049506
	config.input_dropout: 0.09808115871073814
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2675186371641098
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 64
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.23935197564944327
	optimization_config.init_lr: 3.7811001713848206e-06
	optimization_config.lr_decay_power: 3.4464889699613814
	optimization_config.lr_frac_warmup_steps: 0.04136617749608229
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007869040686545408
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 19:11:25,140][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.11991697005049506
	config.input_dropout: 0.09808115871073814
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2675186371641098
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 64
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.23935197564944327
	optimization_config.init_lr: 3.7811001713848206e-06
	optimization_config.lr_decay_power: 3.4464889699613814
	optimization_config.lr_frac_warmup_steps: 0.04136617749608229
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007869040686545408
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 19:11:25,150 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.11991697005049506 config.input_dropout=0.09808115871073814 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2675186371641098 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=64 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.23935197564944327 optimization_config.init_lr=3.7811001713848206e-06 optimization_config.lr_decay_power=3.4464889699613814 optimization_config.lr_frac_warmup_steps=0.04136617749608229 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007869040686545408 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 19:11:25,150][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.11991697005049506 config.input_dropout=0.09808115871073814 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2675186371641098 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=64 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.23935197564944327 optimization_config.init_lr=3.7811001713848206e-06 optimization_config.lr_decay_power=3.4464889699613814 optimization_config.lr_frac_warmup_steps=0.04136617749608229 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007869040686545408 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 19:11:30,162 - wandb.wandb_agent - INFO - Running runs: ['ie806lmw']
[2025-04-09 19:11:30,162][wandb.wandb_agent][INFO] - Running runs: ['ie806lmw']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_191132-ie806lmw
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/ie806lmw
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.11991697005049506
Overwriting input_dropout in config from 0.25407516893041604 to 0.09808115871073814
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2675186371641098
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.11991697005049506
Overwriting input_dropout in config from 0.25407516893041604 to 0.09808115871073814
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2675186371641098
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.61it/s][rank1]:[E409 19:41:41.168969368 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800004 milliseconds before timing out.
[rank1]:[E409 19:41:41.169335371 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 19:41:41.169361639 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 19:41:41.169374673 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 19:41:41.169387224 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 19:41:41.172356147 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800004 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x788887e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7888353fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7888354007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7888354016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7888a61065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7888aaa94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7888aab26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800004 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x788887e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7888353fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7888354007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7888354016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7888a61065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7888aaa94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7888aab26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x788887e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x78883505c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7888a61065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7888aaa94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7888aab26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E409 19:41:41.203522377 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800039 milliseconds before timing out.
[rank0]:[E409 19:41:41.203693363 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 19:41:41.203702308 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 19:41:41.203707190 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 19:41:41.203710724 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 19:41:41.204740678 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800039 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71a38a16c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x71a3375fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x71a3376007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71a3376016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x71a3a84e65c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x71a3ace94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x71a3acf26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800039 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71a38a16c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x71a3375fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x71a3376007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71a3376016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x71a3a84e65c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x71a3ace94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x71a3acf26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71a38a16c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x71a33725c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x71a3a84e65c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x71a3ace94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x71a3acf26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 19:41:45,797 - wandb.wandb_agent - INFO - Cleaning up finished run: ie806lmw
[2025-04-09 19:41:45,797][wandb.wandb_agent][INFO] - Cleaning up finished run: ie806lmw
2025-04-09 19:41:46,330 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 19:41:46,330][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 19:41:46,331 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.49563234667666056
	config.input_dropout: 0.12615147752650657
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.23676454243267508
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 8
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.1338283017043478
	optimization_config.init_lr: 0.00047683982338081465
	optimization_config.lr_decay_power: 2.5955185641170515
	optimization_config.lr_frac_warmup_steps: 0.07789976062646649
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.006780648872846168
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 19:41:46,331][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.49563234667666056
	config.input_dropout: 0.12615147752650657
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.23676454243267508
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 8
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.1338283017043478
	optimization_config.init_lr: 0.00047683982338081465
	optimization_config.lr_decay_power: 2.5955185641170515
	optimization_config.lr_frac_warmup_steps: 0.07789976062646649
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.006780648872846168
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 19:41:46,341 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.49563234667666056 config.input_dropout=0.12615147752650657 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.23676454243267508 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=8 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.1338283017043478 optimization_config.init_lr=0.00047683982338081465 optimization_config.lr_decay_power=2.5955185641170515 optimization_config.lr_frac_warmup_steps=0.07789976062646649 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.006780648872846168 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 19:41:46,341][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.49563234667666056 config.input_dropout=0.12615147752650657 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.23676454243267508 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=8 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.1338283017043478 optimization_config.init_lr=0.00047683982338081465 optimization_config.lr_decay_power=2.5955185641170515 optimization_config.lr_frac_warmup_steps=0.07789976062646649 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.006780648872846168 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 19:41:51,354 - wandb.wandb_agent - INFO - Running runs: ['1fq4cgbn']
[2025-04-09 19:41:51,354][wandb.wandb_agent][INFO] - Running runs: ['1fq4cgbn']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_194153-1fq4cgbn
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/1fq4cgbn
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.49563234667666056
Overwriting input_dropout in config from 0.25407516893041604 to 0.12615147752650657
Overwriting resid_dropout in config from 0.4697133384759005 to 0.23676454243267508
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.49563234667666056
Overwriting input_dropout in config from 0.25407516893041604 to 0.12615147752650657
Overwriting resid_dropout in config from 0.4697133384759005 to 0.23676454243267508
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.61it/s][rank0]:[E409 20:12:02.178955760 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800097 milliseconds before timing out.
[rank0]:[E409 20:12:02.179194591 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 20:12:02.179210984 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 20:12:02.179219151 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 20:12:02.179237593 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 20:12:02.180551595 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800098 milliseconds before timing out.
[rank0]:[E409 20:12:02.180656428 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800097 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ee4bad6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7ee4681fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7ee4682007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ee4682016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ee4d88a55c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ee4dd894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ee4dd926850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank1]:[E409 20:12:02.180739457 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 20:12:02.180750495 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 20:12:02.180756546 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 20:12:02.180761727 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800097 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ee4bad6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7ee4681fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7ee4682007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ee4682016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ee4d88a55c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ee4dd894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ee4dd926850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ee4bad6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7ee467e5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7ee4d88a55c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7ee4dd894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7ee4dd926850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E409 20:12:02.181959754 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800098 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f603006c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f5fdd5fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f5fdd6007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f5fdd6016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f604e4b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f6052e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f6052f26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800098 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f603006c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f5fdd5fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f5fdd6007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f5fdd6016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f604e4b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f6052e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f6052f26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f603006c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f5fdd25c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f604e4b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f6052e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7f6052f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 20:12:05,436 - wandb.wandb_agent - INFO - Cleaning up finished run: 1fq4cgbn
[2025-04-09 20:12:05,436][wandb.wandb_agent][INFO] - Cleaning up finished run: 1fq4cgbn
2025-04-09 20:12:05,941 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 20:12:05,941][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 20:12:05,941 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.0808756206369331
	config.input_dropout: 0.3430821037902425
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2491382805779956
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 8
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.20028546608235784
	optimization_config.init_lr: 0.00015784736444960217
	optimization_config.lr_decay_power: 0.7991711673831321
	optimization_config.lr_frac_warmup_steps: 0.0003446097130327178
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007701596583510125
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 20:12:05,941][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.0808756206369331
	config.input_dropout: 0.3430821037902425
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2491382805779956
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 8
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.20028546608235784
	optimization_config.init_lr: 0.00015784736444960217
	optimization_config.lr_decay_power: 0.7991711673831321
	optimization_config.lr_frac_warmup_steps: 0.0003446097130327178
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007701596583510125
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 20:12:05,951 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.0808756206369331 config.input_dropout=0.3430821037902425 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2491382805779956 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=8 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.20028546608235784 optimization_config.init_lr=0.00015784736444960217 optimization_config.lr_decay_power=0.7991711673831321 optimization_config.lr_frac_warmup_steps=0.0003446097130327178 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007701596583510125 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 20:12:05,951][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.0808756206369331 config.input_dropout=0.3430821037902425 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2491382805779956 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=8 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.20028546608235784 optimization_config.init_lr=0.00015784736444960217 optimization_config.lr_decay_power=0.7991711673831321 optimization_config.lr_frac_warmup_steps=0.0003446097130327178 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007701596583510125 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 20:12:10,965 - wandb.wandb_agent - INFO - Running runs: ['f9m2gscb']
[2025-04-09 20:12:10,965][wandb.wandb_agent][INFO] - Running runs: ['f9m2gscb']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_201213-f9m2gscb
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/f9m2gscb
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.0808756206369331
Overwriting input_dropout in config from 0.25407516893041604 to 0.3430821037902425
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2491382805779956
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.0808756206369331
Overwriting input_dropout in config from 0.25407516893041604 to 0.3430821037902425
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2491382805779956
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.62it/s][rank1]:[E409 20:42:22.836551889 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800019 milliseconds before timing out.
[rank1]:[E409 20:42:22.836921779 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 20:42:22.836948211 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 20:42:22.836961022 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 20:42:22.836973519 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 20:42:22.839986853 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800019 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70f4d486c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x70f481dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x70f481e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70f481e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x70f4f2ab75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x70f4f7494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x70f4f7526850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank0]:[E409 20:42:22.840279806 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
[rank0]:[E409 20:42:22.840468437 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 20:42:22.840482355 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 20:42:22.840490001 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 20:42:22.840505741 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800019 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70f4d486c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x70f481dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x70f481e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70f481e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x70f4f2ab75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x70f4f7494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x70f4f7526850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70f4d486c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x70f481a5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x70f4f2ab75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x70f4f7494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x70f4f7526850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E409 20:42:22.841933507 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78b2e796c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x78b294dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x78b294e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78b294e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x78b3054cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x78b30a494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x78b30a526850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78b2e796c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x78b294dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x78b294e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78b294e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x78b3054cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x78b30a494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x78b30a526850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78b2e796c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x78b294a5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x78b3054cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x78b30a494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x78b30a526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 20:42:25,802 - wandb.wandb_agent - INFO - Cleaning up finished run: f9m2gscb
[2025-04-09 20:42:25,802][wandb.wandb_agent][INFO] - Cleaning up finished run: f9m2gscb
2025-04-09 20:42:26,254 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 20:42:26,254][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 20:42:26,254 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3731533383507928
	config.input_dropout: 0.3587169209535478
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.08571142526780745
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 64
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.019285384288215895
	optimization_config.init_lr: 6.119563271034763e-06
	optimization_config.lr_decay_power: 1.5029639045507082
	optimization_config.lr_frac_warmup_steps: 1.0217383139359764e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007681320190256824
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 20:42:26,254][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3731533383507928
	config.input_dropout: 0.3587169209535478
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.08571142526780745
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 64
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.019285384288215895
	optimization_config.init_lr: 6.119563271034763e-06
	optimization_config.lr_decay_power: 1.5029639045507082
	optimization_config.lr_frac_warmup_steps: 1.0217383139359764e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007681320190256824
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 20:42:26,264 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3731533383507928 config.input_dropout=0.3587169209535478 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.08571142526780745 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=64 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.019285384288215895 optimization_config.init_lr=6.119563271034763e-06 optimization_config.lr_decay_power=1.5029639045507082 optimization_config.lr_frac_warmup_steps=1.0217383139359764e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007681320190256824 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 20:42:26,264][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3731533383507928 config.input_dropout=0.3587169209535478 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.08571142526780745 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=64 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.019285384288215895 optimization_config.init_lr=6.119563271034763e-06 optimization_config.lr_decay_power=1.5029639045507082 optimization_config.lr_frac_warmup_steps=1.0217383139359764e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007681320190256824 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-09 20:42:31,277 - wandb.wandb_agent - INFO - Running runs: ['kfcxijgt']
Seed set to 1
[2025-04-09 20:42:31,277][wandb.wandb_agent][INFO] - Running runs: ['kfcxijgt']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_204233-kfcxijgt
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/kfcxijgt
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3731533383507928
Overwriting input_dropout in config from 0.25407516893041604 to 0.3587169209535478
Overwriting resid_dropout in config from 0.4697133384759005 to 0.08571142526780745
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3731533383507928
Overwriting input_dropout in config from 0.25407516893041604 to 0.3587169209535478
Overwriting resid_dropout in config from 0.4697133384759005 to 0.08571142526780745
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.64it/s][rank0]:[E409 21:12:42.141783699 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800060 milliseconds before timing out.
[rank0]:[E409 21:12:42.142150433 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 21:12:42.142179343 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 21:12:42.142192870 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 21:12:42.142205321 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 21:12:42.146379945 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800060 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c08fcf6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7c08aa3fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7c08aa4007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c08aa4016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7c091b3015c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7c091fc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7c091fd26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800060 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c08fcf6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7c08aa3fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7c08aa4007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c08aa4016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7c091b3015c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7c091fc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7c091fd26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c08fcf6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7c08aa05c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7c091b3015c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7c091fc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7c091fd26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E409 21:12:42.156626403 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800075 milliseconds before timing out.
[rank1]:[E409 21:12:42.156970111 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 21:12:42.156994448 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 21:12:42.157039874 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 21:12:42.157057087 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 21:12:42.160101185 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800075 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e40cdd6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7e407b1fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7e407b2007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e407b2016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7e40ec1385c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7e40f0a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7e40f0b26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800075 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e40cdd6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7e407b1fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7e407b2007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e407b2016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7e40ec1385c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7e40f0a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7e40f0b26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e40cdd6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7e407ae5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7e40ec1385c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7e40f0a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7e40f0b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 21:12:44,841 - wandb.wandb_agent - INFO - Cleaning up finished run: kfcxijgt
[2025-04-09 21:12:44,841][wandb.wandb_agent][INFO] - Cleaning up finished run: kfcxijgt
2025-04-09 21:12:45,302 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 21:12:45,302][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 21:12:45,303 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.011309377349201344
	config.input_dropout: 0.09221992037171985
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.25706807740270293
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 16
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0065311256836178035
	optimization_config.init_lr: 1.8164243696801006e-07
	optimization_config.lr_decay_power: 2.313131645252489
	optimization_config.lr_frac_warmup_steps: 0.02615908119893604
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0061744186672815305
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 21:12:45,303][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.011309377349201344
	config.input_dropout: 0.09221992037171985
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.25706807740270293
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 16
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0065311256836178035
	optimization_config.init_lr: 1.8164243696801006e-07
	optimization_config.lr_decay_power: 2.313131645252489
	optimization_config.lr_frac_warmup_steps: 0.02615908119893604
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0061744186672815305
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 21:12:45,313 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.011309377349201344 config.input_dropout=0.09221992037171985 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.25706807740270293 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=16 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0065311256836178035 optimization_config.init_lr=1.8164243696801006e-07 optimization_config.lr_decay_power=2.313131645252489 optimization_config.lr_frac_warmup_steps=0.02615908119893604 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0061744186672815305 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 21:12:45,313][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.011309377349201344 config.input_dropout=0.09221992037171985 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.25706807740270293 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=16 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0065311256836178035 optimization_config.init_lr=1.8164243696801006e-07 optimization_config.lr_decay_power=2.313131645252489 optimization_config.lr_frac_warmup_steps=0.02615908119893604 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0061744186672815305 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-09 21:12:50,326 - wandb.wandb_agent - INFO - Running runs: ['tma3c9vz']
[2025-04-09 21:12:50,326][wandb.wandb_agent][INFO] - Running runs: ['tma3c9vz']
Seed set to 1
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_211252-tma3c9vz
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/tma3c9vz
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.011309377349201344
Overwriting input_dropout in config from 0.25407516893041604 to 0.09221992037171985
Overwriting resid_dropout in config from 0.4697133384759005 to 0.25706807740270293
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.011309377349201344
Overwriting input_dropout in config from 0.25407516893041604 to 0.09221992037171985
Overwriting resid_dropout in config from 0.4697133384759005 to 0.25706807740270293
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.64it/s][rank1]:[E409 21:43:01.202467283 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800050 milliseconds before timing out.
[rank1]:[E409 21:43:01.202806948 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 21:43:01.202832776 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 21:43:01.202845697 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 21:43:01.202858107 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 21:43:01.203321563 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800051 milliseconds before timing out.
[rank0]:[E409 21:43:01.203649385 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 21:43:01.203671140 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 21:43:01.203681333 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 21:43:01.203691672 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 21:43:01.205850592 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800050 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x702bd5e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x702b833fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x702b834007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x702b834016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x702bf40ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x702bf8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x702bf8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank0]:[E409 21:43:01.206697241 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800051 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7867b936c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7867667fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7867668007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7867668016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7867d6eb15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7867dbe94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7867dbf26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800050 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x702bd5e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x702b833fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x702b834007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x702b834016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x702bf40ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x702bf8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x702bf8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x702bd5e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x702b8305c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x702bf40ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x702bf8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x702bf8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800051 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7867b936c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7867667fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7867668007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7867668016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7867d6eb15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7867dbe94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7867dbf26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7867b936c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x78676645c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7867d6eb15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7867dbe94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7867dbf26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 21:43:04,987 - wandb.wandb_agent - INFO - Cleaning up finished run: tma3c9vz
[2025-04-09 21:43:04,987][wandb.wandb_agent][INFO] - Cleaning up finished run: tma3c9vz
2025-04-09 21:43:05,781 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 21:43:05,781][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 21:43:05,781 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4550799621859056
	config.input_dropout: 0.11574081729234242
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.03757177919340943
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 43
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00919476790879204
	optimization_config.init_lr: 0.12788205638046016
	optimization_config.lr_decay_power: 3.699617366522253
	optimization_config.lr_frac_warmup_steps: 0.009101458333858165
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005428101192399366
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 21:43:05,781][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4550799621859056
	config.input_dropout: 0.11574081729234242
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.03757177919340943
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 43
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00919476790879204
	optimization_config.init_lr: 0.12788205638046016
	optimization_config.lr_decay_power: 3.699617366522253
	optimization_config.lr_frac_warmup_steps: 0.009101458333858165
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005428101192399366
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 21:43:05,785 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4550799621859056 config.input_dropout=0.11574081729234242 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.03757177919340943 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=43 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00919476790879204 optimization_config.init_lr=0.12788205638046016 optimization_config.lr_decay_power=3.699617366522253 optimization_config.lr_frac_warmup_steps=0.009101458333858165 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005428101192399366 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 21:43:05,785][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4550799621859056 config.input_dropout=0.11574081729234242 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.03757177919340943 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=43 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00919476790879204 optimization_config.init_lr=0.12788205638046016 optimization_config.lr_decay_power=3.699617366522253 optimization_config.lr_frac_warmup_steps=0.009101458333858165 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005428101192399366 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 21:43:10,795 - wandb.wandb_agent - INFO - Running runs: ['lt5jllnm']
[2025-04-09 21:43:10,795][wandb.wandb_agent][INFO] - Running runs: ['lt5jllnm']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_214312-lt5jllnm
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/lt5jllnm
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4550799621859056
Overwriting input_dropout in config from 0.25407516893041604 to 0.11574081729234242
Overwriting resid_dropout in config from 0.4697133384759005 to 0.03757177919340943
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4550799621859056
Overwriting input_dropout in config from 0.25407516893041604 to 0.11574081729234242
Overwriting resid_dropout in config from 0.4697133384759005 to 0.03757177919340943
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.70it/s][rank0]:[E409 22:13:21.588292502 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800051 milliseconds before timing out.
[rank0]:[E409 22:13:21.588706630 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 22:13:21.588740809 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 22:13:21.588760817 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 22:13:21.588777440 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 22:13:21.590034949 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800053 milliseconds before timing out.
[rank1]:[E409 22:13:21.590212041 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 22:13:21.590223183 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 22:13:21.590229084 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 22:13:21.590234432 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 22:13:21.591629004 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800053 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74744fb6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7473fcffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7473fd0007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7473fd0016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x74746df2d5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x747472894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x747472926850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800053 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74744fb6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7473fcffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7473fd0007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7473fd0016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x74746df2d5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x747472894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x747472926850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74744fb6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7473fcc5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x74746df2d5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x747472894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x747472926850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E409 22:13:21.592830499 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800051 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78e8c4d6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x78e8721fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x78e8722007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78e8722016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x78e8e28b15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x78e8e7894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x78e8e7926850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800051 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78e8c4d6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x78e8721fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x78e8722007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78e8722016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x78e8e28b15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x78e8e7894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x78e8e7926850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78e8c4d6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x78e871e5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x78e8e28b15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x78e8e7894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x78e8e7926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 22:13:24,869 - wandb.wandb_agent - INFO - Cleaning up finished run: lt5jllnm
[2025-04-09 22:13:24,869][wandb.wandb_agent][INFO] - Cleaning up finished run: lt5jllnm
2025-04-09 22:13:25,453 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 22:13:25,453][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 22:13:25,453 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.20279198746494995
	config.input_dropout: 0.019870514330882716
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.003602924614753089
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 57
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00023035491562603473
	optimization_config.init_lr: 0.002146464538852583
	optimization_config.lr_decay_power: 3.533988022935464
	optimization_config.lr_frac_warmup_steps: 0.0002052702433228485
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.001051391146520149
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 22:13:25,453][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.20279198746494995
	config.input_dropout: 0.019870514330882716
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.003602924614753089
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 57
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00023035491562603473
	optimization_config.init_lr: 0.002146464538852583
	optimization_config.lr_decay_power: 3.533988022935464
	optimization_config.lr_frac_warmup_steps: 0.0002052702433228485
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.001051391146520149
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 22:13:25,463 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.20279198746494995 config.input_dropout=0.019870514330882716 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.003602924614753089 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=57 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00023035491562603473 optimization_config.init_lr=0.002146464538852583 optimization_config.lr_decay_power=3.533988022935464 optimization_config.lr_frac_warmup_steps=0.0002052702433228485 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.001051391146520149 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 22:13:25,463][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.20279198746494995 config.input_dropout=0.019870514330882716 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.003602924614753089 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=57 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00023035491562603473 optimization_config.init_lr=0.002146464538852583 optimization_config.lr_decay_power=3.533988022935464 optimization_config.lr_frac_warmup_steps=0.0002052702433228485 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.001051391146520149 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 22:13:30,476 - wandb.wandb_agent - INFO - Running runs: ['ef3shr2p']
[2025-04-09 22:13:30,476][wandb.wandb_agent][INFO] - Running runs: ['ef3shr2p']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_221332-ef3shr2p
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/ef3shr2p
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.20279198746494995
Overwriting input_dropout in config from 0.25407516893041604 to 0.019870514330882716
Overwriting resid_dropout in config from 0.4697133384759005 to 0.003602924614753089
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.20279198746494995
Overwriting input_dropout in config from 0.25407516893041604 to 0.019870514330882716
Overwriting resid_dropout in config from 0.4697133384759005 to 0.003602924614753089
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.65it/s][rank1]:[E409 22:43:41.372941660 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800061 milliseconds before timing out.
[rank1]:[E409 22:43:41.373300293 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 22:43:41.373324887 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 22:43:41.373336183 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 22:43:41.373346673 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 22:43:41.376328549 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800061 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75a64ad6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x75a5f81fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x75a5f82007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75a5f82016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x75a66913e5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x75a66da94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x75a66db26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800061 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75a64ad6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x75a5f81fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x75a5f82007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75a5f82016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x75a66913e5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x75a66da94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x75a66db26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75a64ad6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x75a5f7e5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x75a66913e5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x75a66da94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x75a66db26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E409 22:43:41.382979507 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800071 milliseconds before timing out.
[rank0]:[E409 22:43:41.383253578 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 22:43:41.383273063 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 22:43:41.383282024 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 22:43:41.383290446 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 22:43:41.386336348 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800071 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f060ed6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f05bc1fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f05bc2007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f05bc2016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f062d1075c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f0631a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f0631b26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800071 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f060ed6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f05bc1fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f05bc2007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f05bc2016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f062d1075c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f0631a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f0631b26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f060ed6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f05bbe5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f062d1075c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f0631a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7f0631b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 22:43:45,243 - wandb.wandb_agent - INFO - Cleaning up finished run: ef3shr2p
[2025-04-09 22:43:45,243][wandb.wandb_agent][INFO] - Cleaning up finished run: ef3shr2p
2025-04-09 22:43:45,866 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 22:43:45,866][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 22:43:45,866 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4548791851603536
	config.input_dropout: 0.35742430760977956
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.00959350917071472
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 12
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00034443861423593406
	optimization_config.init_lr: 1.319977055594456e-05
	optimization_config.lr_decay_power: 3.85401431785974
	optimization_config.lr_frac_warmup_steps: 3.2624869485883484e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0019778126033462096
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 22:43:45,866][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4548791851603536
	config.input_dropout: 0.35742430760977956
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.00959350917071472
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 12
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00034443861423593406
	optimization_config.init_lr: 1.319977055594456e-05
	optimization_config.lr_decay_power: 3.85401431785974
	optimization_config.lr_frac_warmup_steps: 3.2624869485883484e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0019778126033462096
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 22:43:45,876 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4548791851603536 config.input_dropout=0.35742430760977956 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.00959350917071472 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=12 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00034443861423593406 optimization_config.init_lr=1.319977055594456e-05 optimization_config.lr_decay_power=3.85401431785974 optimization_config.lr_frac_warmup_steps=3.2624869485883484e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0019778126033462096 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 22:43:45,876][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4548791851603536 config.input_dropout=0.35742430760977956 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.00959350917071472 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=12 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00034443861423593406 optimization_config.init_lr=1.319977055594456e-05 optimization_config.lr_decay_power=3.85401431785974 optimization_config.lr_frac_warmup_steps=3.2624869485883484e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0019778126033462096 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 22:43:50,889 - wandb.wandb_agent - INFO - Running runs: ['2t16lrfy']
[2025-04-09 22:43:50,889][wandb.wandb_agent][INFO] - Running runs: ['2t16lrfy']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_224352-2t16lrfy
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/2t16lrfy
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4548791851603536
Overwriting input_dropout in config from 0.25407516893041604 to 0.35742430760977956
Overwriting resid_dropout in config from 0.4697133384759005 to 0.00959350917071472
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4548791851603536
Overwriting input_dropout in config from 0.25407516893041604 to 0.35742430760977956
Overwriting resid_dropout in config from 0.4697133384759005 to 0.00959350917071472
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.72it/s][rank0]:[E409 23:14:02.852677522 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800074 milliseconds before timing out.
[rank0]:[E409 23:14:02.853042438 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 23:14:02.853068066 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 23:14:02.853079820 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 23:14:02.853089878 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E409 23:14:02.856091751 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800074 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e5151b6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7e50feffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7e50ff0007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e50ff0016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7e516f6785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7e5174694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7e5174726850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800074 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e5151b6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7e50feffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7e50ff0007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e50ff0016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7e516f6785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7e5174694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7e5174726850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e5151b6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7e50fec5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7e516f6785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7e5174694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7e5174726850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E409 23:14:02.860369330 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800081 milliseconds before timing out.
[rank1]:[E409 23:14:02.860658860 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 23:14:02.860687246 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 23:14:02.860696371 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 23:14:02.860704709 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 23:14:02.863657300 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800081 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x799cb7a6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x799c64ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x799c650007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x799c650016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x799cd5cea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x799cda694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x799cda726850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800081 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x799cb7a6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x799c64ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x799c650007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x799c650016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x799cd5cea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x799cda694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x799cda726850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x799cb7a6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x799c64c5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x799cd5cea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x799cda694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x799cda726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 23:14:04,170 - wandb.wandb_agent - INFO - Cleaning up finished run: 2t16lrfy
[2025-04-09 23:14:04,170][wandb.wandb_agent][INFO] - Cleaning up finished run: 2t16lrfy
2025-04-09 23:14:04,915 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 23:14:04,915][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 23:14:04,915 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.21158041406199665
	config.input_dropout: 0.4669354431514918
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2758577651619234
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 21
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.04722481311103115
	optimization_config.init_lr: 0.0002712916723394125
	optimization_config.lr_decay_power: 1.710498436110135
	optimization_config.lr_frac_warmup_steps: 0.0015054037708376422
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005595873797792152
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 23:14:04,915][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.21158041406199665
	config.input_dropout: 0.4669354431514918
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2758577651619234
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 21
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.04722481311103115
	optimization_config.init_lr: 0.0002712916723394125
	optimization_config.lr_decay_power: 1.710498436110135
	optimization_config.lr_frac_warmup_steps: 0.0015054037708376422
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005595873797792152
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 23:14:04,931 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.21158041406199665 config.input_dropout=0.4669354431514918 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2758577651619234 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=21 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.04722481311103115 optimization_config.init_lr=0.0002712916723394125 optimization_config.lr_decay_power=1.710498436110135 optimization_config.lr_frac_warmup_steps=0.0015054037708376422 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005595873797792152 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 23:14:04,931][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.21158041406199665 config.input_dropout=0.4669354431514918 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2758577651619234 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=21 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.04722481311103115 optimization_config.init_lr=0.0002712916723394125 optimization_config.lr_decay_power=1.710498436110135 optimization_config.lr_frac_warmup_steps=0.0015054037708376422 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005595873797792152 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 23:14:09,940 - wandb.wandb_agent - INFO - Running runs: ['enoakbf1']
[2025-04-09 23:14:09,940][wandb.wandb_agent][INFO] - Running runs: ['enoakbf1']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_231412-enoakbf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/enoakbf1
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.21158041406199665
Overwriting input_dropout in config from 0.25407516893041604 to 0.4669354431514918
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2758577651619234
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.21158041406199665
Overwriting input_dropout in config from 0.25407516893041604 to 0.4669354431514918
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2758577651619234
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.66it/s][rank0]:[E409 23:44:21.036472804 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800044 milliseconds before timing out.
[rank0]:[E409 23:44:21.036809944 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E409 23:44:21.036826177 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E409 23:44:21.036835235 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E409 23:44:21.036843110 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 23:44:21.037051661 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800045 milliseconds before timing out.
[rank1]:[E409 23:44:21.037218550 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E409 23:44:21.037228304 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E409 23:44:21.037233362 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E409 23:44:21.037237723 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E409 23:44:21.038721363 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800045 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73d48706c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x73d4345fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x73d4346007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73d4346016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x73d4a54b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x73d4a9e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x73d4a9f26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800045 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73d48706c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x73d4345fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x73d4346007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73d4346016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x73d4a54b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x73d4a9e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x73d4a9f26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73d48706c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x73d43425c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x73d4a54b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x73d4a9e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x73d4a9f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E409 23:44:21.039871870 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800044 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fc0e7b6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fc094ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fc0950007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fc0950016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fc105f3e5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fc10a894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fc10a926850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800044 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fc0e7b6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fc094ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fc0950007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fc0950016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fc105f3e5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fc10a894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fc10a926850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fc0e7b6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7fc094c5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7fc105f3e5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7fc10a894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7fc10a926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-09 23:44:23,383 - wandb.wandb_agent - INFO - Cleaning up finished run: enoakbf1
[2025-04-09 23:44:23,383][wandb.wandb_agent][INFO] - Cleaning up finished run: enoakbf1
2025-04-09 23:44:24,042 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-09 23:44:24,042][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-09 23:44:24,043 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3944612442670336
	config.input_dropout: 0.21739815265532192
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.44695835722038824
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 18
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00015381958015353155
	optimization_config.init_lr: 0.054981156396576074
	optimization_config.lr_decay_power: 1.1622121978220046
	optimization_config.lr_frac_warmup_steps: 0.00016253965032975618
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00691520618342861
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-09 23:44:24,043][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3944612442670336
	config.input_dropout: 0.21739815265532192
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.44695835722038824
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 18
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00015381958015353155
	optimization_config.init_lr: 0.054981156396576074
	optimization_config.lr_decay_power: 1.1622121978220046
	optimization_config.lr_frac_warmup_steps: 0.00016253965032975618
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00691520618342861
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-09 23:44:24,052 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3944612442670336 config.input_dropout=0.21739815265532192 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.44695835722038824 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=18 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00015381958015353155 optimization_config.init_lr=0.054981156396576074 optimization_config.lr_decay_power=1.1622121978220046 optimization_config.lr_frac_warmup_steps=0.00016253965032975618 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00691520618342861 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-09 23:44:24,052][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3944612442670336 config.input_dropout=0.21739815265532192 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.44695835722038824 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=18 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00015381958015353155 optimization_config.init_lr=0.054981156396576074 optimization_config.lr_decay_power=1.1622121978220046 optimization_config.lr_frac_warmup_steps=0.00016253965032975618 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00691520618342861 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-09 23:44:29,065 - wandb.wandb_agent - INFO - Running runs: ['j4hp2iy3']
[2025-04-09 23:44:29,065][wandb.wandb_agent][INFO] - Running runs: ['j4hp2iy3']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250409_234431-j4hp2iy3
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/j4hp2iy3
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3944612442670336
Overwriting input_dropout in config from 0.25407516893041604 to 0.21739815265532192
Overwriting resid_dropout in config from 0.4697133384759005 to 0.44695835722038824
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3944612442670336
Overwriting input_dropout in config from 0.25407516893041604 to 0.21739815265532192
Overwriting resid_dropout in config from 0.4697133384759005 to 0.44695835722038824
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.65it/s][rank1]:[E410 00:14:40.948121935 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800048 milliseconds before timing out.
[rank1]:[E410 00:14:40.948442373 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 00:14:40.948462804 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 00:14:40.948474227 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 00:14:40.948484630 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 00:14:40.949597387 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800049 milliseconds before timing out.
[rank0]:[E410 00:14:40.949880027 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 00:14:40.949900642 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 00:14:40.949911975 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 00:14:40.949922304 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 00:14:40.951578682 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800048 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x706b8ac6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x706b381fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x706b382007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x706b382016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x706ba88cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x706bada94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x706badb26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800048 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x706b8ac6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x706b381fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x706b382007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x706b382016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x706ba88cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x706bada94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x706badb26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x706b8ac6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x706b37e5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x706ba88cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x706bada94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x706badb26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 00:14:40.952910593 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800049 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75bc2746c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x75bbd49fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x75bbd4a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75bbd4a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x75bc457065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x75bc4a094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x75bc4a126850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800049 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75bc2746c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x75bbd49fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x75bbd4a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75bbd4a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x75bc457065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x75bc4a094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x75bc4a126850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75bc2746c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x75bbd465c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x75bc457065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x75bc4a094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x75bc4a126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 00:14:41,576 - wandb.wandb_agent - INFO - Cleaning up finished run: j4hp2iy3
[2025-04-10 00:14:41,576][wandb.wandb_agent][INFO] - Cleaning up finished run: j4hp2iy3
2025-04-10 00:14:42,234 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 00:14:42,234][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 00:14:42,234 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3986457977642952
	config.input_dropout: 0.03747619406458502
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3423329937818933
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 55
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0006116261752677302
	optimization_config.init_lr: 5.936377005955464e-05
	optimization_config.lr_decay_power: 4.975806399396092
	optimization_config.lr_frac_warmup_steps: 1.074867320461746e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.006437557727720874
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 00:14:42,234][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3986457977642952
	config.input_dropout: 0.03747619406458502
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3423329937818933
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 55
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0006116261752677302
	optimization_config.init_lr: 5.936377005955464e-05
	optimization_config.lr_decay_power: 4.975806399396092
	optimization_config.lr_frac_warmup_steps: 1.074867320461746e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.006437557727720874
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 00:14:42,244 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3986457977642952 config.input_dropout=0.03747619406458502 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3423329937818933 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=55 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0006116261752677302 optimization_config.init_lr=5.936377005955464e-05 optimization_config.lr_decay_power=4.975806399396092 optimization_config.lr_frac_warmup_steps=1.074867320461746e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.006437557727720874 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 00:14:42,244][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3986457977642952 config.input_dropout=0.03747619406458502 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3423329937818933 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=55 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0006116261752677302 optimization_config.init_lr=5.936377005955464e-05 optimization_config.lr_decay_power=4.975806399396092 optimization_config.lr_frac_warmup_steps=1.074867320461746e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.006437557727720874 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 00:14:47,256 - wandb.wandb_agent - INFO - Running runs: ['rwdzcffi']
[2025-04-10 00:14:47,256][wandb.wandb_agent][INFO] - Running runs: ['rwdzcffi']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_001449-rwdzcffi
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/rwdzcffi
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3986457977642952
Overwriting input_dropout in config from 0.25407516893041604 to 0.03747619406458502
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3423329937818933
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3986457977642952
Overwriting input_dropout in config from 0.25407516893041604 to 0.03747619406458502
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3423329937818933
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.58it/s][rank1]:[E410 00:44:58.026829180 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800010 milliseconds before timing out.
[rank1]:[E410 00:44:58.027180728 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 00:44:58.027203734 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 00:44:58.027215644 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 00:44:58.027226324 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 00:44:58.030218661 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800010 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7855af26c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x78555c7fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x78555c8007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78555c8016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7855cd4ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7855d1e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7855d1f26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800010 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7855af26c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x78555c7fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x78555c8007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78555c8016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7855cd4ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7855d1e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7855d1f26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7855af26c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x78555c45c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7855cd4ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7855d1e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7855d1f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 00:44:58.042137393 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800026 milliseconds before timing out.
[rank0]:[E410 00:44:58.042422583 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 00:44:58.042438833 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 00:44:58.042447434 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 00:44:58.042469953 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 00:44:58.045448159 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800026 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78440e66c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7843bbbfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7843bbc007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7843bbc016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x78442c2785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x784431294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x784431326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800026 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78440e66c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7843bbbfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7843bbc007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7843bbc016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x78442c2785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x784431294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x784431326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78440e66c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7843bb85c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x78442c2785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x784431294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x784431326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 00:45:02,646 - wandb.wandb_agent - INFO - Cleaning up finished run: rwdzcffi
[2025-04-10 00:45:02,646][wandb.wandb_agent][INFO] - Cleaning up finished run: rwdzcffi
2025-04-10 00:45:03,271 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 00:45:03,271][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 00:45:03,271 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3327855667737942
	config.input_dropout: 0.3746575924059342
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.06529803205725376
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 55
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.370985284477982
	optimization_config.init_lr: 0.16318989702449557
	optimization_config.lr_decay_power: 4.657599809342539
	optimization_config.lr_frac_warmup_steps: 0.0008475200023641915
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.008784206078301514
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 00:45:03,271][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3327855667737942
	config.input_dropout: 0.3746575924059342
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.06529803205725376
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 55
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.370985284477982
	optimization_config.init_lr: 0.16318989702449557
	optimization_config.lr_decay_power: 4.657599809342539
	optimization_config.lr_frac_warmup_steps: 0.0008475200023641915
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.008784206078301514
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 00:45:03,281 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3327855667737942 config.input_dropout=0.3746575924059342 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.06529803205725376 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=55 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.370985284477982 optimization_config.init_lr=0.16318989702449557 optimization_config.lr_decay_power=4.657599809342539 optimization_config.lr_frac_warmup_steps=0.0008475200023641915 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.008784206078301514 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 00:45:03,281][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3327855667737942 config.input_dropout=0.3746575924059342 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.06529803205725376 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=55 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.370985284477982 optimization_config.init_lr=0.16318989702449557 optimization_config.lr_decay_power=4.657599809342539 optimization_config.lr_frac_warmup_steps=0.0008475200023641915 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.008784206078301514 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 00:45:08,293 - wandb.wandb_agent - INFO - Running runs: ['vuwbwssj']
[2025-04-10 00:45:08,293][wandb.wandb_agent][INFO] - Running runs: ['vuwbwssj']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_004510-vuwbwssj
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/vuwbwssj
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3327855667737942
Overwriting input_dropout in config from 0.25407516893041604 to 0.3746575924059342
Overwriting resid_dropout in config from 0.4697133384759005 to 0.06529803205725376
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3327855667737942
Overwriting input_dropout in config from 0.25407516893041604 to 0.3746575924059342
Overwriting resid_dropout in config from 0.4697133384759005 to 0.06529803205725376
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.56it/s][rank1]:[E410 01:15:19.984733132 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800054 milliseconds before timing out.
[rank1]:[E410 01:15:19.985074619 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 01:15:19.985105113 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 01:15:19.985129787 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 01:15:19.985138419 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 01:15:19.988165601 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800054 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79423b46c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7941e89fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7941e8a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7941e8a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7942598b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x79425e294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x79425e326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800054 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79423b46c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7941e89fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7941e8a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7941e8a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7942598b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x79425e294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x79425e326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79423b46c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7941e865c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7942598b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x79425e294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x79425e326850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 01:15:19.006735537 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800076 milliseconds before timing out.
[rank0]:[E410 01:15:19.007023087 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 01:15:19.007041941 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 01:15:19.007050926 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 01:15:19.007058791 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 01:15:19.010123988 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800076 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ad0bb66c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7ad068bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7ad068c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ad068c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ad0d98ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ad0de294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ad0de326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800076 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ad0bb66c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7ad068bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7ad068c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ad068c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ad0d98ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ad0de294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ad0de326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ad0bb66c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7ad06885c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7ad0d98ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7ad0de294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7ad0de326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 01:15:23,871 - wandb.wandb_agent - INFO - Cleaning up finished run: vuwbwssj
[2025-04-10 01:15:23,871][wandb.wandb_agent][INFO] - Cleaning up finished run: vuwbwssj
2025-04-10 01:15:24,591 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 01:15:24,591][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 01:15:24,591 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.0815222205576891
	config.input_dropout: 0.32506230705923117
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.1483416174831066
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 40
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00017038484667825134
	optimization_config.init_lr: 0.03217686342648206
	optimization_config.lr_decay_power: 4.536408348441466
	optimization_config.lr_frac_warmup_steps: 0.04799118433190545
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.008132404956590747
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 01:15:24,591][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.0815222205576891
	config.input_dropout: 0.32506230705923117
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.1483416174831066
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 40
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00017038484667825134
	optimization_config.init_lr: 0.03217686342648206
	optimization_config.lr_decay_power: 4.536408348441466
	optimization_config.lr_frac_warmup_steps: 0.04799118433190545
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.008132404956590747
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 01:15:24,597 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.0815222205576891 config.input_dropout=0.32506230705923117 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.1483416174831066 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=40 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00017038484667825134 optimization_config.init_lr=0.03217686342648206 optimization_config.lr_decay_power=4.536408348441466 optimization_config.lr_frac_warmup_steps=0.04799118433190545 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.008132404956590747 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 01:15:24,597][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.0815222205576891 config.input_dropout=0.32506230705923117 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.1483416174831066 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=40 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00017038484667825134 optimization_config.init_lr=0.03217686342648206 optimization_config.lr_decay_power=4.536408348441466 optimization_config.lr_frac_warmup_steps=0.04799118433190545 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.008132404956590747 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 01:15:29,609 - wandb.wandb_agent - INFO - Running runs: ['9lv6p8zt']
[2025-04-10 01:15:29,609][wandb.wandb_agent][INFO] - Running runs: ['9lv6p8zt']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_011531-9lv6p8zt
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/9lv6p8zt
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.0815222205576891
Overwriting input_dropout in config from 0.25407516893041604 to 0.32506230705923117
Overwriting resid_dropout in config from 0.4697133384759005 to 0.1483416174831066
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.0815222205576891
Overwriting input_dropout in config from 0.25407516893041604 to 0.32506230705923117
Overwriting resid_dropout in config from 0.4697133384759005 to 0.1483416174831066
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.66it/s][rank0]:[E410 01:45:40.585358478 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800017 milliseconds before timing out.
[rank0]:[E410 01:45:40.585578342 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 01:45:40.585589656 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 01:45:40.585596342 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 01:45:40.585601944 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 01:45:40.586694372 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800017 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ca24a46c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7ca1f79fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7ca1f7a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ca1f7a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ca2686ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ca26d094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ca26d126850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800017 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ca24a46c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7ca1f79fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7ca1f7a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ca1f7a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ca2686ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ca26d094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ca26d126850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ca24a46c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7ca1f765c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7ca2686ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7ca26d094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7ca26d126850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E410 01:45:40.587287309 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800018 milliseconds before timing out.
[rank1]:[E410 01:45:40.587455150 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 01:45:40.587462327 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 01:45:40.587465826 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 01:45:40.587469116 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 01:45:40.588552819 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800018 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76c1dc76c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x76c189bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x76c189c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76c189c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x76c1faae65c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x76c1ff494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x76c1ff526850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800018 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76c1dc76c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x76c189bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x76c189c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76c189c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x76c1faae65c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x76c1ff494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x76c1ff526850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76c1dc76c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x76c18985c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x76c1faae65c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x76c1ff494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x76c1ff526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 01:45:44,638 - wandb.wandb_agent - INFO - Cleaning up finished run: 9lv6p8zt
[2025-04-10 01:45:44,638][wandb.wandb_agent][INFO] - Cleaning up finished run: 9lv6p8zt
2025-04-10 01:45:45,575 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 01:45:45,575][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 01:45:45,575 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4703296286157722
	config.input_dropout: 0.12927082629440506
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.22363744860292567
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 40
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.005260153741184706
	optimization_config.init_lr: 0.0035823018393959117
	optimization_config.lr_decay_power: 3.2106353846276363
	optimization_config.lr_frac_warmup_steps: 2.3041278509113673e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007473137299691017
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 01:45:45,575][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4703296286157722
	config.input_dropout: 0.12927082629440506
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.22363744860292567
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 40
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.005260153741184706
	optimization_config.init_lr: 0.0035823018393959117
	optimization_config.lr_decay_power: 3.2106353846276363
	optimization_config.lr_frac_warmup_steps: 2.3041278509113673e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007473137299691017
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 01:45:45,585 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4703296286157722 config.input_dropout=0.12927082629440506 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.22363744860292567 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=40 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.005260153741184706 optimization_config.init_lr=0.0035823018393959117 optimization_config.lr_decay_power=3.2106353846276363 optimization_config.lr_frac_warmup_steps=2.3041278509113673e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007473137299691017 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 01:45:45,585][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4703296286157722 config.input_dropout=0.12927082629440506 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.22363744860292567 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=40 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.005260153741184706 optimization_config.init_lr=0.0035823018393959117 optimization_config.lr_decay_power=3.2106353846276363 optimization_config.lr_frac_warmup_steps=2.3041278509113673e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007473137299691017 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 01:45:50,596 - wandb.wandb_agent - INFO - Running runs: ['0aj48muv']
[2025-04-10 01:45:50,596][wandb.wandb_agent][INFO] - Running runs: ['0aj48muv']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_014552-0aj48muv
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/0aj48muv
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4703296286157722
Overwriting input_dropout in config from 0.25407516893041604 to 0.12927082629440506
Overwriting resid_dropout in config from 0.4697133384759005 to 0.22363744860292567
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4703296286157722
Overwriting input_dropout in config from 0.25407516893041604 to 0.12927082629440506
Overwriting resid_dropout in config from 0.4697133384759005 to 0.22363744860292567
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.58it/s][rank0]:[E410 02:16:01.550309549 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800032 milliseconds before timing out.
[rank0]:[E410 02:16:01.550647994 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 02:16:01.550668455 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 02:16:01.550679949 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 02:16:01.550690941 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 02:16:01.552307058 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800034 milliseconds before timing out.
[rank1]:[E410 02:16:01.552661538 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 02:16:01.552688957 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 02:16:01.552707741 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 02:16:01.552724574 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 02:16:01.554338659 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800032 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a8409c6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7a83b71fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7a83b72007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a83b72016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7a8427ecb5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7a842c894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7a842c926850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800032 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a8409c6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7a83b71fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7a83b72007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a83b72016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7a8427ecb5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7a842c894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7a842c926850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a8409c6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7a83b6e5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7a8427ecb5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7a842c894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7a842c926850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E410 02:16:01.556829563 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800034 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fefa9e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fef573fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fef574007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fef574016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fefc80b75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fefcca94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fefccb26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800034 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fefa9e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fef573fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fef574007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fef574016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fefc80b75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fefcca94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fefccb26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fefa9e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7fef5705c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7fefc80b75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7fefcca94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7fefccb26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 02:16:03,485 - wandb.wandb_agent - INFO - Cleaning up finished run: 0aj48muv
[2025-04-10 02:16:03,485][wandb.wandb_agent][INFO] - Cleaning up finished run: 0aj48muv
2025-04-10 02:16:04,108 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 02:16:04,108][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 02:16:04,108 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4384489490160975
	config.input_dropout: 0.28553053105981363
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.17745897227399982
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 47
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0001226992996206237
	optimization_config.init_lr: 1.69414368436764e-06
	optimization_config.lr_decay_power: 2.2033018927001686
	optimization_config.lr_frac_warmup_steps: 3.5965968760296773e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004406249301861234
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 02:16:04,108][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4384489490160975
	config.input_dropout: 0.28553053105981363
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.17745897227399982
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 47
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0001226992996206237
	optimization_config.init_lr: 1.69414368436764e-06
	optimization_config.lr_decay_power: 2.2033018927001686
	optimization_config.lr_frac_warmup_steps: 3.5965968760296773e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004406249301861234
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 02:16:04,118 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4384489490160975 config.input_dropout=0.28553053105981363 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.17745897227399982 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=47 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0001226992996206237 optimization_config.init_lr=1.69414368436764e-06 optimization_config.lr_decay_power=2.2033018927001686 optimization_config.lr_frac_warmup_steps=3.5965968760296773e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004406249301861234 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 02:16:04,118][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4384489490160975 config.input_dropout=0.28553053105981363 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.17745897227399982 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=47 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0001226992996206237 optimization_config.init_lr=1.69414368436764e-06 optimization_config.lr_decay_power=2.2033018927001686 optimization_config.lr_frac_warmup_steps=3.5965968760296773e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004406249301861234 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 02:16:09,130 - wandb.wandb_agent - INFO - Running runs: ['0c0jhfxv']
[2025-04-10 02:16:09,130][wandb.wandb_agent][INFO] - Running runs: ['0c0jhfxv']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_021611-0c0jhfxv
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/0c0jhfxv
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4384489490160975
Overwriting input_dropout in config from 0.25407516893041604 to 0.28553053105981363
Overwriting resid_dropout in config from 0.4697133384759005 to 0.17745897227399982
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4384489490160975
Overwriting input_dropout in config from 0.25407516893041604 to 0.28553053105981363
Overwriting resid_dropout in config from 0.4697133384759005 to 0.17745897227399982
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.56it/s][rank0]:[E410 02:46:20.858895764 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800077 milliseconds before timing out.
[rank0]:[E410 02:46:20.859242047 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 02:46:20.859266390 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 02:46:20.859277830 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 02:46:20.859288133 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 02:46:20.862296709 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800077 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bd8a566c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7bd852bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7bd852c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bd852c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7bd8c39065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7bd8c8294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7bd8c8326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800077 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bd8a566c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7bd852bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7bd852c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bd852c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7bd8c39065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7bd8c8294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7bd8c8326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bd8a566c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7bd85285c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7bd8c39065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7bd8c8294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7bd8c8326850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E410 02:46:20.865613279 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800084 milliseconds before timing out.
[rank1]:[E410 02:46:20.865766801 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 02:46:20.865775165 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 02:46:20.865779006 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 02:46:20.865782762 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 02:46:20.867067254 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800084 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a14dca6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7a1489ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7a148a0007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a148a0016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7a14fa4ad5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7a14ff694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7a14ff726850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800084 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a14dca6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7a1489ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7a148a0007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a148a0016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7a14fa4ad5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7a14ff694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7a14ff726850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a14dca6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7a1489c5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7a14fa4ad5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7a14ff694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7a14ff726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 02:46:22,858 - wandb.wandb_agent - INFO - Cleaning up finished run: 0c0jhfxv
[2025-04-10 02:46:22,858][wandb.wandb_agent][INFO] - Cleaning up finished run: 0c0jhfxv
2025-04-10 02:46:23,361 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 02:46:23,361][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 02:46:23,361 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.2258850353947776
	config.input_dropout: 0.16597806496045053
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.4994751792254868
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 53
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0189811570782469
	optimization_config.init_lr: 0.20818709431053792
	optimization_config.lr_decay_power: 2.182205802110733
	optimization_config.lr_frac_warmup_steps: 0.00918324041861005
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0015161468455341604
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 02:46:23,361][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.2258850353947776
	config.input_dropout: 0.16597806496045053
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.4994751792254868
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 53
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0189811570782469
	optimization_config.init_lr: 0.20818709431053792
	optimization_config.lr_decay_power: 2.182205802110733
	optimization_config.lr_frac_warmup_steps: 0.00918324041861005
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0015161468455341604
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 02:46:23,371 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2258850353947776 config.input_dropout=0.16597806496045053 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.4994751792254868 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=53 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0189811570782469 optimization_config.init_lr=0.20818709431053792 optimization_config.lr_decay_power=2.182205802110733 optimization_config.lr_frac_warmup_steps=0.00918324041861005 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0015161468455341604 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 02:46:23,371][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2258850353947776 config.input_dropout=0.16597806496045053 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.4994751792254868 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=53 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0189811570782469 optimization_config.init_lr=0.20818709431053792 optimization_config.lr_decay_power=2.182205802110733 optimization_config.lr_frac_warmup_steps=0.00918324041861005 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0015161468455341604 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 02:46:28,383 - wandb.wandb_agent - INFO - Running runs: ['aay16xz6']
[2025-04-10 02:46:28,383][wandb.wandb_agent][INFO] - Running runs: ['aay16xz6']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_024630-aay16xz6
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/aay16xz6
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.2258850353947776
Overwriting input_dropout in config from 0.25407516893041604 to 0.16597806496045053
Overwriting resid_dropout in config from 0.4697133384759005 to 0.4994751792254868
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.2258850353947776
Overwriting input_dropout in config from 0.25407516893041604 to 0.16597806496045053
Overwriting resid_dropout in config from 0.4697133384759005 to 0.4994751792254868
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.57it/s][rank0]:[E410 03:16:39.327126367 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
[rank0]:[E410 03:16:39.327335434 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 03:16:39.327346574 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 03:16:39.327352235 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 03:16:39.327357576 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 03:16:39.328812105 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76e67cc6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x76e62a1fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x76e62a2007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76e62a2016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x76e69a8785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x76e69f894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x76e69f926850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76e67cc6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x76e62a1fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x76e62a2007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76e62a2016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x76e69a8785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x76e69f894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x76e69f926850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76e67cc6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x76e629e5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x76e69a8785c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x76e69f894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x76e69f926850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E410 03:16:39.340539139 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800036 milliseconds before timing out.
[rank1]:[E410 03:16:39.340690248 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 03:16:39.340697281 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 03:16:39.340700646 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 03:16:39.340703895 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 03:16:39.341760591 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800036 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e188706c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7e18345fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7e18346007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e18346016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7e18a4aad5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7e18a9c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7e18a9d26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800036 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e188706c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7e18345fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7e18346007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e18346016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7e18a4aad5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7e18a9c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7e18a9d26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e188706c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7e183425c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7e18a4aad5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7e18a9c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7e18a9d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 03:16:44,025 - wandb.wandb_agent - INFO - Cleaning up finished run: aay16xz6
[2025-04-10 03:16:44,025][wandb.wandb_agent][INFO] - Cleaning up finished run: aay16xz6
2025-04-10 03:16:44,636 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 03:16:44,636][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 03:16:44,636 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4069938321211485
	config.input_dropout: 0.3549804442230614
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.07560544037926215
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 41
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00019257208087495857
	optimization_config.init_lr: 0.005945864491339805
	optimization_config.lr_decay_power: 4.424320781952467
	optimization_config.lr_frac_warmup_steps: 1.615769028260814e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009843501579891186
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 03:16:44,636][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4069938321211485
	config.input_dropout: 0.3549804442230614
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.07560544037926215
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 41
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00019257208087495857
	optimization_config.init_lr: 0.005945864491339805
	optimization_config.lr_decay_power: 4.424320781952467
	optimization_config.lr_frac_warmup_steps: 1.615769028260814e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009843501579891186
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 03:16:44,646 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4069938321211485 config.input_dropout=0.3549804442230614 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.07560544037926215 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=41 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00019257208087495857 optimization_config.init_lr=0.005945864491339805 optimization_config.lr_decay_power=4.424320781952467 optimization_config.lr_frac_warmup_steps=1.615769028260814e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009843501579891186 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 03:16:44,646][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4069938321211485 config.input_dropout=0.3549804442230614 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.07560544037926215 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=41 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00019257208087495857 optimization_config.init_lr=0.005945864491339805 optimization_config.lr_decay_power=4.424320781952467 optimization_config.lr_frac_warmup_steps=1.615769028260814e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009843501579891186 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 03:16:49,658 - wandb.wandb_agent - INFO - Running runs: ['hmymnl6c']
[2025-04-10 03:16:49,658][wandb.wandb_agent][INFO] - Running runs: ['hmymnl6c']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_031651-hmymnl6c
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/hmymnl6c
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4069938321211485
Overwriting input_dropout in config from 0.25407516893041604 to 0.3549804442230614
Overwriting resid_dropout in config from 0.4697133384759005 to 0.07560544037926215
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4069938321211485
Overwriting input_dropout in config from 0.25407516893041604 to 0.3549804442230614
Overwriting resid_dropout in config from 0.4697133384759005 to 0.07560544037926215
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.58it/s][rank1]:[E410 03:47:00.578851184 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800002 milliseconds before timing out.
[rank1]:[E410 03:47:00.579188548 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 03:47:00.585252020 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 03:47:00.585267896 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 03:47:00.585278492 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 03:47:00.588367600 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800002 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff6dc56c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7ff6899fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7ff689a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ff689a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ff6fa0b15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ff6ff094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ff6ff126850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800002 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff6dc56c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7ff6899fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7ff689a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ff689a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ff6fa0b15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ff6ff094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ff6ff126850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff6dc56c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7ff68965c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7ff6fa0b15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7ff6ff094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7ff6ff126850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 03:47:00.592519693 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800015 milliseconds before timing out.
[rank0]:[E410 03:47:00.592693108 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 03:47:00.592702695 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 03:47:00.592707835 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 03:47:00.592712035 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 03:47:00.594158576 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800015 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73437856c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7343259fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x734325a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x734325a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7343968fa5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x73439b294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x73439b326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800015 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73437856c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7343259fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x734325a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x734325a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7343968fa5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x73439b294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x73439b326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73437856c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x73432565c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7343968fa5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x73439b294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x73439b326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 03:47:03,132 - wandb.wandb_agent - INFO - Cleaning up finished run: hmymnl6c
[2025-04-10 03:47:03,132][wandb.wandb_agent][INFO] - Cleaning up finished run: hmymnl6c
2025-04-10 03:47:03,659 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 03:47:03,659][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 03:47:03,660 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4138720506000237
	config.input_dropout: 0.4199523679282692
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3673538961052003
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 25
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00010111210831440264
	optimization_config.init_lr: 0.0024608769304400703
	optimization_config.lr_decay_power: 1.9136011272908264
	optimization_config.lr_frac_warmup_steps: 0.04998210260304091
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007986425515297603
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 03:47:03,660][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4138720506000237
	config.input_dropout: 0.4199523679282692
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3673538961052003
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 25
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00010111210831440264
	optimization_config.init_lr: 0.0024608769304400703
	optimization_config.lr_decay_power: 1.9136011272908264
	optimization_config.lr_frac_warmup_steps: 0.04998210260304091
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.007986425515297603
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 03:47:03,670 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4138720506000237 config.input_dropout=0.4199523679282692 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3673538961052003 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=25 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00010111210831440264 optimization_config.init_lr=0.0024608769304400703 optimization_config.lr_decay_power=1.9136011272908264 optimization_config.lr_frac_warmup_steps=0.04998210260304091 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007986425515297603 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 03:47:03,670][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4138720506000237 config.input_dropout=0.4199523679282692 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3673538961052003 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=25 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00010111210831440264 optimization_config.init_lr=0.0024608769304400703 optimization_config.lr_decay_power=1.9136011272908264 optimization_config.lr_frac_warmup_steps=0.04998210260304091 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.007986425515297603 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 03:47:08,682 - wandb.wandb_agent - INFO - Running runs: ['3kc71gem']
[2025-04-10 03:47:08,682][wandb.wandb_agent][INFO] - Running runs: ['3kc71gem']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_034710-3kc71gem
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/3kc71gem
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4138720506000237
Overwriting input_dropout in config from 0.25407516893041604 to 0.4199523679282692
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3673538961052003
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4138720506000237
Overwriting input_dropout in config from 0.25407516893041604 to 0.4199523679282692
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3673538961052003
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.58it/s][rank1]:[E410 04:17:19.611809134 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800043 milliseconds before timing out.
[rank0]:[E410 04:17:19.611890193 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800043 milliseconds before timing out.
[rank1]:[E410 04:17:19.612159765 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 04:17:19.612182955 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 04:17:19.612193988 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 04:17:19.612204043 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 04:17:19.612197680 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 04:17:19.612239730 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 04:17:19.612249579 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 04:17:19.612257741 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 04:17:19.615236757 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800043 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77a8c216c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x77a86f5fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x77a86f6007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77a86f6016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x77a8dfc5c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x77a8e4c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x77a8e4d26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of '[rank1]:[E410 04:17:19.615281379 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800043 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77eaaf76c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x77ea5cbfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x77ea5cc007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77ea5cc016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x77eacd2565c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x77ead2294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x77ead2326850 in /lib/x86_64-linux-gnu/libc.so.6)

c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800043 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77a8c216c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x77a86f5fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x77a86f6007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77a86f6016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x77a8dfc5c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x77a8e4c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x77a8e4d26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77a8c216c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x77a86f25c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x77a8dfc5c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x77a8e4c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x77a8e4d26850 in /lib/x86_64-linux-gnu/libc.so.6)

  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800043 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77eaaf76c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x77ea5cbfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x77ea5cc007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77ea5cc016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x77eacd2565c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x77ead2294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x77ead2326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77eaaf76c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x77ea5c85c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x77eacd2565c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x77ead2294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x77ead2326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 04:17:23,155 - wandb.wandb_agent - INFO - Cleaning up finished run: 3kc71gem
[2025-04-10 04:17:23,155][wandb.wandb_agent][INFO] - Cleaning up finished run: 3kc71gem
2025-04-10 04:17:23,884 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 04:17:23,884][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 04:17:23,884 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.29214741171559105
	config.input_dropout: 0.4931149552883829
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.027084508484850337
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 34
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.6954944082568605
	optimization_config.init_lr: 0.2346203367876711
	optimization_config.lr_decay_power: 1.955746030984313
	optimization_config.lr_frac_warmup_steps: 2.704088378942969e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.008598075742804448
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 04:17:23,884][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.29214741171559105
	config.input_dropout: 0.4931149552883829
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.027084508484850337
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 34
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.6954944082568605
	optimization_config.init_lr: 0.2346203367876711
	optimization_config.lr_decay_power: 1.955746030984313
	optimization_config.lr_frac_warmup_steps: 2.704088378942969e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.008598075742804448
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 04:17:23,894 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.29214741171559105 config.input_dropout=0.4931149552883829 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.027084508484850337 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=34 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.6954944082568605 optimization_config.init_lr=0.2346203367876711 optimization_config.lr_decay_power=1.955746030984313 optimization_config.lr_frac_warmup_steps=2.704088378942969e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.008598075742804448 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 04:17:23,894][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.29214741171559105 config.input_dropout=0.4931149552883829 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.027084508484850337 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=34 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.6954944082568605 optimization_config.init_lr=0.2346203367876711 optimization_config.lr_decay_power=1.955746030984313 optimization_config.lr_frac_warmup_steps=2.704088378942969e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.008598075742804448 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 04:17:28,906 - wandb.wandb_agent - INFO - Running runs: ['wjxbp38z']
[2025-04-10 04:17:28,906][wandb.wandb_agent][INFO] - Running runs: ['wjxbp38z']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_041731-wjxbp38z
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/wjxbp38z
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.29214741171559105
Overwriting input_dropout in config from 0.25407516893041604 to 0.4931149552883829
Overwriting resid_dropout in config from 0.4697133384759005 to 0.027084508484850337
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.29214741171559105
Overwriting input_dropout in config from 0.25407516893041604 to 0.4931149552883829
Overwriting resid_dropout in config from 0.4697133384759005 to 0.027084508484850337
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.59it/s][rank1]:[E410 04:47:40.005896646 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800074 milliseconds before timing out.
[rank1]:[E410 04:47:40.006246767 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 04:47:40.006267539 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 04:47:40.006276781 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 04:47:40.006284875 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 04:47:40.009274383 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800074 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70865466c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x708601bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x708601c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x708601c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7086728f95c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x708677294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x708677326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800074 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70865466c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x708601bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x708601c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x708601c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7086728f95c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x708677294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x708677326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70865466c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x70860185c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7086728f95c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x708677294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x708677326850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 04:47:40.017292469 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800086 milliseconds before timing out.
[rank0]:[E410 04:47:40.017511978 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 04:47:40.017526394 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 04:47:40.017533883 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 04:47:40.017540557 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 04:47:40.019447725 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800086 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x756c7f96c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x756c2cdfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x756c2ce007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x756c2ce016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x756c9dd2d5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x756ca2694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x756ca2726850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800086 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x756c7f96c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x756c2cdfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x756c2ce007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x756c2ce016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x756c9dd2d5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x756ca2694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x756ca2726850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x756c7f96c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x756c2ca5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x756c9dd2d5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x756ca2694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x756ca2726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 04:47:42,209 - wandb.wandb_agent - INFO - Cleaning up finished run: wjxbp38z
[2025-04-10 04:47:42,209][wandb.wandb_agent][INFO] - Cleaning up finished run: wjxbp38z
2025-04-10 04:47:42,934 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 04:47:42,934][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 04:47:42,935 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4499335368258795
	config.input_dropout: 0.22510829953286843
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3907022785825745
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 63
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0011415624194871538
	optimization_config.init_lr: 2.9263321336113796e-08
	optimization_config.lr_decay_power: 3.4810703419821443
	optimization_config.lr_frac_warmup_steps: 0.00010434012401404622
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004090111214130171
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 04:47:42,935][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4499335368258795
	config.input_dropout: 0.22510829953286843
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3907022785825745
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 63
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0011415624194871538
	optimization_config.init_lr: 2.9263321336113796e-08
	optimization_config.lr_decay_power: 3.4810703419821443
	optimization_config.lr_frac_warmup_steps: 0.00010434012401404622
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004090111214130171
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 04:47:42,944 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4499335368258795 config.input_dropout=0.22510829953286843 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3907022785825745 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=63 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0011415624194871538 optimization_config.init_lr=2.9263321336113796e-08 optimization_config.lr_decay_power=3.4810703419821443 optimization_config.lr_frac_warmup_steps=0.00010434012401404622 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004090111214130171 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 04:47:42,944][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4499335368258795 config.input_dropout=0.22510829953286843 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3907022785825745 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=63 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0011415624194871538 optimization_config.init_lr=2.9263321336113796e-08 optimization_config.lr_decay_power=3.4810703419821443 optimization_config.lr_frac_warmup_steps=0.00010434012401404622 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004090111214130171 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 04:47:47,956 - wandb.wandb_agent - INFO - Running runs: ['rru2vroc']
[2025-04-10 04:47:47,956][wandb.wandb_agent][INFO] - Running runs: ['rru2vroc']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_044750-rru2vroc
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/rru2vroc
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4499335368258795
Overwriting input_dropout in config from 0.25407516893041604 to 0.22510829953286843
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3907022785825745
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4499335368258795
Overwriting input_dropout in config from 0.25407516893041604 to 0.22510829953286843
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3907022785825745
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.64it/s][rank1]:[E410 05:17:59.941152545 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800018 milliseconds before timing out.
[rank1]:[E410 05:17:59.941484488 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 05:17:59.941505894 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 05:17:59.941517127 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 05:17:59.941526972 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 05:17:59.944520549 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800018 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x751e7766c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x751e24bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x751e24c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x751e24c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x751e959065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x751e9a294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x751e9a326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800018 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x751e7766c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x751e24bfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x751e24c007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x751e24c016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x751e959065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x751e9a294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x751e9a326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x751e7766c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x751e2485c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x751e959065c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x751e9a294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x751e9a326850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 05:17:59.945974476 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
[rank0]:[E410 05:17:59.946255440 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 05:17:59.946282376 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 05:17:59.946291671 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 05:17:59.946310001 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 05:17:59.949327262 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d1dd306c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7d1d805fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7d1d806007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d1d806016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7d1df12ac5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7d1df5c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7d1df5d26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d1dd306c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7d1d805fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7d1d806007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d1d806016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7d1df12ac5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7d1df5c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7d1df5d26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d1dd306c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7d1d8025c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7d1df12ac5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7d1df5c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7d1df5d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 05:18:01,140 - wandb.wandb_agent - INFO - Cleaning up finished run: rru2vroc
[2025-04-10 05:18:01,140][wandb.wandb_agent][INFO] - Cleaning up finished run: rru2vroc
2025-04-10 05:18:01,956 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 05:18:01,956][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 05:18:01,956 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4459342939892863
	config.input_dropout: 0.3209145672779442
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2121850767960804
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 22
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.8118660358413808
	optimization_config.init_lr: 1.5683215135861585e-05
	optimization_config.lr_decay_power: 4.648317998866242
	optimization_config.lr_frac_warmup_steps: 0.00010636046609793648
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0009403712705677448
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 05:18:01,956][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4459342939892863
	config.input_dropout: 0.3209145672779442
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2121850767960804
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 22
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.8118660358413808
	optimization_config.init_lr: 1.5683215135861585e-05
	optimization_config.lr_decay_power: 4.648317998866242
	optimization_config.lr_frac_warmup_steps: 0.00010636046609793648
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.0009403712705677448
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 05:18:01,966 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4459342939892863 config.input_dropout=0.3209145672779442 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2121850767960804 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=22 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.8118660358413808 optimization_config.init_lr=1.5683215135861585e-05 optimization_config.lr_decay_power=4.648317998866242 optimization_config.lr_frac_warmup_steps=0.00010636046609793648 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0009403712705677448 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 05:18:01,966][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4459342939892863 config.input_dropout=0.3209145672779442 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2121850767960804 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=22 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.8118660358413808 optimization_config.init_lr=1.5683215135861585e-05 optimization_config.lr_decay_power=4.648317998866242 optimization_config.lr_frac_warmup_steps=0.00010636046609793648 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.0009403712705677448 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 05:18:06,978 - wandb.wandb_agent - INFO - Running runs: ['5rmahsqj']
[2025-04-10 05:18:06,978][wandb.wandb_agent][INFO] - Running runs: ['5rmahsqj']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_051809-5rmahsqj
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/5rmahsqj
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4459342939892863
Overwriting input_dropout in config from 0.25407516893041604 to 0.3209145672779442
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2121850767960804
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4459342939892863
Overwriting input_dropout in config from 0.25407516893041604 to 0.3209145672779442
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2121850767960804
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.61it/s][rank1]:[E410 05:48:18.145878051 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800056 milliseconds before timing out.
[rank1]:[E410 05:48:18.146226084 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 05:48:18.146248087 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 05:48:18.146259630 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 05:48:18.146269626 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 05:48:18.149351650 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800056 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d71f5a6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7d71a2ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7d71a30007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d71a30016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7d7213cb75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7d7218694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7d7218726850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800056 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d71f5a6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7d71a2ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7d71a30007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d71a30016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7d7213cb75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7d7218694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7d7218726850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d71f5a6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7d71a2c5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7d7213cb75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7d7218694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7d7218726850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 05:48:18.157038185 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800067 milliseconds before timing out.
[rank0]:[E410 05:48:18.157317456 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 05:48:18.157334850 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 05:48:18.157344078 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 05:48:18.157366494 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 05:48:18.160349393 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800067 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bdf3036c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7bdedd7fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7bdedd8007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bdedd8016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7bdf4e7565c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7bdf53094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7bdf53126850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800067 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bdf3036c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7bdedd7fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7bdedd8007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bdedd8016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7bdf4e7565c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7bdf53094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7bdf53126850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bdf3036c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7bdedd45c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7bdf4e7565c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7bdf53094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7bdf53126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 05:48:22,480 - wandb.wandb_agent - INFO - Cleaning up finished run: 5rmahsqj
[2025-04-10 05:48:22,480][wandb.wandb_agent][INFO] - Cleaning up finished run: 5rmahsqj
2025-04-10 05:48:23,089 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 05:48:23,089][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 05:48:23,089 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.4452324510167555
	config.input_dropout: 0.4241305242451566
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2943145088519746
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 28
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0013045014632225372
	optimization_config.init_lr: 0.06452992375602849
	optimization_config.lr_decay_power: 3.5325993640170315
	optimization_config.lr_frac_warmup_steps: 5.943159107882081e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009925118610773194
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 05:48:23,089][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.4452324510167555
	config.input_dropout: 0.4241305242451566
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.2943145088519746
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 28
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0013045014632225372
	optimization_config.init_lr: 0.06452992375602849
	optimization_config.lr_decay_power: 3.5325993640170315
	optimization_config.lr_frac_warmup_steps: 5.943159107882081e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009925118610773194
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 05:48:23,099 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4452324510167555 config.input_dropout=0.4241305242451566 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2943145088519746 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=28 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0013045014632225372 optimization_config.init_lr=0.06452992375602849 optimization_config.lr_decay_power=3.5325993640170315 optimization_config.lr_frac_warmup_steps=5.943159107882081e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009925118610773194 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 05:48:23,099][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.4452324510167555 config.input_dropout=0.4241305242451566 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.2943145088519746 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=28 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0013045014632225372 optimization_config.init_lr=0.06452992375602849 optimization_config.lr_decay_power=3.5325993640170315 optimization_config.lr_frac_warmup_steps=5.943159107882081e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009925118610773194 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 05:48:28,112 - wandb.wandb_agent - INFO - Running runs: ['zsn5ff9q']
[2025-04-10 05:48:28,112][wandb.wandb_agent][INFO] - Running runs: ['zsn5ff9q']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_054830-zsn5ff9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/zsn5ff9q
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4452324510167555
Overwriting input_dropout in config from 0.25407516893041604 to 0.4241305242451566
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2943145088519746
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.4452324510167555
Overwriting input_dropout in config from 0.25407516893041604 to 0.4241305242451566
Overwriting resid_dropout in config from 0.4697133384759005 to 0.2943145088519746
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.58it/s][2025-04-10 06:14:19,222][wandb.sdk.internal.internal_api][ERROR] - 502 response executing GraphQL.
[2025-04-10 06:14:19,222][wandb.sdk.internal.internal_api][ERROR] - 
<html><head>
<meta http-equiv="content-type" content="text/html;charset=utf-8">
<title>502 Server Error</title>
</head>
<body text=#000000 bgcolor=#ffffff>
<h1>Error: Server Error</h1>
<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>
<h2></h2>
</body></html>

[rank1]:[E410 06:18:39.979646813 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800063 milliseconds before timing out.
[rank1]:[E410 06:18:39.979977973 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 06:18:39.979998268 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 06:18:39.980021532 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 06:18:39.980032008 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 06:18:39.983008389 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800063 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71de6f56c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x71de1c9fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x71de1ca007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71de1ca016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x71de8d9075c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x71de92294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x71de92326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800063 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71de6f56c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x71de1c9fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x71de1ca007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71de1ca016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x71de8d9075c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x71de92294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x71de92326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71de6f56c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x71de1c65c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x71de8d9075c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x71de92294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x71de92326850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 06:18:39.990904698 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800075 milliseconds before timing out.
[rank0]:[E410 06:18:39.991180498 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 06:18:39.991199438 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 06:18:39.991208463 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 06:18:39.991216238 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 06:18:39.994150289 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800075 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b1b6896c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7b1b15dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7b1b15e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b1b15e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7b1b8645c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7b1b8b494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7b1b8b526850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800075 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b1b6896c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7b1b15dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7b1b15e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b1b15e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7b1b8645c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7b1b8b494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7b1b8b526850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b1b6896c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7b1b15a5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7b1b8645c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7b1b8b494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7b1b8b526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 06:18:44,183 - wandb.wandb_agent - INFO - Cleaning up finished run: zsn5ff9q
[2025-04-10 06:18:44,183][wandb.wandb_agent][INFO] - Cleaning up finished run: zsn5ff9q
2025-04-10 06:18:44,748 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 06:18:44,748][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 06:18:44,748 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.13058479285054325
	config.input_dropout: 0.1846707592198158
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.02296347125944448
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 20
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0007554818358551955
	optimization_config.init_lr: 0.0002374046198959291
	optimization_config.lr_decay_power: 4.566277615264477
	optimization_config.lr_frac_warmup_steps: 0.0017503897074016158
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 3.636623287094295e-05
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 06:18:44,748][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.13058479285054325
	config.input_dropout: 0.1846707592198158
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.02296347125944448
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 20
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0007554818358551955
	optimization_config.init_lr: 0.0002374046198959291
	optimization_config.lr_decay_power: 4.566277615264477
	optimization_config.lr_frac_warmup_steps: 0.0017503897074016158
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 3.636623287094295e-05
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 06:18:44,758 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.13058479285054325 config.input_dropout=0.1846707592198158 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.02296347125944448 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=20 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0007554818358551955 optimization_config.init_lr=0.0002374046198959291 optimization_config.lr_decay_power=4.566277615264477 optimization_config.lr_frac_warmup_steps=0.0017503897074016158 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=3.636623287094295e-05 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 06:18:44,758][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.13058479285054325 config.input_dropout=0.1846707592198158 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.02296347125944448 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=20 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0007554818358551955 optimization_config.init_lr=0.0002374046198959291 optimization_config.lr_decay_power=4.566277615264477 optimization_config.lr_frac_warmup_steps=0.0017503897074016158 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=3.636623287094295e-05 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 06:18:49,770 - wandb.wandb_agent - INFO - Running runs: ['wl8w0nhl']
[2025-04-10 06:18:49,770][wandb.wandb_agent][INFO] - Running runs: ['wl8w0nhl']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_061851-wl8w0nhl
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/wl8w0nhl
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.13058479285054325
Overwriting input_dropout in config from 0.25407516893041604 to 0.1846707592198158
Overwriting resid_dropout in config from 0.4697133384759005 to 0.02296347125944448
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.13058479285054325
Overwriting input_dropout in config from 0.25407516893041604 to 0.1846707592198158
Overwriting resid_dropout in config from 0.4697133384759005 to 0.02296347125944448
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.54it/s][rank0]:[E410 06:49:00.680143106 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800018 milliseconds before timing out.
[rank0]:[E410 06:49:00.680465014 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 06:49:00.680484539 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 06:49:00.680496097 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 06:49:00.680506441 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 06:49:00.681115173 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800019 milliseconds before timing out.
[rank1]:[E410 06:49:00.681412938 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 06:49:00.681433290 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 06:49:00.681444429 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 06:49:00.681454115 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 06:49:00.683364949 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800018 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77872396c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7786d0dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7786d0e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7786d0e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7787414cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x778746494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x778746526850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800018 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77872396c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7786d0dfec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7786d0e007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7786d0e016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7787414cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x778746494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x778746526850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77872396c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7786d0a5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7787414cd5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x778746494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x778746526850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E410 06:49:00.684591259 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800019 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fcc37e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fcbe53fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fcbe54007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fcbe54016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fcc562b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fcc5ac94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fcc5ad26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800019 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fcc37e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fcbe53fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fcbe54007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fcbe54016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fcc562b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fcc5ac94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fcc5ad26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fcc37e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7fcbe505c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7fcc562b25c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7fcc5ac94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7fcc5ad26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 06:49:04,416 - wandb.wandb_agent - INFO - Cleaning up finished run: wl8w0nhl
[2025-04-10 06:49:04,416][wandb.wandb_agent][INFO] - Cleaning up finished run: wl8w0nhl
2025-04-10 06:49:04,930 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 06:49:04,930][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 06:49:04,931 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.22561343738899836
	config.input_dropout: 0.2912518036948212
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.07156694839698319
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 56
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.16231106837872372
	optimization_config.init_lr: 0.00022242143737942672
	optimization_config.lr_decay_power: 0.7737313289389696
	optimization_config.lr_frac_warmup_steps: 0.0012950933447117537
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.002982750865763388
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 06:49:04,931][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.22561343738899836
	config.input_dropout: 0.2912518036948212
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.07156694839698319
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 56
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.16231106837872372
	optimization_config.init_lr: 0.00022242143737942672
	optimization_config.lr_decay_power: 0.7737313289389696
	optimization_config.lr_frac_warmup_steps: 0.0012950933447117537
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.002982750865763388
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 06:49:04,940 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.22561343738899836 config.input_dropout=0.2912518036948212 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.07156694839698319 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=56 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.16231106837872372 optimization_config.init_lr=0.00022242143737942672 optimization_config.lr_decay_power=0.7737313289389696 optimization_config.lr_frac_warmup_steps=0.0012950933447117537 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.002982750865763388 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 06:49:04,940][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.22561343738899836 config.input_dropout=0.2912518036948212 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.07156694839698319 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=56 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.16231106837872372 optimization_config.init_lr=0.00022242143737942672 optimization_config.lr_decay_power=0.7737313289389696 optimization_config.lr_frac_warmup_steps=0.0012950933447117537 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.002982750865763388 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 06:49:09,952 - wandb.wandb_agent - INFO - Running runs: ['mfmsfhs2']
[2025-04-10 06:49:09,952][wandb.wandb_agent][INFO] - Running runs: ['mfmsfhs2']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_064911-mfmsfhs2
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/mfmsfhs2
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.22561343738899836
Overwriting input_dropout in config from 0.25407516893041604 to 0.2912518036948212
Overwriting resid_dropout in config from 0.4697133384759005 to 0.07156694839698319
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.22561343738899836
Overwriting input_dropout in config from 0.25407516893041604 to 0.2912518036948212
Overwriting resid_dropout in config from 0.4697133384759005 to 0.07156694839698319
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.61it/s][rank1]:[E410 07:19:21.835070540 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800089 milliseconds before timing out.
[rank1]:[E410 07:19:21.835402584 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 07:19:21.835422632 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 07:19:21.835433565 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 07:19:21.835443601 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 07:19:21.838435874 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800089 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x728e6016c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x728e0d5fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x728e0d6007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x728e0d6016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x728e7dcb15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x728e82c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x728e82d26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800089 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x728e6016c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x728e0d5fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x728e0d6007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x728e0d6016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x728e7dcb15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x728e82c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x728e82d26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x728e6016c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x728e0d25c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x728e7dcb15c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x728e82c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x728e82d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 07:19:21.841570201 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800096 milliseconds before timing out.
[rank0]:[E410 07:19:21.841855732 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 07:19:21.841871901 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 07:19:21.841895598 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 07:19:21.841904427 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 07:19:21.844914427 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800096 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe0b6a6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fe063ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fe0640007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fe0640016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fe0d4cb75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fe0d9694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fe0d9726850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800096 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe0b6a6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fe063ffec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fe0640007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fe0640016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fe0d4cb75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fe0d9694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fe0d9726850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe0b6a6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7fe063c5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7fe0d4cb75c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7fe0d9694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7fe0d9726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 07:19:24,224 - wandb.wandb_agent - INFO - Cleaning up finished run: mfmsfhs2
[2025-04-10 07:19:24,224][wandb.wandb_agent][INFO] - Cleaning up finished run: mfmsfhs2
2025-04-10 07:19:24,744 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 07:19:24,744][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 07:19:24,744 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.22607982127283915
	config.input_dropout: 0.22350860861149852
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.23339951957025712
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 25
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.01323596469946459
	optimization_config.init_lr: 0.00023034080089559352
	optimization_config.lr_decay_power: 2.1771861948797637
	optimization_config.lr_frac_warmup_steps: 0.00011466289324696864
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00982366909631958
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 07:19:24,744][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.22607982127283915
	config.input_dropout: 0.22350860861149852
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.23339951957025712
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 25
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.01323596469946459
	optimization_config.init_lr: 0.00023034080089559352
	optimization_config.lr_decay_power: 2.1771861948797637
	optimization_config.lr_frac_warmup_steps: 0.00011466289324696864
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00982366909631958
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 07:19:24,751 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.22607982127283915 config.input_dropout=0.22350860861149852 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.23339951957025712 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=25 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.01323596469946459 optimization_config.init_lr=0.00023034080089559352 optimization_config.lr_decay_power=2.1771861948797637 optimization_config.lr_frac_warmup_steps=0.00011466289324696864 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00982366909631958 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 07:19:24,751][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.22607982127283915 config.input_dropout=0.22350860861149852 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.23339951957025712 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=25 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.01323596469946459 optimization_config.init_lr=0.00023034080089559352 optimization_config.lr_decay_power=2.1771861948797637 optimization_config.lr_frac_warmup_steps=0.00011466289324696864 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00982366909631958 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 07:19:29,761 - wandb.wandb_agent - INFO - Running runs: ['aw010yfg']
[2025-04-10 07:19:29,761][wandb.wandb_agent][INFO] - Running runs: ['aw010yfg']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_071931-aw010yfg
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/aw010yfg
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.22607982127283915
Overwriting input_dropout in config from 0.25407516893041604 to 0.22350860861149852
Overwriting resid_dropout in config from 0.4697133384759005 to 0.23339951957025712
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.22607982127283915
Overwriting input_dropout in config from 0.25407516893041604 to 0.22350860861149852
Overwriting resid_dropout in config from 0.4697133384759005 to 0.23339951957025712
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.61it/s][rank1]:[E410 07:49:40.664673354 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800025 milliseconds before timing out.
[rank1]:[E410 07:49:40.664908609 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 07:49:40.664930361 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 07:49:40.664935665 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 07:49:40.664940199 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 07:49:40.666451440 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800025 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7179a3e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7179513fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7179514007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7179514016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7179c20ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7179c6a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7179c6b26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800025 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7179a3e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7179513fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7179514007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7179514016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7179c20ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7179c6a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7179c6b26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7179a3e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x71795105c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7179c20ea5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7179c6a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7179c6b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E410 07:49:40.686989881 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800047 milliseconds before timing out.
[rank0]:[E410 07:49:40.687282323 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 07:49:40.687298759 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 07:49:40.687307994 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 07:49:40.687315845 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 07:49:40.690267030 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800047 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76e8a5e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x76e8533fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x76e8534007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76e8534016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x76e8c38ad5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x76e8c8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x76e8c8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800047 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76e8a5e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x76e8533fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x76e8534007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76e8534016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x76e8c38ad5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x76e8c8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x76e8c8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76e8a5e6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x76e85305c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x76e8c38ad5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x76e8c8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x76e8c8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-10 07:50:46,095 - wandb.wandb_agent - INFO - Cleaning up finished run: aw010yfg
[2025-04-10 07:50:46,095][wandb.wandb_agent][INFO] - Cleaning up finished run: aw010yfg
2025-04-10 07:50:46,610 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 07:50:46,610][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 07:50:46,616 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.0020045542762892055
	config.input_dropout: 0.164752358121586
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.14440822249180318
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 41
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0022426418585799112
	optimization_config.init_lr: 7.385030030354898e-08
	optimization_config.lr_decay_power: 1.6866391271336565
	optimization_config.lr_frac_warmup_steps: 1.6324651507631313e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005786899976070193
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 07:50:46,616][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.0020045542762892055
	config.input_dropout: 0.164752358121586
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.14440822249180318
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 41
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0022426418585799112
	optimization_config.init_lr: 7.385030030354898e-08
	optimization_config.lr_decay_power: 1.6866391271336565
	optimization_config.lr_frac_warmup_steps: 1.6324651507631313e-05
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.005786899976070193
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 07:50:46,626 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.0020045542762892055 config.input_dropout=0.164752358121586 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.14440822249180318 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=41 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0022426418585799112 optimization_config.init_lr=7.385030030354898e-08 optimization_config.lr_decay_power=1.6866391271336565 optimization_config.lr_frac_warmup_steps=1.6324651507631313e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005786899976070193 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 07:50:46,626][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.0020045542762892055 config.input_dropout=0.164752358121586 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.14440822249180318 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=41 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0022426418585799112 optimization_config.init_lr=7.385030030354898e-08 optimization_config.lr_decay_power=1.6866391271336565 optimization_config.lr_frac_warmup_steps=1.6324651507631313e-05 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.005786899976070193 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 07:50:51,639 - wandb.wandb_agent - INFO - Running runs: ['9cw9vdlw']
[2025-04-10 07:50:51,639][wandb.wandb_agent][INFO] - Running runs: ['9cw9vdlw']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_075053-9cw9vdlw
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/9cw9vdlw
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.0020045542762892055
Overwriting input_dropout in config from 0.25407516893041604 to 0.164752358121586
Overwriting resid_dropout in config from 0.4697133384759005 to 0.14440822249180318
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.0020045542762892055
Overwriting input_dropout in config from 0.25407516893041604 to 0.164752358121586
Overwriting resid_dropout in config from 0.4697133384759005 to 0.14440822249180318
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.64it/s][rank: 1] Child process with PID 829822 terminated with code -9. Forcefully terminating all other processes to avoid zombies üßü
2025-04-10 08:16:00,430 - wandb.wandb_agent - INFO - Cleaning up finished run: 9cw9vdlw
[2025-04-10 08:16:00,430][wandb.wandb_agent][INFO] - Cleaning up finished run: 9cw9vdlw
2025-04-10 08:16:01,068 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 08:16:01,068][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 08:16:01,068 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.29212131324529117
	config.input_dropout: 0.32076356825687913
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3711750557093416
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 16
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.004150127510188593
	optimization_config.init_lr: 2.2408893411221118e-07
	optimization_config.lr_decay_power: 4.495735197550413
	optimization_config.lr_frac_warmup_steps: 0.025969996323455008
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00790686484393152
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 08:16:01,068][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.29212131324529117
	config.input_dropout: 0.32076356825687913
	config.is_cls_dist: False
	config.is_event_classification: True
	config.resid_dropout: 0.3711750557093416
	config.task_specific_params.pooling_method: last
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 16
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.004150127510188593
	optimization_config.init_lr: 2.2408893411221118e-07
	optimization_config.lr_decay_power: 4.495735197550413
	optimization_config.lr_frac_warmup_steps: 0.025969996323455008
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.00790686484393152
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_event_label
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 08:16:01,078 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.29212131324529117 config.input_dropout=0.32076356825687913 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3711750557093416 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=16 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.004150127510188593 optimization_config.init_lr=2.2408893411221118e-07 optimization_config.lr_decay_power=4.495735197550413 optimization_config.lr_frac_warmup_steps=0.025969996323455008 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00790686484393152 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 08:16:01,078][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.29212131324529117 config.input_dropout=0.32076356825687913 config.is_cls_dist=False config.is_event_classification=True config.resid_dropout=0.3711750557093416 config.task_specific_params.pooling_method=last data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=16 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.004150127510188593 optimization_config.init_lr=2.2408893411221118e-07 optimization_config.lr_decay_power=4.495735197550413 optimization_config.lr_frac_warmup_steps=0.025969996323455008 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.00790686484393152 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_event_label trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
2025-04-10 08:16:06,090 - wandb.wandb_agent - INFO - Running runs: ['z2s92780']
[2025-04-10 08:16:06,090][wandb.wandb_agent][INFO] - Running runs: ['z2s92780']
wandb: Currently logged in as: marcus-student-chalmers (marcus-student-chalmers-personal). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/wandb/run-20250410_081608-z2s92780
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run generative_event_stream_transformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep
wandb: üßπ View sweep at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/o9qlrb7q
wandb: üöÄ View run at https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/runs/z2s92780
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
[rank: 1] Seed set to 1
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name        | Type                               | Params | Mode 
---------------------------------------------------------------------------
0 | tte_metrics | ModuleDict                         | 0      | train
1 | metrics     | ModuleDict                         | 0      | train
2 | model       | CIPPTForGenerativeSequenceModeling | 1.6 M  | eval 
---------------------------------------------------------------------------
1.6 M     Trainable params
0         Non-trainable params
1.6 M     Total params
6.369     Total estimated model params size (MB)
92        Modules in train mode
199       Modules in eval mode
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.29212131324529117
Overwriting input_dropout in config from 0.25407516893041604 to 0.32076356825687913
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3711750557093416
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_event_label/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_event_label', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.29212131324529117
Overwriting input_dropout in config from 0.25407516893041604 to 0.32076356825687913
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3711750557093416
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to True
Overwriting task_specific_params in config from None to {'pooling_method': 'last', 'num_samples': None}
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/train_0.parquet
Re-loading task data for task_df_eneryield_event_label from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_event_label/tuning_0.parquet
torch.float32
torch.Size([40, 180]) True
torch.Size([90]) True
torch.Size([90]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180, 180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([128, 180]) True
torch.Size([128]) True
torch.Size([180, 128]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([180]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([28, 180]) True
torch.Size([28]) True
torch.Size([40, 180]) True
torch.Size([40]) True
torch.Size([36900]) True
torch.Size([36900]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 180]) True
torch.Size([1]) True
torch.Size([1, 36900]) True
torch.Size([1]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
torch.Size([2, 180]) True
torch.Size([2]) True
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.60it/s][2025-04-10 08:24:48,278][wandb.sdk.internal.internal_api][ERROR] - 404 response executing GraphQL.
[2025-04-10 08:24:48,278][wandb.sdk.internal.internal_api][ERROR] - {"errors":[{"message":"could not find agent bjlyy2fh during agentHeartbeat","path":["agentHeartbeat"]}],"data":{"agentHeartbeat":null}}
wandb: ERROR Error while calling W&B API: could not find agent bjlyy2fh during agentHeartbeat (<Response [404]>)
wandb: Terminating and syncing runs. Press ctrl-c to kill.
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
[rank0]:[E410 08:46:17.192131873 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800021 milliseconds before timing out.
[rank0]:[E410 08:46:17.192380410 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 146 PG status: last enqueued work: 146, last completed work: 145
[rank0]:[E410 08:46:17.192397399 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E410 08:46:17.192406364 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E410 08:46:17.192415154 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E410 08:46:17.193834767 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800021 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x712671d6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x71261f1fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x71261f2007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71261f2016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x71268f85c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x712694894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x712694926850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800021 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x712671d6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x71261f1fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x71261f2007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71261f2016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x71268f85c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x712694894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x712694926850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x712671d6c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x71261ee5c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x71268f85c5c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x712694894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x712694926850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E410 08:46:17.209678860 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800039 milliseconds before timing out.
[rank1]:[E410 08:46:17.209929600 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 145 PG status: last enqueued work: 145, last completed work: 144
[rank1]:[E410 08:46:17.209946812 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E410 08:46:17.209955210 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E410 08:46:17.209962563 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E410 08:46:17.211329118 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800039 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7714f856c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7714a59fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7714a5a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7714a5a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7715169075c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x77151b294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x77151b326850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=145, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800039 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7714f856c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7714a59fec74 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7714a5a007d0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7714a5a016ed in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7715169075c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x77151b294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x77151b326850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7714f856c1b6 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7714a565c6fc in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7715169075c0 in /home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x77151b294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x77151b326850 in /lib/x86_64-linux-gnu/libc.so.6)

Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/lib/retry.py", line 131, in __call__
    result = self._call_fn(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py", line 366, in execute
    return self.client.execute(*args, **kwargs)  # type: ignore
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py", line 52, in execute
    result = self._get_result(document, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py", line 60, in _get_result
    return self.transport.execute(document, *args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/lib/gql_request.py", line 59, in execute
    request.raise_for_status()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://api.wandb.ai/graphql

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py", line 2777, in agent_heartbeat
    response = self.gql(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py", line 338, in gql
    ret = self._retry_gql(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/lib/retry.py", line 147, in __call__
    retry_timedelta_triggered = check_retry_fn(e)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/util.py", line 906, in no_retry_auth
    raise CommError(
wandb.errors.CommError: It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 404: Not Found)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/launch_finetuning_wandb_hp_sweep.py", line 113, in main
    wandb.agent(project=sweep_kwargs["project"], entity="marcus-student-chalmers-personal", sweep_id=sweep_id)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/wandb_agent.py", line 583, in agent
    return run_agent(
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/wandb_agent.py", line 528, in run_agent
    agent.run()
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/wandb_agent.py", line 277, in run
    commands = self._api.agent_heartbeat(agent_id, {}, run_status)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/apis/internal.py", line 153, in agent_heartbeat
    return self.api.agent_heartbeat(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py", line 2788, in agent_heartbeat
    message = ast.literal_eval(e.args[0])["message"]
  File "/usr/lib/python3.10/ast.py", line 64, in literal_eval
    node_or_string = parse(node_or_string.lstrip(" \t"), mode='eval')
  File "/usr/lib/python3.10/ast.py", line 50, in parse
    return compile(source, filename, mode, flags,
  File "<unknown>", line 1
    It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 404: Not Found)
       ^^^^^^^
SyntaxError: invalid syntax

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Running sweep with config: FT_hp_sweep_interruption_in_seq
2025-04-10 08:46:26,688 - wandb.wandb_agent - INFO - Running runs: []
Create sweep with ID: 3j56549p
Sweep URL: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/3j56549p
[2025-04-10 08:46:26,688][wandb.wandb_agent][INFO] - Running runs: []
2025-04-10 08:46:27,107 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 08:46:27,107][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 08:46:27,107 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3675446727364325
	config.input_dropout: 0.28300686403864567
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.10824367047693224
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 9
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00011635406755459532
	optimization_config.init_lr: 0.003252305150124029
	optimization_config.lr_decay_power: 1.6753841589360383
	optimization_config.lr_frac_warmup_steps: 0.25395946462557834
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.003267621358906629
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_in_seq
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 08:46:27,107][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3675446727364325
	config.input_dropout: 0.28300686403864567
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.10824367047693224
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 9
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00011635406755459532
	optimization_config.init_lr: 0.003252305150124029
	optimization_config.lr_decay_power: 1.6753841589360383
	optimization_config.lr_frac_warmup_steps: 0.25395946462557834
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.003267621358906629
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_in_seq
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 08:46:27,111 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3675446727364325 config.input_dropout=0.28300686403864567 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.10824367047693224 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=9 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00011635406755459532 optimization_config.init_lr=0.003252305150124029 optimization_config.lr_decay_power=1.6753841589360383 optimization_config.lr_frac_warmup_steps=0.25395946462557834 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.003267621358906629 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_in_seq trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 08:46:27,111][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3675446727364325 config.input_dropout=0.28300686403864567 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.10824367047693224 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=9 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00011635406755459532 optimization_config.init_lr=0.003252305150124029 optimization_config.lr_decay_power=1.6753841589360383 optimization_config.lr_frac_warmup_steps=0.25395946462557834 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.003267621358906629 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_in_seq trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-10 08:46:32,121 - wandb.wandb_agent - INFO - Running runs: ['8huamq5s']
[2025-04-10 08:46:32,121][wandb.wandb_agent][INFO] - Running runs: ['8huamq5s']
Seed set to 1
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
None
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_interruption_in_seq', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3675446727364325
Overwriting input_dropout in config from 0.25407516893041604 to 0.28300686403864567
Overwriting resid_dropout in config from 0.4697133384759005 to 0.10824367047693224
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to False
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_interruption_in_seq from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq/train_0.parquet
Re-loading task data for task_df_eneryield_interruption_in_seq from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_interruption_in_seq/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
EXCEPTION: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
Error executing job with overrides: ['config.attention_dropout=0.3675446727364325', 'config.input_dropout=0.28300686403864567', 'config.is_cls_dist=False', 'config.is_event_classification=False', 'config.resid_dropout=0.10824367047693224', 'config.task_specific_params.pooling_method=mean', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=9', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.00011635406755459532', 'optimization_config.init_lr=0.003252305150124029', 'optimization_config.lr_decay_power=1.6753841589360383', 'optimization_config.lr_frac_warmup_steps=0.25395946462557834', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.003267621358906629', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_interruption_in_seq', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
2025-04-10 08:46:37,290 - wandb.wandb_agent - INFO - Cleaning up finished run: 8huamq5s
[2025-04-10 08:46:37,290][wandb.wandb_agent][INFO] - Cleaning up finished run: 8huamq5s
2025-04-10 08:46:37,777 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 08:46:37,777][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 08:46:37,778 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.20487330821355415
	config.input_dropout: 0.44587314020326585
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.416583842531198
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 48
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.004738991388023681
	optimization_config.init_lr: 3.364901987505325e-07
	optimization_config.lr_decay_power: 0.6724035373026875
	optimization_config.lr_frac_warmup_steps: 0.00014287627784865723
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.006689109380041764
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_in_seq
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 08:46:37,778][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.20487330821355415
	config.input_dropout: 0.44587314020326585
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.416583842531198
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 48
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.004738991388023681
	optimization_config.init_lr: 3.364901987505325e-07
	optimization_config.lr_decay_power: 0.6724035373026875
	optimization_config.lr_frac_warmup_steps: 0.00014287627784865723
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.006689109380041764
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_in_seq
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 08:46:37,788 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.20487330821355415 config.input_dropout=0.44587314020326585 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.416583842531198 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=48 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.004738991388023681 optimization_config.init_lr=3.364901987505325e-07 optimization_config.lr_decay_power=0.6724035373026875 optimization_config.lr_frac_warmup_steps=0.00014287627784865723 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.006689109380041764 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_in_seq trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 08:46:37,788][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.20487330821355415 config.input_dropout=0.44587314020326585 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.416583842531198 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=48 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.004738991388023681 optimization_config.init_lr=3.364901987505325e-07 optimization_config.lr_decay_power=0.6724035373026875 optimization_config.lr_frac_warmup_steps=0.00014287627784865723 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.006689109380041764 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_in_seq trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-10 08:46:42,801 - wandb.wandb_agent - INFO - Running runs: ['tga8bh1u']
[2025-04-10 08:46:42,801][wandb.wandb_agent][INFO] - Running runs: ['tga8bh1u']
Seed set to 1
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
None
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_interruption_in_seq', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.20487330821355415
Overwriting input_dropout in config from 0.25407516893041604 to 0.44587314020326585
Overwriting resid_dropout in config from 0.4697133384759005 to 0.416583842531198
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to False
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_interruption_in_seq from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq/train_0.parquet
Re-loading task data for task_df_eneryield_interruption_in_seq from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_interruption_in_seq/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
EXCEPTION: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
Error executing job with overrides: ['config.attention_dropout=0.20487330821355415', 'config.input_dropout=0.44587314020326585', 'config.is_cls_dist=False', 'config.is_event_classification=False', 'config.resid_dropout=0.416583842531198', 'config.task_specific_params.pooling_method=mean', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=48', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.004738991388023681', 'optimization_config.init_lr=3.364901987505325e-07', 'optimization_config.lr_decay_power=0.6724035373026875', 'optimization_config.lr_frac_warmup_steps=0.00014287627784865723', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.006689109380041764', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_interruption_in_seq', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
2025-04-10 08:46:47,967 - wandb.wandb_agent - INFO - Cleaning up finished run: tga8bh1u
[2025-04-10 08:46:47,967][wandb.wandb_agent][INFO] - Cleaning up finished run: tga8bh1u
2025-04-10 08:46:48,510 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 08:46:48,510][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 08:46:48,510 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.035361495357762285
	config.input_dropout: 0.2518099621382687
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.016959336266136216
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 29
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0013870537426550144
	optimization_config.init_lr: 1.6278516213450614e-06
	optimization_config.lr_decay_power: 4.838248656114823
	optimization_config.lr_frac_warmup_steps: 5.192594469287364e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004124974794653667
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_in_seq
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 08:46:48,510][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.035361495357762285
	config.input_dropout: 0.2518099621382687
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.016959336266136216
	config.task_specific_params.pooling_method: max
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 29
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0013870537426550144
	optimization_config.init_lr: 1.6278516213450614e-06
	optimization_config.lr_decay_power: 4.838248656114823
	optimization_config.lr_frac_warmup_steps: 5.192594469287364e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004124974794653667
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_in_seq
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 08:46:48,523 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.035361495357762285 config.input_dropout=0.2518099621382687 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.016959336266136216 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=29 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0013870537426550144 optimization_config.init_lr=1.6278516213450614e-06 optimization_config.lr_decay_power=4.838248656114823 optimization_config.lr_frac_warmup_steps=5.192594469287364e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004124974794653667 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_in_seq trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 08:46:48,523][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.035361495357762285 config.input_dropout=0.2518099621382687 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.016959336266136216 config.task_specific_params.pooling_method=max data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=29 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0013870537426550144 optimization_config.init_lr=1.6278516213450614e-06 optimization_config.lr_decay_power=4.838248656114823 optimization_config.lr_frac_warmup_steps=5.192594469287364e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004124974794653667 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_in_seq trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-10 08:46:53,536 - wandb.wandb_agent - INFO - Running runs: ['83cpict0']
[2025-04-10 08:46:53,536][wandb.wandb_agent][INFO] - Running runs: ['83cpict0']
Seed set to 1
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
None
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_interruption_in_seq', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.035361495357762285
Overwriting input_dropout in config from 0.25407516893041604 to 0.2518099621382687
Overwriting resid_dropout in config from 0.4697133384759005 to 0.016959336266136216
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to False
Overwriting task_specific_params in config from None to {'pooling_method': 'max', 'num_samples': None}
Re-loading task data for task_df_eneryield_interruption_in_seq from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq/train_0.parquet
Re-loading task data for task_df_eneryield_interruption_in_seq from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_in_seq/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_interruption_in_seq/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
EXCEPTION: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
Error executing job with overrides: ['config.attention_dropout=0.035361495357762285', 'config.input_dropout=0.2518099621382687', 'config.is_cls_dist=False', 'config.is_event_classification=False', 'config.resid_dropout=0.016959336266136216', 'config.task_specific_params.pooling_method=max', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=29', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.0013870537426550144', 'optimization_config.init_lr=1.6278516213450614e-06', 'optimization_config.lr_decay_power=4.838248656114823', 'optimization_config.lr_frac_warmup_steps=5.192594469287364e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.004124974794653667', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_interruption_in_seq', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
2025-04-10 08:46:58,710 - wandb.wandb_agent - ERROR - Detected 3 failed runs in the first 60 seconds, shutting down.
[2025-04-10 08:46:58,710][wandb.wandb_agent][ERROR] - Detected 3 failed runs in the first 60 seconds, shutting down.
2025-04-10 08:46:58,710 - wandb.wandb_agent - INFO - To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true
[2025-04-10 08:46:58,710][wandb.wandb_agent][INFO] - To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true
wandb: Terminating and syncing runs. Press ctrl-c to kill.
Running sweep with config: FT_hp_sweep_interruption_next_week
2025-04-10 08:47:06,883 - wandb.wandb_agent - INFO - Running runs: []
Create sweep with ID: ikftf8to
Sweep URL: https://wandb.ai/marcus-student-chalmers-personal/eneryield_ft_sweep/sweeps/ikftf8to
[2025-04-10 08:47:06,883][wandb.wandb_agent][INFO] - Running runs: []
2025-04-10 08:47:07,235 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 08:47:07,235][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 08:47:07,235 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3065342464708923
	config.input_dropout: 0.17607057355718114
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.1455451109271239
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 35
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0003217121651250506
	optimization_config.init_lr: 0.07876654038495019
	optimization_config.lr_decay_power: 1.9759074974477369
	optimization_config.lr_frac_warmup_steps: 0.00017441185139286598
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.001871221850961781
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_next_week
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 08:47:07,235][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3065342464708923
	config.input_dropout: 0.17607057355718114
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.1455451109271239
	config.task_specific_params.pooling_method: mean
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 35
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0003217121651250506
	optimization_config.init_lr: 0.07876654038495019
	optimization_config.lr_decay_power: 1.9759074974477369
	optimization_config.lr_frac_warmup_steps: 0.00017441185139286598
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.001871221850961781
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_next_week
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 08:47:07,249 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3065342464708923 config.input_dropout=0.17607057355718114 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.1455451109271239 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=35 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0003217121651250506 optimization_config.init_lr=0.07876654038495019 optimization_config.lr_decay_power=1.9759074974477369 optimization_config.lr_frac_warmup_steps=0.00017441185139286598 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.001871221850961781 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_next_week trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 08:47:07,249][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3065342464708923 config.input_dropout=0.17607057355718114 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.1455451109271239 config.task_specific_params.pooling_method=mean data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=35 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0003217121651250506 optimization_config.init_lr=0.07876654038495019 optimization_config.lr_decay_power=1.9759074974477369 optimization_config.lr_frac_warmup_steps=0.00017441185139286598 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.001871221850961781 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_next_week trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 08:47:12,263 - wandb.wandb_agent - INFO - Running runs: ['lhym9j5q']
[2025-04-10 08:47:12,263][wandb.wandb_agent][INFO] - Running runs: ['lhym9j5q']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
None
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_interruption_next_week', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3065342464708923
Overwriting input_dropout in config from 0.25407516893041604 to 0.17607057355718114
Overwriting resid_dropout in config from 0.4697133384759005 to 0.1455451109271239
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to False
Overwriting task_specific_params in config from None to {'pooling_method': 'mean', 'num_samples': None}
Re-loading task data for task_df_eneryield_interruption_next_week from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week/train_0.parquet
Re-loading task data for task_df_eneryield_interruption_next_week from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_interruption_next_week/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
EXCEPTION: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
Error executing job with overrides: ['config.attention_dropout=0.3065342464708923', 'config.input_dropout=0.17607057355718114', 'config.is_cls_dist=False', 'config.is_event_classification=False', 'config.resid_dropout=0.1455451109271239', 'config.task_specific_params.pooling_method=mean', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=35', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.0003217121651250506', 'optimization_config.init_lr=0.07876654038495019', 'optimization_config.lr_decay_power=1.9759074974477369', 'optimization_config.lr_frac_warmup_steps=0.00017441185139286598', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.001871221850961781', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_interruption_next_week', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
2025-04-10 08:47:17,445 - wandb.wandb_agent - INFO - Cleaning up finished run: lhym9j5q
[2025-04-10 08:47:17,445][wandb.wandb_agent][INFO] - Cleaning up finished run: lhym9j5q
2025-04-10 08:47:17,884 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 08:47:17,884][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 08:47:17,885 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.3545844020924182
	config.input_dropout: 0.05802383685796558
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.3347741998810022
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 56
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0006688727979102802
	optimization_config.init_lr: 0.10386891526058552
	optimization_config.lr_decay_power: 4.024220188642799
	optimization_config.lr_frac_warmup_steps: 1.7957331141252462e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009098719692459636
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_next_week
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 08:47:17,885][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.3545844020924182
	config.input_dropout: 0.05802383685796558
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.3347741998810022
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 56
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.0006688727979102802
	optimization_config.init_lr: 0.10386891526058552
	optimization_config.lr_decay_power: 4.024220188642799
	optimization_config.lr_frac_warmup_steps: 1.7957331141252462e-06
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.009098719692459636
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_next_week
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 08:47:17,894 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3545844020924182 config.input_dropout=0.05802383685796558 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.3347741998810022 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=56 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0006688727979102802 optimization_config.init_lr=0.10386891526058552 optimization_config.lr_decay_power=4.024220188642799 optimization_config.lr_frac_warmup_steps=1.7957331141252462e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009098719692459636 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_next_week trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 08:47:17,894][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.3545844020924182 config.input_dropout=0.05802383685796558 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.3347741998810022 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=56 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.0006688727979102802 optimization_config.init_lr=0.10386891526058552 optimization_config.lr_decay_power=4.024220188642799 optimization_config.lr_frac_warmup_steps=1.7957331141252462e-06 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.009098719692459636 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_next_week trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
2025-04-10 08:47:22,909 - wandb.wandb_agent - INFO - Running runs: ['a9i1yipr']
[2025-04-10 08:47:22,909][wandb.wandb_agent][INFO] - Running runs: ['a9i1yipr']
Seed set to 1
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
None
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_interruption_next_week', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.3545844020924182
Overwriting input_dropout in config from 0.25407516893041604 to 0.05802383685796558
Overwriting resid_dropout in config from 0.4697133384759005 to 0.3347741998810022
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to False
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_interruption_next_week from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week/train_0.parquet
Re-loading task data for task_df_eneryield_interruption_next_week from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_interruption_next_week/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
EXCEPTION: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
Error executing job with overrides: ['config.attention_dropout=0.3545844020924182', 'config.input_dropout=0.05802383685796558', 'config.is_cls_dist=False', 'config.is_event_classification=False', 'config.resid_dropout=0.3347741998810022', 'config.task_specific_params.pooling_method=cls', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=56', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.0006688727979102802', 'optimization_config.init_lr=0.10386891526058552', 'optimization_config.lr_decay_power=4.024220188642799', 'optimization_config.lr_frac_warmup_steps=1.7957331141252462e-06', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.009098719692459636', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_interruption_next_week', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
2025-04-10 08:47:28,069 - wandb.wandb_agent - INFO - Cleaning up finished run: a9i1yipr
[2025-04-10 08:47:28,069][wandb.wandb_agent][INFO] - Cleaning up finished run: a9i1yipr
2025-04-10 08:47:28,497 - wandb.wandb_agent - INFO - Agent received command: run
[2025-04-10 08:47:28,497][wandb.wandb_agent][INFO] - Agent received command: run
2025-04-10 08:47:28,497 - wandb.wandb_agent - INFO - Agent starting run with config:
	config.attention_dropout: 0.2593361601351463
	config.input_dropout: 0.13589079450218844
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.18572847042995025
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 16
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00383919415608445
	optimization_config.init_lr: 0.04723660541304248
	optimization_config.lr_decay_power: 1.5060063939566914
	optimization_config.lr_frac_warmup_steps: 0.00010717644356287764
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004128865189172603
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_next_week
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
[2025-04-10 08:47:28,497][wandb.wandb_agent][INFO] - Agent starting run with config:
	config.attention_dropout: 0.2593361601351463
	config.input_dropout: 0.13589079450218844
	config.is_cls_dist: False
	config.is_event_classification: False
	config.resid_dropout: 0.18572847042995025
	config.task_specific_params.pooling_method: cls
	data_config.save_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
	do_overwrite: True
	load_from_model_dir: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/
	optimization_config.batch_size: 16
	optimization_config.end_lr: null
	optimization_config.end_lr_frac_of_init_lr: 0.00383919415608445
	optimization_config.init_lr: 0.04723660541304248
	optimization_config.lr_decay_power: 1.5060063939566914
	optimization_config.lr_frac_warmup_steps: 0.00010717644356287764
	optimization_config.max_epochs: 200
	optimization_config.num_dataloader_workers: 5
	optimization_config.patience: 8
	optimization_config.weight_decay: 0.004128865189172603
	pretrained_weights_fp: /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights
	seed: 1
	task_df_name: task_df_eneryield_interruption_next_week
	trainer_config.detect_anomaly: False
	trainer_config.log_every_n_steps: 50
	wandb_logger_kwargs.do_log_graph: False
	wandb_logger_kwargs.log_model: False
	wandb_logger_kwargs.name: generative_event_stream_transformer
	wandb_logger_kwargs.project: eneryield_ft_sweep
2025-04-10 08:47:28,507 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2593361601351463 config.input_dropout=0.13589079450218844 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.18572847042995025 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=16 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00383919415608445 optimization_config.init_lr=0.04723660541304248 optimization_config.lr_decay_power=1.5060063939566914 optimization_config.lr_frac_warmup_steps=0.00010717644356287764 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004128865189172603 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_next_week trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
[2025-04-10 08:47:28,507][wandb.wandb_agent][INFO] - About to run command: /usr/bin/env python scripts/finetune.py config.attention_dropout=0.2593361601351463 config.input_dropout=0.13589079450218844 config.is_cls_dist=False config.is_event_classification=False config.resid_dropout=0.18572847042995025 config.task_specific_params.pooling_method=cls data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield do_overwrite=True load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/ optimization_config.batch_size=16 optimization_config.end_lr=null optimization_config.end_lr_frac_of_init_lr=0.00383919415608445 optimization_config.init_lr=0.04723660541304248 optimization_config.lr_decay_power=1.5060063939566914 optimization_config.lr_frac_warmup_steps=0.00010717644356287764 optimization_config.max_epochs=200 optimization_config.num_dataloader_workers=5 optimization_config.patience=8 optimization_config.weight_decay=0.004128865189172603 pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights seed=1 task_df_name=task_df_eneryield_interruption_next_week trainer_config.detect_anomaly=False trainer_config.log_every_n_steps=50 wandb_logger_kwargs.do_log_graph=False wandb_logger_kwargs.log_model=False wandb_logger_kwargs.name=generative_event_stream_transformer wandb_logger_kwargs.project=eneryield_ft_sweep
2025-04-10 08:47:33,519 - wandb.wandb_agent - INFO - Running runs: ['cp5zumt4']
[2025-04-10 08:47:33,519][wandb.wandb_agent][INFO] - Running runs: ['cp5zumt4']
WARNING: For a conditionally_independent model, measurements_per_dep_graph_level is not used; got []. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
Seed set to 1
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
None
Loading data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/data_config.json
reloaded_data_config PytorchDatasetConfig(save_dir=PosixPath('/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield'), max_seq_len=205, min_seq_len=2, seq_padding_side='right', subsequence_sampling_strategy='random', train_subset_size='FULL', train_subset_seed=1, task_df_name='task_df_eneryield_interruption_next_week', do_include_subsequence_indices=False, do_include_subject_id=False, do_include_start_time_min=False)
Overwriting save_dir in data_config from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield to /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield
Overwriting seq_padding_side in data_config from right to right
Overwriting subsequence_sampling_strategy in data_config from random to to_end
Overwriting train_subset_size in data_config from FULL to FULL
Overwriting train_subset_seed in data_config from 1 to 1
Loading config from /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/config.json
Overwriting attention_dropout in config from 0.3462434018978263 to 0.2593361601351463
Overwriting input_dropout in config from 0.25407516893041604 to 0.13589079450218844
Overwriting resid_dropout in config from 0.4697133384759005 to 0.18572847042995025
Overwriting is_cls_dist in config from False to False
Overwriting is_event_classification in config from False to False
Overwriting task_specific_params in config from None to {'pooling_method': 'cls', 'num_samples': None}
Re-loading task data for task_df_eneryield_interruption_next_week from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week/train_0.parquet
Re-loading task data for task_df_eneryield_interruption_next_week from /home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week:
/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield/DL_reps/for_task/task_df_eneryield_interruption_next_week/tuning_0.parquet
Saving config files...
Writing to /home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/finetuning/task_df_eneryield_interruption_next_week/config.json
WARNING: For a conditionally_independent model, do_full_block_in_seq_attention is not used; got False. Setting to None.
WARNING: For a conditionally_independent model, do_full_block_in_dep_graph_attention is not used; got True. Setting to None.
WARNING: For a conditionally_independent model, dep_graph_window_size is not used; got 2. Setting to None.
torch.float32
EXCEPTION: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'
Error executing job with overrides: ['config.attention_dropout=0.2593361601351463', 'config.input_dropout=0.13589079450218844', 'config.is_cls_dist=False', 'config.is_event_classification=False', 'config.resid_dropout=0.18572847042995025', 'config.task_specific_params.pooling_method=cls', 'data_config.save_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/data/processed/eneryield', 'do_overwrite=True', 'load_from_model_dir=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/', 'optimization_config.batch_size=16', 'optimization_config.end_lr=null', 'optimization_config.end_lr_frac_of_init_lr=0.00383919415608445', 'optimization_config.init_lr=0.04723660541304248', 'optimization_config.lr_decay_power=1.5060063939566914', 'optimization_config.lr_frac_warmup_steps=0.00010717644356287764', 'optimization_config.max_epochs=200', 'optimization_config.num_dataloader_workers=5', 'optimization_config.patience=8', 'optimization_config.weight_decay=0.004128865189172603', 'pretrained_weights_fp=/home/filip-marcus/ESGPT_new/EventStreamGPT/pretrain/eneryield/pretrained_weights', 'seed=1', 'task_df_name=task_df_eneryield_interruption_next_week', 'trainer_config.detect_anomaly=False', 'trainer_config.log_every_n_steps=50', 'wandb_logger_kwargs.do_log_graph=False', 'wandb_logger_kwargs.log_model=False', 'wandb_logger_kwargs.name=generative_event_stream_transformer', 'wandb_logger_kwargs.project=eneryield_ft_sweep']
Traceback (most recent call last):
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/scripts/finetune.py", line 44, in main
    return train(cfg)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 385, in wrap
    raise ex
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/utils.py", line 378, in wrap
    fn_return = task_func(*args, **kwargs)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/fine_tuning_dev.py", line 281, in train
    LM = ESTForGenerativeSequenceModelingLM(**model_params)
  File "/home/filip-marcus/ESGPT_new/EventStreamGPT/EventStream/transformer/lightning_modules/generative_modeling.py", line 125, in __init__
    self.model.output_layer.TaskClassificationLayer.bias.data.fill_(-0.08)
  File "/home/filip-marcus/ESGPT_new/esgpt-new-venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'ConditionallyIndependentGenerativeOutputLayer' object has no attribute 'TaskClassificationLayer'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
2025-04-10 08:47:38,679 - wandb.wandb_agent - ERROR - Detected 3 failed runs in the first 60 seconds, shutting down.
[2025-04-10 08:47:38,679][wandb.wandb_agent][ERROR] - Detected 3 failed runs in the first 60 seconds, shutting down.
2025-04-10 08:47:38,679 - wandb.wandb_agent - INFO - To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true
[2025-04-10 08:47:38,679][wandb.wandb_agent][INFO] - To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true
wandb: Terminating and syncing runs. Press ctrl-c to kill.
